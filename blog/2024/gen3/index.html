<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="fqpmQwGNtZ1kJ8UAsQMb7RD2N7DcYRDlyGbAJsnZuGM"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Generative Models 3 - Normalizing Flows | Roy Friedman</title> <meta name="author" content="Roy Friedman"> <meta name="description" content=""> <meta name="keywords" content="academic-website, roy-friedman, machine-learning, ML, bayesian, PhD"> <meta name="google-site-verification" content="EUCyoY6MaSyn4ZvM9TfAuzzeleW7dR0PHS4_UCtyZ4I"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://friedmanroy.github.io/blog/2024/gen3/"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Generative Models 3 - Normalizing Flows",
      "description": "",
      "published": "August 21, 2024",
      "authors": [
        {
          "author": "Roy Friedman",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Hebrew University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Roy </span>Friedman</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/BML/">Bayesian Machine Learning</a> </li> <li class="nav-item "> <a class="nav-link" href="/_pages/cv/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Generative Models 3 - Normalizing Flows</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#when-the-generator-is-invertible">When the Generator is Invertible</a></div> <div><a href="#invertible-layers">Invertible Layers</a></div> <div><a href="#continuous-flows">Continuous Flows</a></div> <div><a href="#problems-with-normalizing-and-continuous-flows">Problems with Normalizing and Continuous Flows</a></div> </nav> </d-contents> <p><span style="float:left"><a href="https://friedmanroy.github.io/blog/2024/gen2/">← Variation Methods</a></span><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen4/">DDPM →</a></span> <br></p> <d-byline></d-byline> <p>So, VAEs were kind of a headache. If we think back and try to find out why, the reasons were: the latent space was smaller than the data space, adding an observation model means we need an integral to go back to the latent space, and we have no direct way to “invert” the generator. Other than that, VAEs are very general and the only constraints are those imposed implicitly by the encoder/decoder.</p> <p>Instead, we can try to add back into the mix stronger constraints, but in a way that makes sure that the generator is invertible. If we manage to do this, we can go back from data space to the latent space at will. Normalizing flows <d-cite key="papamakarios2021normalizing"></d-cite> are the method to do this. <br></p> <h1 id="when-the-generator-is-invertible"><strong>When the Generator is Invertible</strong></h1> <d-byline></d-byline> <p>Again, we assume $z\sim p_{z}\left(z\right)$ and now we will assume $x=G_{\theta}\left(z\right)$ and $\text{dim}\left(z\right)=\text{dim}\left(x\right)$. That is, no observation noise. If $G_{\theta}\left(z\right)$ is completely bijective (one-to-one and onto), then the probability of $x$ can be written in terms of the probability of $z$:</p> \[\begin{equation} p_{\theta}\left(x\right)=p_{z}\left(G_{\theta}^{-1}\left(x\right)\right)\left\vert \text{det}\left[\frac{\partial G_{\theta}^{-1}\left(x\right)}{\partial x}\right]\right\vert \end{equation}\] <p>where $\partial G_{\theta}^{-1}\left(x\right)/\partial x$ is the Jacobian of the inverse mapping $z=G_{\theta}^{-1}\left(x\right)$. So we can just directly use the MLE criterion to train $G_{\theta}\left(z\right)$:</p> \[\begin{equation} L\left(\theta\right)=-\frac{1}{N}\sum_{i=1}^{N}\log p_{\theta}\left(x_{i}\right)=-\frac{1}{N}\sum_{i=1}^{N}\left(\log p_{z}\left(G_{\theta}^{-1}\left(x_{i}\right)\right)+\log\left\vert \text{det}\left[\frac{\partial G_{\theta}^{-1}\left(x_{i}\right)}{\partial x_{i}}\right]\right\vert \right)\label{eq:change-MLE} \end{equation}\] <p>We are, of course, going to use multiple layers to define our mapping, so that:</p> \[\begin{equation} x=G_{\theta}\left(z\right)=\ell_{L}\circ\ell_{L-1}\circ\cdots\circ\ell_{1}\left(z\right)\Leftrightarrow z=G_{\theta}^{-1}\left(x\right)=\ell_{1}^{-1}\circ\ell_{2}^{-1}\circ\cdots\circ\ell_{L}^{-1}\left(x\right) \end{equation}\] <p>where $L$ is the number of layers we use and $\ell_{i}\left(\cdot\right)$ is the $i$-th layer. Let’s call $y_{i}=\ell_{i}^{-1}\left(y_{i+1}\right)$ the output of the inverse of $i$-th layer with $y_{L+1}=x$, then we can rewrite equation \eqref{eq:change-MLE} as:</p> \[\begin{equation} L\left(\theta\right)=-\frac{1}{N}\sum_{i=1}^{N}\left(\log p_{z}\left(G_{\theta}^{-1}\left(x_{i}\right)\right)+\sum_{j=1}^{L}\log\left\vert \text{det}\left[\frac{\partial\ell_{j}^{-1}\left(y_{j+1}\right)}{\partial y_{j+1}}\right]\right\vert \right) \end{equation}\] <p>through the chain rule. In other words, if we know the log-absolute-determinant of each layer individually, we just have to calculate them on the fly.</p> <p>The question now becomes: how can we define $\ell_{i}\left(\cdot\right)$ so that it is invertible <em>and</em> it’s easy to calculate the determinant of the Jacobian?</p> <p><br></p> <h1 id="invertible-layers"><strong>Invertible Layers</strong></h1> <d-byline></d-byline> <p>To ensure that the determinant is easily calculable, we essentially have three types of transformations we can use:</p> <ol> <li>Orthogonal transformations whose determinant is equal to 1 (or -1). These include things like a rotation around the origin, translation or a change of basis (like Fourier). Another very common orthogonal transformation (that is invertible) is a random shuffle of the coordinates.</li> <li>Transformations where each coordinate is changed independently, making the Jacobian diagonal in which case the determinant is the product of said diagonal.</li> <li>Transformations where some coordinates remain unchanged, creating a triangular Jacobian. If the Jacobian is triangular then the determinant is again the product of the terms on the diagonal.</li> </ol> <p>There exist more types of transformations and ways to make this even more confusing, but these are the basics so we’ll stay with them. At any rate, in each of these cases calculating the determinant is feasible and not much slower than just a usual forward pass through the network.</p> <h3 id="independent-coordinate-transformations">Independent Coordinate Transformations</h3> <p>We can use <em>any</em> element-wise transformation that we want, as long as the derivative for each coordinate is simple to calculate.</p> <p>The easiest invertible transformations we can start thinking about are of the following sort:</p> \[\begin{equation} \ell\left(x\right)=e^{s}\odot x+b\quad x,s,b\in\mathbb{R}^{d}\quad\Leftrightarrow\ell^{-1}\left(y\right)=e^{-s}\odot\left(y-b\right) \end{equation}\] <p>In this case, the inversion is very easy to describe. Also, we ensure that the layers are always invertible by using an element-wise multiplication of $x$ with $e^{s}$ instead of $s$ directly - this ensures that $\left[e^{s}\right]_{i}&gt;0$ is always true. The Jacobian of the inverse of this transformation is simply $\text{diag}\left(e^{-s}\right)$, which is really easy to calculate on the fly. Sometimes this transformation is called an <em>ActNorm layer</em> and is initialized so that $e^{-s}=1/\sigma^{2}$ where $\sigma^{2}$ is the variance of the incoming signal and $b$ is equal to the mean, essentially standardizing the incoming signals.</p> <p>Another useful transformation is one that moves everything in the range $\left(-\infty,\infty\right)$ to $\left(0,1\right)$, i.e. the softmax:</p> \[\begin{equation} \text{sig}\left(x_{i}\right)=\frac{1}{1+e^{-x_{i}}}\Leftrightarrow\text{sig}^{-1}\left(y_{i}\right)=\log\left(\frac{1-y_{i}}{y_{i}}\right) \end{equation}\] <p>The derivative of this transformation is given by:</p> \[\begin{equation} \frac{d}{dx_{i}}\text{sig}\left(x_{i}\right)=\text{sig}\left(x_{i}\right)\cdot\left(1-\text{sig}\left(x_{i}\right)\right) \end{equation}\] <p>which is also pretty simple to calculate on the fly.</p> <h3 id="coupling-layers">Coupling Layers</h3> <p>While element-wise transformations are great, they don’t allow the model to learn anything regarding the correlations (or higher order statistics) between coordinates. Really, we need transformations that mix up the information. The most commonly used types of layers that act in this manner are called <em>affine coupling layers</em>. These layers are defined as follows:</p> \[\begin{equation} \ell\left(\left[\begin{matrix}x_{a}\\ x_{b} \end{matrix}\right]\right)=\left[\begin{matrix}x_{a}\\ x_{b}\odot\exp\left[f_{s}\left(x_{a}\right)\right]+f_{t}\left(x_{a}\right) \end{matrix}\right]\qquad x_{a},x_{b}\in\mathbb{R}^{d/2} \end{equation}\] <p>The inverse of this transformation is also easily defined:</p> \[\begin{equation} x=\left[\begin{matrix}x_{a}\\ x_{b} \end{matrix}\right]=\ell^{-1}\left(\left[\begin{matrix}y_{a}\\ y_{b} \end{matrix}\right]\right)=\left[\begin{matrix}y_{a}\\ \left(y_{b}-f_{t}\left(y_{a}\right)\right)\odot\exp\left[-f_{s}\left(x_{a}\right)\right] \end{matrix}\right] \end{equation}\] <p>This splitting of the inputs into two makes the transformation invertible <em>but also</em> makes sure that the Jacobian is triangular:</p> \[\begin{equation} \frac{\partial\ell^{-1}\left(y\right)}{\partial y}=\left[\begin{matrix}I &amp; 0\\ \frac{\partial g\left(y_{b}\vert y_{a}\right)}{\partial y_{a}} &amp; \qquad\text{diag}\left(\exp\left[-f_{s}\left(y_{a}\right)\right]\right) \end{matrix}\right] \end{equation}\] <p>where $g\left(y_{b}\vert y_{a}\right)=\left(y_{b}-f_{t}\left(y_{a}\right)\right)\odot\exp\left[-f_{s}\left(x_{a}\right)\right]$. Because of the zero in the top right corner, the determinant of this Jacobian is simply the product of the terms on the diagonal, so we don’t need to bother with actually calculating $\frac{\partial g\left(y_{b}\vert y_{a}\right)}{\partial y_{a}}$, fortunately.</p> <p>Because we could throw away the derivatives in the bottom left corner, we can use functions of any complexity to fit $f_{s}\left(x_{a}\right)$ and $f_{t}\left(x_{a}\right)$. This fact is essential for the flexibility of many of these models.</p> <h3 id="autoregressive-layers">Autoregressive Layers</h3> <p>The above coupling layer was a single split of the input into two parts. A more flexible variant of the above are <em>autoregressive flow layers</em>, which are basically defined the same way, only in an autoregressive manner:</p> \[\begin{equation} \ell\left(x\right)=\ell\left(x_{1},\cdots,x_{d}\right)=\left[\begin{matrix}x_{1}\cdot\exp\left[f\right]+g\\ x_{2}\cdot\exp\left[f\left(x_{1}\right)\right]+g\left(x_{1}\right)\\ x_{3}\cdot\exp\left[f\left(x_{1},x_{2}\right)\right]+g\left(x_{1},x_{2}\right)\\ \vdots\\ x_{d}\cdot\exp\left[f\left(x_{1},\cdots,x_{d-1}\right)\right]+g\left(x_{1},\cdots,x_{d-1}\right) \end{matrix}\right] \end{equation}\] <p>In other words, each coordinate is now a function of all of the coordinates that came before it in some manner. This is, again, invertible but is kind of a pain to write down, so I’ll leave it as it is. The Jacobian is also triangular again, pretty much in the same manner as the coupling layer.</p> <p>The downside of autoregressive layers is that they’re usually quite computationally expensive. Also, their implementation is not straightforward - how do you define the functions $f\left(x_{1},\cdots\right)$ and $g\left(x_{1},\cdots\right)$ in a way that they can take any number of inputs?</p> <p><br></p> <h1 id="continuous-flows"><strong>Continuous Flows</strong></h1> <d-byline></d-byline> <p>The main shortcoming of normalizing flows is that custom-made layers need to be defined for the whole transformation to be (easily) invertible, and sometimes these layers are also computationally expensive.</p> <p>One way we can get around the problem of custom layers is by, ironically, taking the number of layers in the transformation to be infinite. In this regime, instead of defining the whole transformation as a composition of a finite number layers, we need to define a <em>time-based</em> function that smoothly maps from the source distribution to the target. Basically, what we want is something of the following sort:</p> \[\begin{equation} x\left(t+\Delta t\right)=x\left(t\right)+\Delta t\cdot f_{\theta}\left(x\left(t\right),t\right)\label{eq:cont-flow-euler} \end{equation}\] <p>where $\Delta t$ is a short change in time. So instead of defining the transformation itself, what we want to define is actually <em>how $x$ changes over a short amount of time</em>.</p> <p>How does this help us? Well, now we can use <em>any</em> function we want to define $f_{\theta}\left(x,t\right)$, as long as it is smooth in $t$ and $x$. We are no longer constrained to very specific constructions of invertible layers.</p> <h3 id="flows-as-ordinary-differential-equations-odes">Flows as Ordinary Differential Equations (ODEs)</h3> <p>Equation \eqref{eq:cont-flow-euler}, while being intuitive, does not convey the full meaning of what we want. The continuous flows are defined for infinitesimal values of $\Delta t$, so actually we need to look at:</p> \[\begin{equation} \lim_{\Delta t\rightarrow0}\frac{x\left(t+\Delta t\right)-x\left(t\right)}{\Delta t}=\frac{dx}{dt}=f_{\theta}\left(x,t\right) \end{equation}\] <p>That is, $f_{\theta}\left(x,t\right)$ is <em>the gradient of $x$ at time $t$</em>.</p> <h3 id="generating-data-with-continuous-flows">Generating Data with Continuous Flows</h3> <p>To generate data, we start by sampling $x\left(0\right)$ from the source distribution:</p> \[\begin{equation} x\left(0\right)\sim p\left(z\right) \end{equation}\] <p>After this initialization, we have to basically propagate $x\left(0\right)$ along time to see where we end up. That means we also have to define the amount of time we want to wait, $T$, until we assume that the target distribution is reached. Once we have done that, data is generated by solving:</p> \[\begin{equation} x\left(T\right)=x\left(0\right)+\intop_{0}^{T}f_{\theta}\left(x\left(\tau\right),\tau\right)d\tau \end{equation}\] <p>The inverse transformation is basically the same, just starting at $x\left(T\right)$ and going backwards:</p> \[\begin{equation} x\left(0\right)=x\left(T\right)-\intop_{0}^{T}f_{\theta}\left(x\left(\tau\right),\tau\right)d\tau \end{equation}\] <p>Honestly, the above is really hard to understand though. How do you actually solve the integral? Well, one of the simplest ways is to just use the same formula as equation \eqref{eq:cont-flow-euler}. This is sometimes called Euler’s method and is a pretty crude, but simple, method for solving the integral.</p> <h3 id="log-likelihood-calculation">Log-Likelihood Calculation</h3> <p>The above, while including some terrible notation, basically generalized normalizing flows to a continuum of layers. If the time steps are small enough, the log-likelihood of the continuous flows is also quite similar:</p> \[\begin{equation} \log p_{\theta}\left(x\left(T\right)\right)=\log p_{z}\left(x\left(0\right)\right)-\intop_{0}^{T}\text{trace}\left[\frac{\partial f_{\theta}\left(x\left(\tau\right),\tau\right)}{\partial x\left(\tau\right)}\right]d\tau \end{equation}\] <p>The trace of the Jacobian above is very similar to the summation of the diagonal terms in the log-determinants in the regular normalizing flows. Of course, this is still going to be quite computationally intensive to calculate in practice.</p> <p><br></p> <h1 id="problems-with-normalizing-and-continuous-flows"><strong>Problems with Normalizing and Continuous Flows</strong></h1> <d-byline></d-byline> <p>Normalizing flows are a popular class of <em>explicit likelihood</em> generative models. Because the likelihood is baked into the whole definition of normalizing flows, that means that you don’t need to approximate it during inference like VAEs or the models in the next few posts. This fact has made normalizing flows very popular for scientific applications, where likelihood is explicitly needed during inference.</p> <p>Still, normalizing flows are quite limiting. The transformations are either very rigidly defined or are computationally expensive to calculate, in both cases resulting to difficulties in high dimensions. More than just a matter of computational price, normalizing flows typically have a lot of issues regarding numerical stability. These downsides mean that normalizing flows are a bad fit for applications such as computer vision, where high sample quality is king.</p> <p><br></p> <d-byline></d-byline> <p><span style="float:left"><a href="https://friedmanroy.github.io/blog/2024/gen2/">← Variation Methods</a></span><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen4/">DDPM →</a></span></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/primer_generative_biblio.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Roy Friedman. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener noopener noreferrer" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1B05NVC2PJ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1B05NVC2PJ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>