<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="fqpmQwGNtZ1kJ8UAsQMb7RD2N7DcYRDlyGbAJsnZuGM"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Generative Models 0 - What is a Generative Model? | Roy Friedman</title> <meta name="author" content="Roy Friedman"> <meta name="description" content=""> <meta name="keywords" content="academic-website, roy-friedman, machine-learning, ML, bayesian, PhD"> <meta name="google-site-verification" content="EUCyoY6MaSyn4ZvM9TfAuzzeleW7dR0PHS4_UCtyZ4I"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://friedmanroy.github.io/blog/2024/gen0/"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Generative Models 0 - What is a Generative Model?",
      "description": "",
      "published": "August 20, 2024",
      "authors": [
        {
          "author": "Roy Friedman",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Hebrew University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Roy </span>Friedman</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/BML/">Bayesian Machine Learning</a> </li> <li class="nav-item "> <a class="nav-link" href="/_pages/cv/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Generative Models 0 - What is a Generative Model?</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#definition-of-a-generative-model">Definition of a Generative Model</a></div> <div><a href="#training-generative-models">Training Generative Models</a></div> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <p><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen1/">A Linear Model →</a></span> <br></p> <d-byline></d-byline> <p>You are given some data (let’s say it isn’t labeled). This data, you assume, has some significance. For instance, it might be data collected from a manufacturing factory and it could contain information regarding inefficient work lines. Or it contains information regarding the number of animals and their species in certain areas and you think that the number of animals depends on some human structure that was placed next to these areas. Or, this data is a set of CT scans of cancer patients and you think that certain places in the body have higher or lower chances of growing tumors.</p> <p>Whatever the case, you have data that (again, you assume) contains significant information for a scientific question which you want to answer. To aid you in the process of answering these important questions, you want to somehow simulate the process that created the data. In other words, you want to <em>model the generative process of the creation of the data</em>. After all, if you do so well then maybe, just maybe, you’ll be able to meaningfully solve the problem of interest.</p> <p><br></p> <h1 id="definition-of-a-generative-model"><strong>Definition of a Generative Model</strong></h1> <d-byline></d-byline> <p>To be a bit more precise, given the set of data $\mathcal{D}=\left\{x_{i}\right\}_{i=1}^{N}$ where each point $x_{i}$ is assumed to have been sampled from a distribution $p_{\text{data}}\left(x\right)$, we want to find a parametric distribution such that:</p> \[\begin{equation} p_{\theta}\left(x\right)\equiv p\left(x;\theta\right)\approx p_{\text{data}}\left(x\right) \end{equation}\] <p>where $\theta$ are the parameters of the distribution. All of the assumptions and modeling decisions we make with regards to the distribution of the data are hidden inside the notation $\theta$.</p> <h3 id="a-simple-example-kernel-density-estimation-kde">A Simple Example: Kernel Density Estimation (KDE)</h3> <p>The simplest example for a generative model that I can think of is just to fit a Gaussian to the training data. But usually this won’t give us all that much information (and sometimes it will be hard/impossible to do).</p> <p>Beyond the Gaussian, one of the simplest examples of a generative model is <em>kernel density estimation</em> (KDE, also sometimes called <em>Parzen windows</em>). We’ll start by looking at a very basic instance of KDE and then slightly abstract it. This model assumes very little. In fact, the only thing we’ll assume is that points close to the observed data should have high density and that the distribution should decay quickly the further we are from the training points.</p> <p>So, again, let’s assume we have a dataset $\mathcal{D}=\left\{o_{i}\right\}_{i=1}^{N}$ (I’m calling them $o_{i}$ now so it will be less confusing in the next part). We want something that has high density near training points and decays fast. A base distribution that does this is an isotropic Gaussian distribution! So we can just put a lot of Gaussians centered around the training data:</p> \[\begin{equation} p_{\beta}\left(x\right)=\frac{1}{N}\sum_{i=1}^{N}\mathcal{N}\left(x\vert \ o_{i},I\beta\right)\label{eq:RBF-KDE} \end{equation}\] <p>where $\beta&gt;0$ is called the bandwidth and is the same for all points, that is somehow chosen. This <em>is a valid distribution</em> - you can check to make sure that $p_{\beta}\left(x\right)$ integrates to one.</p> <p>This very basic model “smoothes out” the empirical distribution, defined as:</p> \[\begin{equation} p_{\text{emp}}\left(x\right)=\frac{1}{N}\sum_{i=1}^{N}\delta\left(x-o_{i}\right) \end{equation}\] <p>where $\delta\left(\cdot\right)$ is Dirac’s delta, defined as:</p> \[\begin{equation} \delta\left(x\right)=\begin{cases} \infty &amp; x=0\\ 0 &amp; \text{otherwise} \end{cases}\qquad\quad\intop\delta\left(x\right)dx=1 \end{equation}\] <p>Anyway, in equation \eqref{eq:RBF-KDE} the smoothing function was a Gaussian, but there are many other possible choices that we could have made. The smoothing function is usually called the kernel, and it has to be a positive definite (PD) kernel if we want the KDE approximation to be a valid distribution. Given a kernel $K\left(x,o_{i}\right)$, the general KDE distribution is defined as:</p> \[\begin{equation} p_{\text{KDE}}\left(x\right)=\frac{1}{N}\sum_{i=1}^{N}K\left(x,o_{i}\right) \end{equation}\] <p>which is only a valid distribution if:</p> \[\begin{equation} \intop K\left(x,o_{i}\right)dx&lt;\infty \end{equation}\] <p>That said, everyone just uses the RBF kernel, which is the one from equation \eqref{eq:RBF-KDE}. While this seems like a very simple definition, it’s a very good baseline for comparison (which is why I even put it here). This form of KDE is sort of the nearest neighbors equivalent in the world of generative models, since the nearest neighbor will typically have the largest impact on the density of the point $x$ in $p_{\text{KDE}}\left(x\right)$. In this case, the kernel defines the distance function for the nearest neighbors (and is euclidean when using the RBF kernel).</p> <p><br></p> <h1 id="training-generative-models"><strong>Training Generative Models</strong></h1> <d-byline></d-byline> <p>In general, we will want to train our generative models to be as adequate as possible to explain the observed data. Training a generative model in practice amounts to defining a way to compare two models, then choosing the parameters that give the better model under said comparison.</p> <p>Actually, it’s good that I’ve already presented the KDE, that way we’ll have a concrete example of this. Given two possible bandwidths $\beta_{1}$ and $\beta_{2}$, how are we supposed to choose which bandwidth is better suited to use in a KDE for our purposes?</p> <p>Training, or choosing between models, requires a way to compare generative models, as I’ve already said. How do we decide in practice? This depends on what we’re trying to achieve. It might be that we only want samples $x\sim p_{\theta}$ to be similar to samples from the real data $x\sim p_{\text{data}}$, in which case our criteria might be “. Or we could say that there is a set of statistics (say the mean, variance, etc.) gleaned from the observed data which we have to match, otherwise the distribution should be as general as possible. Or, the most popular, that $p_{\theta}\left(x\right)$ should be as close as possible to $p_{\text{data}}\left(x\right)$ under some divergence.</p> <p>Let’s talk about the last one.</p> <h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3> <p>A natural way to evaluate the quality of a model is through the use of some divergence between distributions:</p> \[\begin{equation} \text{error}\left(\theta\right)=D\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta\right)\right) \end{equation}\] <p>Here “divergence” just means some function $D\left(p\vert \vert q\right)$ which is 0 if and only if $\forall x\quad p(x)=q(x)$, and is monotonically increasing as a function of some difference between $p\left(x\right)$ and $q\left(x\right)$. Basically, a divergence is a sort of distance between distributions.</p> <p>The most commonly used divergence is the <em>Kullback-Leibler divergence</em> (KL-divergence) defined as:</p> \[\begin{align} D_{\text{KL}}\left(p\vert \vert q\right) &amp; =\intop p\left(x\right)\log\frac{p\left(x\right)}{q\left(x\right)}dx=\mathbb{E}_{x\sim p}\left[\log p\left(x\right)-\log q\left(x\right)\right]\\ &amp; =-\mathbb{E}_{p}\left[\log q\left(x\right)\right]-H\left(p\right) \end{align}\] <p>where $H\left(p\right)$ is called the <em>(differential) entropy</em> of the distribution $p\left(x\right)$.</p> <p>We want to compare the data distribution to our model’s distribution:</p> \[\begin{equation} D_{\text{KL}}\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta\right)\right)=-\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta\right)\right]-H\left(p_{\text{data}}\right) \end{equation}\] <p>Now, $H\left(p_{\text{data}}\right)$ will be impossible to calculate if we just observe some data, since we don’t actually know the function $p_{\text{data}}\left(\cdot\right)$. However, if all we want to do is compare between two parameterizations $\theta_{1},\theta_{2}\in\Theta$ and to determine which is best, we have to ask:</p> \[\begin{align} D_{\text{KL}}\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta_{1}\right)\right) &amp; \stackrel{?}{&gt;}D_{\text{KL}}\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta_{2}\right)\right)\\ D_{\text{KL}}\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta_{1}\right)\right)-D_{\text{KL}}\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta_{2}\right)\right) &amp; \stackrel{?}{&gt;}0\\ -\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta_{1}\right)\right]+\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta_{2}\right)\right] &amp; \stackrel{?}{&gt;}0\\ \mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta_{1}\right)\right] &amp; \stackrel{?}{&lt;}\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta_{2}\right)\right] \end{align}\] <p>That is, to decide if $p\left(x;\theta_{1}\right)$ is worse than $p\left(x;\theta_{2}\right)$ you don’t even need access to $H\left(p_{\text{data}}\right)$!</p> <p>The term $\log p\left(x;\theta\right)$ is called the <em>log-likelihood</em> of the model $\theta$. What we saw above is that it is enough to find $\theta\in\Theta$ that maximize the expected log-likelihood in order to say that we found the best parameters to describe the distribution (in terms of the KL-divergence):</p> \[\begin{equation} \theta^{\star}=\arg\min_{\theta\in\Theta}D_{\text{KL}}\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta\right)\right)=\arg\max_{\theta\in\Theta}\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta\right)\right] \end{equation}\] <p>This concept is aptly called <em>maximum likelihood estimation</em> (MLE), because we estimate $p_{\text{data}}\left(x\right)$ with the model $p\left(x;\theta^{\star}\right)$ that maximizes the likelihood.</p> <p>Of course, in the real world calculating the expectation is impossible and we only have access to samples from the data distribution, $\mathcal{D}$. Also, researchers in machine learning are used to talking about losses (or errors). So usually the <em>negative log-likelihood</em> (NLL) loss is used to train generative models:</p> \[\begin{equation} L\left(\theta\right)=-\frac{1}{N}\sum_{x_{i}\in\mathcal{D}}\log p\left(x_{i};\theta\right)\approx-\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta\right)\right] \end{equation}\] <h3 id="difficulties-with-mle">Difficulties with MLE</h3> <p>MLE is the <em>most</em> popular way to train generative models, since it seems like such a natural criteria. After all, even without the whole thing with the KL-divergence, maximizing the log-likelihood of the training examples alone already seems like a good idea. So our criteria is simple to understand and instead training generative models is difficult because it’s hard in many cases to exactly calculate the log-likelihood $\log p\left(x;\theta\right)$.</p> <p>For instance, the most popular form of generative models in computer vision work in the following way:</p> \[\begin{equation} z\sim p\left(z\right)\stackrel{G_{\theta}}{\mapsto}x\in\mathcal{X}\label{eq:latent-space-model} \end{equation}\] <p>In words, there is a <em>latent code</em> $z\in\mathcal{Z}$ that is sampled according to a distribution $p\left(z\right)$, which is then mapped to the image space $\mathcal{X}$ using a (always complicated) function $G_{\theta}\left(z\right)$. If we write this out as a probability function, we would say that:</p> \[\begin{equation} p_{\theta}\left(z,x\right)=p\left(z\right)p_{\theta}\left(x\vert z\right) \end{equation}\] <p>In this case, to calculate the log-likelihood of the model we need to solve the following integral:</p> \[\begin{equation} p_{\theta}\left(x\right)=\intop p\left(z\right)p_{\theta}\left(x\vert z\right)dz \end{equation}\] <p>If $G_{\theta}\left(z\right)$ is anything beyond linear, then calculating the above integral will be pretty much impossible.</p> <p>The bottom line is that training generative models almost always involves maximizing the log-likelihood. To actually do so, we will need to use tricks and approximations in order to calculate the log-likelihood in almost every case.</p> <details><summary>A note about naming</summary> <p>Generative models that have a latent space that is mapped to the data space are sometimes called <em>decoder-based generative models</em> because they use a decoder to describe how data is generated. As I mentioned above, they are very common in computer vision, so now is a good time to get acquainted with some of the terminology.</p> <p>The naming convention of these generative models is a bit weird. Many times, the distribution over the latent codes $p\left(z\right)$ is called the <em>prior distribution</em>. Seemingly, these models follow from Bayesian statistics. If that is the case, $p_{\theta}\left(x\vert z\right)$ should be called the likelihood, but isn’t and instead $p_{\theta}\left(x\vert z\right)$ is usually called the <em>observation model</em> or <em>observation probability</em> or something like that. The distribution $p_{\theta}\left(z\vert x\right)$ is the <em>posterior distribution</em> and finally $p_{\theta}\left(x\right)$ is either the <em>likelihood, marginal likelihood</em> or the <em>evidence</em>. This is a weird mishmash of terms from Bayesian statistics that isn’t fully faithful to the Bayesian interpretation.</p> <p>The way to think about everything so it will make sense is this: given a specific decoder $G_{\theta}:\mathcal{Z}\rightarrow\mathcal{X}$, which we have no influence over, the distribution over $z$ basically defines a prior distribution over the possible $x$s. However, in the real world we don’t directly see $x$, there’s usually some noise involved in the observation - think about photos (especially when it’s dark), they tend to have some grain or noise. So, what we observe in the real world is more like $x=G_{\theta}\left(z\right)+\text{noise}$, which explains the distribution $p_{\theta}\left(x\vert z\right)$ and why it’s called the <em>observation model</em>. The name of the probability $p_{\theta}\left(x\right)$ has to match both Bayesian statistics and generative models in general, so it’s called either the <em>evidence</em> or <em>likelihood</em>, interchangeably.</p> </details> <p><br></p> <h1 id="conclusion"><strong>Conclusion</strong></h1> <d-byline></d-byline> <p>Generative models, the main focus of this series of posts, are very popular. This popularity comes from their unprecedented performance in generating data from computer vision and natural language processing, most of which is very recent. Because of this sudden acceleration in the quality of samples generated from such models, there has been a surge of research into generative models.</p> <p>The purpose of this primer is to give a good foundation to learn about more complex methods, which I hope will be accessible. In the next post in this series we’ll explore a really simple generative model, which should give a kind of starting point into exploring the more complex, state of the art models.</p> <p><br></p> <d-byline></d-byline> <p><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen1/">A Linear Model →</a></span></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/primer_generative_biblio.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Roy Friedman. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener noopener noreferrer" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1B05NVC2PJ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1B05NVC2PJ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>