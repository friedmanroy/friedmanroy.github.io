<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://friedmanroy.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://friedmanroy.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-25T11:41:27+00:00</updated><id>https://friedmanroy.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Generative Models 3 - Normalizing Flows</title><link href="https://friedmanroy.github.io/blog/2024/gen3/" rel="alternate" type="text/html" title="Generative Models 3 - Normalizing Flows"/><published>2024-08-21T00:00:00+00:00</published><updated>2024-08-21T00:00:00+00:00</updated><id>https://friedmanroy.github.io/blog/2024/gen3</id><content type="html" xml:base="https://friedmanroy.github.io/blog/2024/gen3/"><![CDATA[<p><span style="float:left"><a href="https://friedmanroy.github.io/blog/2024/gen2/">← Variation Methods</a></span><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen4/">DDPM →</a></span> <br/></p> <d-byline></d-byline> <p>So, VAEs were kind of a headache. If we think back and try to find out why, the reasons were: the latent space was smaller than the data space, adding an observation model means we need an integral to go back to the latent space, and we have no direct way to “invert” the generator. Other than that, VAEs are very general and the only constraints are those imposed implicitly by the encoder/decoder.</p> <p>Instead, we can try to add back into the mix stronger constraints, but in a way that makes sure that the generator is invertible. If we manage to do this, we can go back from data space to the latent space at will. Normalizing flows <d-cite key="papamakarios2021normalizing"></d-cite> are the method to do this. <br/></p> <h1 id="when-the-generator-is-invertible"><strong>When the Generator is Invertible</strong></h1> <d-byline></d-byline> <p>Again, we assume $z\sim p_{z}\left(z\right)$ and now we will assume $x=G_{\theta}\left(z\right)$ and $\text{dim}\left(z\right)=\text{dim}\left(x\right)$. That is, no observation noise. If $G_{\theta}\left(z\right)$ is completely bijective (one-to-one and onto), then the probability of $x$ can be written in terms of the probability of $z$:</p> \[\begin{equation} p_{\theta}\left(x\right)=p_{z}\left(G_{\theta}^{-1}\left(x\right)\right)\left\vert \text{det}\left[\frac{\partial G_{\theta}^{-1}\left(x\right)}{\partial x}\right]\right\vert \end{equation}\] <p>where $\partial G_{\theta}^{-1}\left(x\right)/\partial x$ is the Jacobian of the inverse mapping $z=G_{\theta}^{-1}\left(x\right)$. So we can just directly use the MLE criterion to train $G_{\theta}\left(z\right)$:</p> \[\begin{equation} L\left(\theta\right)=-\frac{1}{N}\sum_{i=1}^{N}\log p_{\theta}\left(x_{i}\right)=-\frac{1}{N}\sum_{i=1}^{N}\left(\log p_{z}\left(G_{\theta}^{-1}\left(x_{i}\right)\right)+\log\left\vert \text{det}\left[\frac{\partial G_{\theta}^{-1}\left(x_{i}\right)}{\partial x_{i}}\right]\right\vert \right)\label{eq:change-MLE} \end{equation}\] <p>We are, of course, going to use multiple layers to define our mapping, so that:</p> \[\begin{equation} x=G_{\theta}\left(z\right)=\ell_{L}\circ\ell_{L-1}\circ\cdots\circ\ell_{1}\left(z\right)\Leftrightarrow z=G_{\theta}^{-1}\left(x\right)=\ell_{1}^{-1}\circ\ell_{2}^{-1}\circ\cdots\circ\ell_{L}^{-1}\left(x\right) \end{equation}\] <p>where $L$ is the number of layers we use and $\ell_{i}\left(\cdot\right)$ is the $i$-th layer. Let’s call $y_{i}=\ell_{i}^{-1}\left(y_{i+1}\right)$ the output of the inverse of $i$-th layer with $y_{L+1}=x$, then we can rewrite equation \eqref{eq:change-MLE} as:</p> \[\begin{equation} L\left(\theta\right)=-\frac{1}{N}\sum_{i=1}^{N}\left(\log p_{z}\left(G_{\theta}^{-1}\left(x_{i}\right)\right)+\sum_{j=1}^{L}\log\left\vert \text{det}\left[\frac{\partial\ell_{j}^{-1}\left(y_{j+1}\right)}{\partial y_{j+1}}\right]\right\vert \right) \end{equation}\] <p>through the chain rule. In other words, if we know the log-absolute-determinant of each layer individually, we just have to calculate them on the fly.</p> <p>The question now becomes: how can we define $\ell_{i}\left(\cdot\right)$ so that it is invertible <em>and</em> it’s easy to calculate the determinant of the Jacobian?</p> <p><br/></p> <h1 id="invertible-layers"><strong>Invertible Layers</strong></h1> <d-byline></d-byline> <p>To ensure that the determinant is easily calculable, we essentially have three types of transformations we can use:</p> <ol> <li>Orthogonal transformations whose determinant is equal to 1 (or -1). These include things like a rotation around the origin, translation or a change of basis (like Fourier). Another very common orthogonal transformation (that is invertible) is a random shuffle of the coordinates.</li> <li>Transformations where each coordinate is changed independently, making the Jacobian diagonal in which case the determinant is the product of said diagonal.</li> <li>Transformations where some coordinates remain unchanged, creating a triangular Jacobian. If the Jacobian is triangular then the determinant is again the product of the terms on the diagonal.</li> </ol> <p>There exist more types of transformations and ways to make this even more confusing, but these are the basics so we’ll stay with them. At any rate, in each of these cases calculating the determinant is feasible and not much slower than just a usual forward pass through the network.</p> <h3 id="independent-coordinate-transformations">Independent Coordinate Transformations</h3> <p>We can use <em>any</em> element-wise transformation that we want, as long as the derivative for each coordinate is simple to calculate.</p> <p>The easiest invertible transformations we can start thinking about are of the following sort:</p> \[\begin{equation} \ell\left(x\right)=e^{s}\odot x+b\quad x,s,b\in\mathbb{R}^{d}\quad\Leftrightarrow\ell^{-1}\left(y\right)=e^{-s}\odot\left(y-b\right) \end{equation}\] <p>In this case, the inversion is very easy to describe. Also, we ensure that the layers are always invertible by using an element-wise multiplication of $x$ with $e^{s}$ instead of $s$ directly - this ensures that $\left[e^{s}\right]_{i}&gt;0$ is always true. The Jacobian of the inverse of this transformation is simply $\text{diag}\left(e^{-s}\right)$, which is really easy to calculate on the fly. Sometimes this transformation is called an <em>ActNorm layer</em> and is initialized so that $e^{-s}=1/\sigma^{2}$ where $\sigma^{2}$ is the variance of the incoming signal and $b$ is equal to the mean, essentially standardizing the incoming signals.</p> <p>Another useful transformation is one that moves everything in the range $\left(-\infty,\infty\right)$ to $\left(0,1\right)$, i.e. the softmax:</p> \[\begin{equation} \text{sig}\left(x_{i}\right)=\frac{1}{1+e^{-x_{i}}}\Leftrightarrow\text{sig}^{-1}\left(y_{i}\right)=\log\left(\frac{1-y_{i}}{y_{i}}\right) \end{equation}\] <p>The derivative of this transformation is given by:</p> \[\begin{equation} \frac{d}{dx_{i}}\text{sig}\left(x_{i}\right)=\text{sig}\left(x_{i}\right)\cdot\left(1-\text{sig}\left(x_{i}\right)\right) \end{equation}\] <p>which is also pretty simple to calculate on the fly.</p> <h3 id="coupling-layers">Coupling Layers</h3> <p>While element-wise transformations are great, they don’t allow the model to learn anything regarding the correlations (or higher order statistics) between coordinates. Really, we need transformations that mix up the information. The most commonly used types of layers that act in this manner are called <em>affine coupling layers</em>. These layers are defined as follows:</p> \[\begin{equation} \ell\left(\left[\begin{matrix}x_{a}\\ x_{b} \end{matrix}\right]\right)=\left[\begin{matrix}x_{a}\\ x_{b}\odot\exp\left[f_{s}\left(x_{a}\right)\right]+f_{t}\left(x_{a}\right) \end{matrix}\right]\qquad x_{a},x_{b}\in\mathbb{R}^{d/2} \end{equation}\] <p>The inverse of this transformation is also easily defined:</p> \[\begin{equation} x=\left[\begin{matrix}x_{a}\\ x_{b} \end{matrix}\right]=\ell^{-1}\left(\left[\begin{matrix}y_{a}\\ y_{b} \end{matrix}\right]\right)=\left[\begin{matrix}y_{a}\\ \left(y_{b}-f_{t}\left(y_{a}\right)\right)\odot\exp\left[-f_{s}\left(x_{a}\right)\right] \end{matrix}\right] \end{equation}\] <p>This splitting of the inputs into two makes the transformation invertible <em>but also</em> makes sure that the Jacobian is triangular:</p> \[\begin{equation} \frac{\partial\ell^{-1}\left(y\right)}{\partial y}=\left[\begin{matrix}I &amp; 0\\ \frac{\partial g\left(y_{b}\vert y_{a}\right)}{\partial y_{a}} &amp; \qquad\text{diag}\left(\exp\left[-f_{s}\left(y_{a}\right)\right]\right) \end{matrix}\right] \end{equation}\] <p>where $g\left(y_{b}\vert y_{a}\right)=\left(y_{b}-f_{t}\left(y_{a}\right)\right)\odot\exp\left[-f_{s}\left(x_{a}\right)\right]$. Because of the zero in the top right corner, the determinant of this Jacobian is simply the product of the terms on the diagonal, so we don’t need to bother with actually calculating $\frac{\partial g\left(y_{b}\vert y_{a}\right)}{\partial y_{a}}$, fortunately.</p> <p>Because we could throw away the derivatives in the bottom left corner, we can use functions of any complexity to fit $f_{s}\left(x_{a}\right)$ and $f_{t}\left(x_{a}\right)$. This fact is essential for the flexibility of many of these models.</p> <h3 id="autoregressive-layers">Autoregressive Layers</h3> <p>The above coupling layer was a single split of the input into two parts. A more flexible variant of the above are <em>autoregressive flow layers</em>, which are basically defined the same way, only in an autoregressive manner:</p> \[\begin{equation} \ell\left(x\right)=\ell\left(x_{1},\cdots,x_{d}\right)=\left[\begin{matrix}x_{1}\cdot\exp\left[f\right]+g\\ x_{2}\cdot\exp\left[f\left(x_{1}\right)\right]+g\left(x_{1}\right)\\ x_{3}\cdot\exp\left[f\left(x_{1},x_{2}\right)\right]+g\left(x_{1},x_{2}\right)\\ \vdots\\ x_{d}\cdot\exp\left[f\left(x_{1},\cdots,x_{d-1}\right)\right]+g\left(x_{1},\cdots,x_{d-1}\right) \end{matrix}\right] \end{equation}\] <p>In other words, each coordinate is now a function of all of the coordinates that came before it in some manner. This is, again, invertible but is kind of a pain to write down, so I’ll leave it as it is. The Jacobian is also triangular again, pretty much in the same manner as the coupling layer.</p> <p>The downside of autoregressive layers is that they’re usually quite computationally expensive. Also, their implementation is not straightforward - how do you define the functions $f\left(x_{1},\cdots\right)$ and $g\left(x_{1},\cdots\right)$ in a way that they can take any number of inputs?</p> <p><br/></p> <h1 id="continuous-flows"><strong>Continuous Flows</strong></h1> <d-byline></d-byline> <p>The main shortcoming of normalizing flows is that custom-made layers need to be defined for the whole transformation to be (easily) invertible, and sometimes these layers are also computationally expensive.</p> <p>One way we can get around the problem of custom layers is by, ironically, taking the number of layers in the transformation to be infinite. In this regime, instead of defining the whole transformation as a composition of a finite number layers, we need to define a <em>time-based</em> function that smoothly maps from the source distribution to the target. Basically, what we want is something of the following sort:</p> \[\begin{equation} x\left(t+\Delta t\right)=x\left(t\right)+\Delta t\cdot f_{\theta}\left(x\left(t\right),t\right)\label{eq:cont-flow-euler} \end{equation}\] <p>where $\Delta t$ is a short change in time. So instead of defining the transformation itself, what we want to define is actually <em>how $x$ changes over a short amount of time</em>.</p> <p>How does this help us? Well, now we can use <em>any</em> function we want to define $f_{\theta}\left(x,t\right)$, as long as it is smooth in $t$ and $x$. We are no longer constrained to very specific constructions of invertible layers.</p> <h3 id="flows-as-ordinary-differential-equations-odes">Flows as Ordinary Differential Equations (ODEs)</h3> <p>Equation \eqref{eq:cont-flow-euler}, while being intuitive, does not convey the full meaning of what we want. The continuous flows are defined for infinitesimal values of $\Delta t$, so actually we need to look at:</p> \[\begin{equation} \lim_{\Delta t\rightarrow0}\frac{x\left(t+\Delta t\right)-x\left(t\right)}{\Delta t}=\frac{dx}{dt}=f_{\theta}\left(x,t\right) \end{equation}\] <p>That is, $f_{\theta}\left(x,t\right)$ is <em>the gradient of $x$ at time $t$</em>.</p> <h3 id="generating-data-with-continuous-flows">Generating Data with Continuous Flows</h3> <p>To generate data, we start by sampling $x\left(0\right)$ from the source distribution:</p> \[\begin{equation} x\left(0\right)\sim p\left(z\right) \end{equation}\] <p>After this initialization, we have to basically propagate $x\left(0\right)$ along time to see where we end up. That means we also have to define the amount of time we want to wait, $T$, until we assume that the target distribution is reached. Once we have done that, data is generated by solving:</p> \[\begin{equation} x\left(T\right)=x\left(0\right)+\intop_{0}^{T}f_{\theta}\left(x\left(\tau\right),\tau\right)d\tau \end{equation}\] <p>The inverse transformation is basically the same, just starting at $x\left(T\right)$ and going backwards:</p> \[\begin{equation} x\left(0\right)=x\left(T\right)-\intop_{0}^{T}f_{\theta}\left(x\left(\tau\right),\tau\right)d\tau \end{equation}\] <p>Honestly, the above is really hard to understand though. How do you actually solve the integral? Well, one of the simplest ways is to just use the same formula as equation \eqref{eq:cont-flow-euler}. This is sometimes called Euler’s method and is a pretty crude, but simple, method for solving the integral.</p> <h3 id="log-likelihood-calculation">Log-Likelihood Calculation</h3> <p>The above, while including some terrible notation, basically generalized normalizing flows to a continuum of layers. If the time steps are small enough, the log-likelihood of the continuous flows is also quite similar:</p> \[\begin{equation} \log p_{\theta}\left(x\left(T\right)\right)=\log p_{z}\left(x\left(0\right)\right)-\intop_{0}^{T}\text{trace}\left[\frac{\partial f_{\theta}\left(x\left(\tau\right),\tau\right)}{\partial x\left(\tau\right)}\right]d\tau \end{equation}\] <p>The trace of the Jacobian above is very similar to the summation of the diagonal terms in the log-determinants in the regular normalizing flows. Of course, this is still going to be quite computationally intensive to calculate in practice.</p> <p><br/></p> <h1 id="problems-with-normalizing-and-continuous-flows"><strong>Problems with Normalizing and Continuous Flows</strong></h1> <d-byline></d-byline> <p>Normalizing flows are a popular class of <em>explicit likelihood</em> generative models. Because the likelihood is baked into the whole definition of normalizing flows, that means that you don’t need to approximate it during inference like VAEs or the models in the next few posts. This fact has made normalizing flows very popular for scientific applications, where likelihood is explicitly needed during inference.</p> <p>Still, normalizing flows are quite limiting. The transformations are either very rigidly defined or are computationally expensive to calculate, in both cases resulting to difficulties in high dimensions. More than just a matter of computational price, normalizing flows typically have a lot of issues regarding numerical stability. These downsides mean that normalizing flows are a bad fit for applications such as computer vision, where high sample quality is king.</p> <p><br/></p> <d-byline></d-byline> <p><span style="float:left"><a href="https://friedmanroy.github.io/blog/2024/gen2/">← Variation Methods</a></span><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen4/">DDPM →</a></span></p>]]></content><author><name>Roy Friedman</name></author><category term="machine-learning"/><category term="generative-models"/><summary type="html"><![CDATA[← Variation MethodsDDPM →]]></summary></entry><entry><title type="html">Generative Models 4 - Denoising Diffusion Probabilistic Models</title><link href="https://friedmanroy.github.io/blog/2024/gen4/" rel="alternate" type="text/html" title="Generative Models 4 - Denoising Diffusion Probabilistic Models"/><published>2024-08-21T00:00:00+00:00</published><updated>2024-08-21T00:00:00+00:00</updated><id>https://friedmanroy.github.io/blog/2024/gen4</id><content type="html" xml:base="https://friedmanroy.github.io/blog/2024/gen4/"><![CDATA[<p><span style="float:left"><a href="https://friedmanroy.github.io/blog/2024/gen3/">← Normalizing Flows</a></span><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen5/">Score-Based Models →</a></span> <br/></p> <d-byline></d-byline> <details><summary>Disclaimer</summary> <p>The math in the previous posts pretty much followed what you’d see in the literature, although there were small changes so it would be easier to understand. In the next two sections, however, the math and explanations are very different to what you would usually see.</p> <p>Why? Well, the explanations for both DDPM and score-based models are very involved, have a lot of disgusting math and are super hard to get, <em>intuitively</em>. My main goal was to help you <em>understand</em>, not just to copy down the equations you’d see in literature. So I followed the natural progression of the previous posts, which means that notations, steps and sometimes results are a bit different. Hopefully this enables you to go and learn on your own the exact details.</p> <p>Much of the following is a reframing of <d-cite key="luo2022understanding"></d-cite>, which deserves a lot of credit. There are particular parts I decided to expand into or cut out, but I generally followed their setup for DDPM. The parts regarding score-based models, though, are completely different and more fleshed out.</p> </details> <p>We can take what we saw before with VAEs and extend them. Instead of assuming that $z \mapsto x$ in one step, we will now assume that there are many steps from $z$ to $x$. We will define the steps intelligently so we know what happens from one point to the next, and then train something using ELBO to take us from one point to the next.</p> <p><br/></p> <h1 id="the-setting"><strong>The Setting</strong></h1> <d-byline></d-byline> <p>Let’s suppose we have the following chain:</p> \[\begin{equation} x=x_{0}\rightarrow x_{1}\rightarrow\cdots\rightarrow x_{T-1}\rightarrow x_{T}=z \end{equation}\] <p>where <span>$x\sim p_{\text{data}}$</span> and $z\sim p_{T}\left(z\right)$, where $p_{T}\left(z\right)$ is many times just a Gaussian. Up until now we assumed that we have access to the mapping $z\mapsto x$ and had to “guess” or optimize the reverse mapping. This time, we’re gonna assume that we know how to turn $x$ into $z$, so we know $x\mapsto z$, and we want to find the opposite direction. This class of models is called <em>denoising diffusion probabilistic models</em> (DDPMs, <d-cite key="ho2020denoising"></d-cite>) for reasons that will become clear later on.</p> <p>Let’s call the noising direction the forward process, whose transition probabilities we will assume to be:</p> \[\begin{equation} q\left(x_{t}\vert x_{t-1}\right)=\mathcal{N}\left(x_{t}\vert \;\gamma_{t}x_{t-1},I\sigma_{t}^{2}\right) \end{equation}\] <p>where $\gamma_{t}$ is some constant and $\sigma_{t}^{2}$ is the variance of the noise added at step $t$. Both values are assumed to change as a function of the iteration. We want to learn a model of the reverse process, that is we want to learn $p_{\theta}\left(x_{t-1}\vert x_{t}\right)$. We’re going to guess that:</p> \[\begin{equation} q\left(x_{t-1}\vert x_{t}\right)\approx p_{\theta}\left(x_{t-1}\vert x_{t}\right)=\mathcal{N}\left(x_{t-1}\vert \;\mu_{\theta}\left(x_{t},t\right),I\sigma_{t-1}^{2}\right) \end{equation}\] <p>If this is how we model the “reverse process”, then sampling for this model will be quite easy: simply sample $x_{T}\sim p_{T}\left(z\right)$ and then use $p_{\theta}\left(x_{t-1}\vert x_{t}\right)$ to sample until you reach $x_{0}$. This is basically the following algorithm:</p> \[\begin{equation} x_{T}\sim p_{T}\left(z\right)\qquad\forall0\le t\le T-1:\;x_{t-1}=\mu_{\theta}\left(x_{t},t\right)+\sigma_{t-1}\epsilon\label{eq:DDPM-sampling} \end{equation}\] <p>where $\epsilon\sim\mathcal{N}\left(0,I\right)$.</p> <p>To train $p_{\theta}\left(x_{t-1}\vert x_{T}\right)$, what we want is high likelihood under “our guess”. The log-likelihood of an image, according to our model, is given by:</p> \[\begin{align} \log p_{\theta}\left(x_{0}\right) &amp; =\mathbb{E}_{p_{\theta}\left(x_{1},\cdots,x_{T}\right)}\left[\log p_{\theta}\left(x_{0},\cdots,x_{T}\right)\right] \end{align}\] <p>this will of course be quite hard to calculate, never mind optimize. Instead, we will use the same variational bound as we used for VAEs:</p> \[\begin{align} \log p_{\theta}\left(x_{0}\right) &amp; \ge\mathbb{E}_{q\left(x_{1},\cdots,x_{T}\vert x_{0}\right)}\left[\log\frac{p_{\theta}\left(x_{0},\cdots,x_{T}\right)}{q\left(x_{1},\cdots,x_{T}\vert x_{0}\right)}\right]\label{eq:DDPM-ELBO} \end{align}\] <details><summary>Scheduling choices</summary> <p>Because we kept everything abstract as possible so far, there is one thing that isn’t clear. How can we ensure the following:</p> \[\begin{equation} q\left(x_{T}\vert x_{0}\right)\overset{?}{=}p_{T}\left(z\right) \end{equation}\] <p>Notice that, as we defined it, the transition probability of the Markov chain $q\left(\cdot\vert \cdot\right)$ gives us the following:</p> \[\begin{equation} x_{t}=\gamma_{t}x_{t-1}+\sigma_{t}\epsilon_{t}\qquad\epsilon_{t}\sim\mathcal{N}\left(0,I\right) \end{equation}\] <p>and this can be opened up recursively:</p> \[\begin{align} x_{t} &amp; =\gamma_{t}\gamma_{t-1}x_{t-2}+\gamma_{t}\sigma_{t-1}\epsilon_{t-1}+\sigma_{t}\epsilon_{t}\\ \Rightarrow x_{t} &amp; =\gamma_{t}\gamma_{t-1}\gamma_{t-2}x_{t-3}+\gamma_{t}\gamma_{t-1}\sigma_{t-2}\epsilon_{t-2}+\gamma_{t}\sigma_{t-1}\epsilon_{t-1}+\sigma_{t}\epsilon_{t}\\ &amp; \vdots\\ \Rightarrow x_{t} &amp; =\left(\prod_{i=1}^{t}\gamma_{t}\right)x_{0}+\sum_{i=1}^{t}\prod_{j=i+1}^{t}\gamma_{j}\sigma_{i}\epsilon_{i}\label{eq:unrolled-DDPM-q} \end{align}\] <p>Let’s make our life slightly easier and call $\prod_{i}^{t}\gamma_{i}=\bar{\gamma}_{t}$ and $\bar{\sigma}_{t}^{2}=\sum_{i}^{t}\prod_{j=i+1}^{t}\gamma_{j}^{2}\sigma_{i}^{2}$. At any rate, $x_{t}$ is some vector dependent on $x_{0}$ plus a sum of Gaussians - the distribution $q\left(x_{t}\vert x_{0}\right)$ will also be a Gaussian:</p> \[\begin{equation} q\left(x_{t}\vert x_{0}\right)=\mathcal{N}\left(x_{t}\vert \;\bar{\gamma}_{t}x_{0},\;I\bar{\sigma}_{t}^{2}\right) \end{equation}\] <p>So if we want $q\left(x_{T}\vert x_{0}\right)=p_{T}\left(z\right)$ then we have to make sure that $p_{T}\left(z\right)$ is Gaussian and that for any $x_{0}$ we have:</p> \[\begin{equation} \mathbb{E}_{p_{T}}\left[z\right]=\bar{\gamma}_{T}x_{0} \end{equation}\] <p>and:</p> \[\begin{equation} \text{cov}_{p_{T}}\left[z\right]=I\bar{\sigma}_{T}^{2} \end{equation}\] <p>The most obvious choice is to use $p_{T}\left(z\right)=\mathcal{N}\left(z\vert 0,I\right)$, in which case we just need to ensure that $\bar{\gamma}_{T}\stackrel{T\rightarrow\infty}{\longrightarrow}0$ and $\bar{\sigma}_{T}\stackrel{T\rightarrow\infty}{\longrightarrow}1$.</p> </details> <p><br/></p> <h1 id="a-simple-variant-of-ddpm"><strong>A Simple Variant of DDPM</strong></h1> <d-byline></d-byline> <p>There are two ways to proceed, simple or confusing. The confusing method is what (of course) is actually used, but we’ll start with the simple method.</p> <p>We’ll start by rewriting both $p_{\theta}\left(x_{0},\cdots,x_{T}\right)$ and $q\left(x_{1},\cdots,x_{T}\vert x_{0}\right)$ into smaller factorizations. We can do this because both are Markov chains according to their construction, enabling us to rewrite them as:</p> \[\begin{align} p_{\theta}\left(x_{0},\cdots,x_{T}\right) &amp; =p_{\theta}\left(x_{0}\vert x_{1}\right)\cdot p_{\theta}\left(x_{1}\vert x_{2}\right)\cdots p_{\theta}\left(x_{T-1}\vert x_{T}\right)\cdot p_{\theta}\left(x_{T}\right)\\ q\left(x_{1},\cdots,x_{T}\vert x_{0}\right) &amp; =q\left(x_{1}\vert x_{0}\right)\cdot q\left(x_{2}\vert x_{1}\right)\cdots q\left(x_{T}\vert x_{T-1}\right) \end{align}\] <p>See how those line up nicely? Let’s put them back into equation \eqref{eq:DDPM-ELBO}:</p> \[\begin{align} \log p_{\theta}\left(x_{0}\right) &amp; \ge\mathbb{E}_{q\left(x_{1},\cdots,x_{T}\vert x_{0}\right)}\left[\log p_{\theta}\left(x_{T}\right)+\sum_{t=1}^{T}\log p_{\theta}\left(x_{t}\vert x_{t+1}\right)-\sum_{t=1}^{T}\log q\left(x_{t}\vert x_{t-1}\right)\right]\\ &amp; =\mathbb{E}_{q\left(x_{1},\cdots,x_{T}\vert x_{0}\right)}\left[\log p_{\theta}\left(x_{T}\right)+\log p_{\theta}\left(x_{0}\vert x_{1}\right)-\log q\left(x_{T}\vert x_{T-1}\right)\right.\\&amp;\qquad\qquad\qquad\qquad\left.+\sum_{t=1}^{T-1}\log\frac{p_{\theta}\left(x_{t}\vert x_{t+1}\right)}{q\left(x_{t}\vert x_{t-1}\right)}\right]\\ &amp; =\overbrace{\mathbb{E}_{q\left(x_{1}\vert x_{0}\right)}\left[\log p_{\theta}\left(x_{0}\vert x_{1}\right)\right]}^{\text{reconstruction term}}-\sum_{t=1}^{T-1}\mathbb{E}_{q\left(x_{t+1},x_{t-1}\vert x_{0}\right)}\overbrace{\left[D_{\text{KL}}\left(q\left(x_{t}\vert x_{t-1}\right)\,\vert \vert \,p_{\theta}\left(x_{t}\vert x_{t+1}\right)\right)\right]}^{\text{Markov chain term}}\\ &amp; \hfill\qquad\qquad\hfill\quad-\mathbb{E}_{q\left(x_{T-1}\vert x_{0}\right)}\underbrace{\left[D_{\text{KL}}\left(q\left(x_{T}\vert x_{T-1}\right)\,\vert \vert \,p_{\theta}\left(x_{T}\right)\right)\right]}_{\text{prior term}} \end{align}\] <p>We now have a lower bound on the log-likelihood. There are three terms of interest:</p> <ol> <li>The reconstruction term, which is basically the same as the reconstruction error we saw in the standard VAEs</li> <li>The KL-divergences inside the Markov chain term, which make up the bulk of the training in diffusion models. These KL terms try to ensure that if we go forward in the Markov chain, using $q\left(\cdot\vert \cdot\right)$ starting at $x_{0}$, or backward, using $p_{\theta}\left(\cdot\vert \cdot\right)$ starting at $x_{T}$, we’ll get the same distribution</li> <li>The prior term, which tries to make sure that are prior distribution in the latent space is correct. Notice that we assumed that $p_{\theta}\left(x_{T}\right)$ has a fixed distribution so there’s nothing to optimize here during training</li> </ol> <p>To optimize the lower bound, what we essentially need to do is make sure that for each $t$ the KL-divergence is low. Ignoring the reconstruction term for a moment, if we want to maximize the above lower bound, we just need to optimize the following loss:</p> \[\begin{equation} L\left(\theta\right)=\mathbb{E}_{t,x_{0},q\left(x_{t+1},x_{t-1}\vert x_{0}\right)}\left[D_{\text{KL}}\left(q\left(x_{t}\vert x_{t-1}\right)\,\vert \vert \,p_{\theta}\left(x_{t}\vert x_{t+1}\right)\right)\right] \end{equation}\] <p>Of course, we chose both $q\left(x_{t}\vert x_{t-1}\right)$ and $p_{\theta}\left(x_{t}\vert x_{t+1}\right)$ to be Gaussian distributions (with the same covariance even!), so we know what the KL-divergence equals:</p> \[\begin{equation} \Rightarrow L\left(\theta\right)=\mathbb{E}_{t,x_{0},q\left(x_{t+1},x_{t-1}\vert x_{0}\right)}\left[\frac{1}{\sigma_{t}}\| \mu_{\theta}\left(x_{t+1},t\right)-\gamma_{t}x_{t-1}\| ^{2}\right] \end{equation}\] <p>What a simple loss!</p> <p>So, why is this loss not usually used? The reason is that to calculate the loss at any point we need to make two MC estimates:</p> \[\begin{align} x_{t-1} &amp; \sim q\left(x_{t-1}\vert x_{0}\right)\\ x_{t+1} &amp; \sim q\left(x_{t+1}\vert x_{t-1}\right) \end{align}\] <p>The cited reason is then that this has more variance than the (soon to be seen) complex version.</p> <p>At any rate, the intuition for diffusion models is the same even in the complex setting.</p> <p><br/></p> <h1 id="ddpm-breakdown"><strong>DDPM Breakdown</strong></h1> <d-byline></d-byline> <p>We are now going to break down $q\left(x_{1},\cdots,x_{T}\vert x_{0}\right)$ in a strange way:</p> \[\begin{align} q\left(x_{1},\cdots,x_{T}\vert x_{0}\right) &amp; =q\left(x_{T}\vert x_{0}\right)q\left(x_{T-1}\vert x_{T},x_{0}\right)q\left(x_{T-2}\vert x_{T},x_{T-1},x_{0}\right)\cdots q\left(x_{1}\vert x_{T},\cdots,x_{2},x_{0}\right) \end{align}\] <p>Because of the fact that $q\left(x_{1},\cdots,x_{T}\vert x_{0}\right)$ is a Markov chain, given both $x_{0}$ and $x_{t}$ is enough to completely describe $x_{t-1}$, so the above can be rewritten as:</p> \[\begin{equation} q\left(x_{1},\cdots,x_{T}\vert x_{0}\right)=q\left(x_{T}\vert x_{0}\right)q\left(x_{T-1}\vert x_{T},x_{0}\right)\cdots q\left(x_{1}\vert x_{2},x_{0}\right) \end{equation}\] <p>Using this factorization, we now have:</p> \[\begin{align} \log p_{\theta}\left(x_{0}\right) &amp; \ge\mathbb{E}_{q\left(x_{1},\cdots,x_{T}\vert x_{0}\right)}\left[\log p_{\theta}\left(x_{0}\vert x_{1}\right)+\sum_{t=1}^{T}\frac{\log p_{\theta}\left(x_{t-1}\vert x_{t}\right)}{\log q\left(x_{t-1}\vert x_{t},x_{0}\right)}+\log\frac{p_{\theta}\left(x_{T}\right)}{q\left(x_{T}\vert x_{0}\right)}\right]\\ &amp; =\mathbb{E}_{q\left(x_{1}\vert x_{0}\right)}\left[\log p_{\theta}\left(x_{0}\vert x_{1}\right)\right]-\sum_{t=1}^{T}\mathbb{E}_{q\left(x_{t}\vert x_{0}\right)}\left[D_{\text{KL}}\left(q\left(x_{t-1}\vert x_{t},x_{0}\right)\vert \vert p_{\theta}\left(x_{t-1},x_{t}\right)\right)\right]-D_{\text{KL}}\left(q\left(x_{T}\vert x_{0}\right)\vert \vert p_{\theta}\left(x_{T}\right)\right) \end{align}\] <p>For now, let’s ignore that first term. Notice that in the last term we have nothing to optimize, so even if we train a model we won’t have to take it into account. We’ll now divert our attention to the middle expression which is the bulk of the lower bound.</p> <h3 id="the-really-nasty-part">The Really Nasty Part</h3> <p>As we saw from the breakdown that led to equation \eqref{eq:unrolled-DDPM-q}, because of our choice for the forward process, $q\left(x_{t}\vert x_{0}\right)$ is a Gaussian whose mean and covariance we know explicitly. This is, first of all, good to know… but we want $q\left(x_{t-1}\vert x_{t},x_{0}\right)$. Using Bayes’ law:</p> \[\begin{equation} q\left(x_{t-1}\vert x_{t},x_{0}\right)\propto q\left(x_{t}\vert x_{t-1}\right)q\left(x_{t-1}\vert x_{0}\right) \end{equation}\] <p>Basically, $q\left(x_{t-1}\vert x_{t},x_{0}\right)$ is the multiplication of two Gaussian distributions (which are linear in $x_{t-1}$), which means that it will also be Gaussian.</p> <p>After massaging this expression for a while, you’ll find out that:</p> \[\begin{equation} q\left(x_{t-1}\vert x_{t},x_{0}\right)=\mathcal{N}\left(x_{t-1}\vert \;a_{t}x_{0}+b_{t}x_{t},\;I\kappa_{t}^{2}\right) \end{equation}\] <p>where $a_{t}$, $b_{t}$ and $\kappa_{t}$ are some constants that depend on $t$ which don’t really matter for now. You can look below for their exact values, but the bottom line is that $q\left(x_{t-1}\vert x_{t},x_{0}\right)$ is also Gaussian and we can find an analytical expression for it.</p> <details><summary>Finding the Gaussian</summary> <p>Given $x_{0}$, $x_{t}$ and $x_{t-1}$ are jointly Gaussian:</p> \[\begin{equation} p\left(x_{t},x_{t-1}\vert x_{0}\right)=\mathcal{N}\left(\left(\begin{matrix}x_{t}\\ x_{t-1} \end{matrix}\right)\vert \quad\left(\begin{matrix}\bar{\gamma}_{t}x_{0}\\ \bar{\gamma}_{t-1}x_{0} \end{matrix}\right),\Sigma\right) \end{equation}\] <p>with:</p> \[\begin{equation} \Sigma=\left(\begin{matrix}I\bar{\sigma}_{t}^{2} &amp; I\sigma_{t}^{2}\\ I\sigma_{t}^{2} &amp; I\bar{\sigma}_{t-1}^{2} \end{matrix}\right) \end{equation}\] <p>Using identities for conditionals of jointly Gaussian distributions<d-footnote>See my <a href="https://friedmanroy.github.io/BML/3_gaussians">BML notes for this</a></d-footnote>, we have:</p> \[\begin{align} \mathbb{E}\left[x_{t-1}\vert x_{t},x_{0}\right] &amp; =\bar{\gamma}_{t-1}x_{0}+\frac{\sigma_{t}^{2}}{\bar{\sigma}_{t}^{2}}\left(x_{t}-\bar{\gamma}_{t}x_{0}\right)\\ &amp; =\left(\bar{\gamma}_{t-1}-\frac{\sigma_{t}^{2}}{\bar{\sigma}_{t}^{2}}\bar{\gamma}_{t}\right)x_{0}+\frac{\sigma_{t}^{2}}{\bar{\sigma}_{t}^{2}}x_{t}\\ \text{cov}\left[x_{t-1}\vert x_{t},x_{0}\right] &amp; =I\left(\bar{\sigma}_{t-1}^{2}-\frac{\sigma_{t}^{4}}{\bar{\sigma}_{t}^{2}}\right)=I\left(\bar{\sigma}_{t-1}^{2}-\frac{\sigma_{t}^{2}}{\bar{\sigma}_{t-1}^{2}}\right) \end{align}\] <p>Define:</p> \[\begin{align} a_{t} &amp; =\bar{\gamma}_{t-1}-\frac{\sigma_{t}^{2}}{\bar{\sigma}_{t}^{2}}\bar{\gamma}_{t}\\ b_{t} &amp; =\frac{\sigma_{t}^{2}}{\bar{\sigma}_{t}^{2}}\\ \kappa_{t}^{2} &amp; =\bar{\sigma}_{t-1}^{2}-\frac{\sigma_{t}^{2}}{\bar{\sigma}_{t-1}^{2}} \end{align}\] <p>Then: \(\begin{equation} q\left(x_{t-1}\vert x_{t},x_{0}\right)=\mathcal{N}\left(x_{t-1}\vert \;a_{t}x_{0}+b_{t}x_{t},\;I\kappa_{t}^{2}\right) \end{equation}\)</p> </details> <h3 id="the-training-loss-finally">The Training Loss, Finally</h3> <p>Remember, what we wanted was to find the terms $D\left(q\left(x_{t-1}\vert x_{t},x_{0}\right)\vert \vert p_{\theta}\left(x_{t-1}\vert x_{t}\right)\right)$. We now have all we need to actually state what the loss at time point $t$ is actually equal to, since the KL-divergence between two Gaussians has a closed-form equation. The KL divergence between Gaussians is given by:</p> \[\begin{align} D_{\text{KL}}\left(\mathcal{N}\left(x\vert \mu_{x},\Sigma_{x}\right)\vert \vert \mathcal{N}\left(y\vert \mu_{y},\Sigma_{y}\right)\right)&amp;=\frac{1}{2}\left[\log\left\vert \Sigma_{y}\Sigma_{x}^{-1}\right\vert +\text{trace}\left(\Sigma_{y}^{-1}\Sigma_{x}\right)+\right. \\ &amp;\left.\left(\mu_{y}-\mu_{x}\right)^{T}\Sigma_{y}^{-1}\left(\mu_{y}-\mu_{x}\right)-\text{dim}\left(x\right)\right] \end{align}\] <p>In our setting, the covariances aren’t learnable parameters, and the dimension is constant, so we can just ignore everything that doesn’t correspond to the means. Our training loss is then:</p> \[\begin{equation} L\left(\theta\right)=\mathbb{E}_{x_{0},\,t,\,q\left(x_{t}\vert x_{0}\right)}\left[\frac{1}{\kappa_{t}^{2}}\| \mu_{\theta}\left(x_{t},t\right)-a_{t}x_{0}-b_{t}x_{t}\| ^{2}\right] \end{equation}\] <p>Notice that as we defined it, our model $\mu_{\theta}\left(x_{t},t\right)$ already depends on $x_{t}$. We can maybe improve our model a bit by making sure that it learns something that takes this information into account. If we choose:</p> \[\begin{equation} \mu_{\theta}\left(x_{t},t\right)=a_{t}s_{\theta}\left(x_{t},t\right)+b_{t}x_{t} \end{equation}\] <p>then we can get rid of one of the terms in the loss to get:</p> \[\begin{equation} L\left(\theta\right)=\mathbb{E}_{x_{0},\,t,\,q\left(x_{t}\vert x_{0}\right)}\left[\frac{a_{t}^{2}}{\kappa_{t}^{2}}\| s_{\theta}\left(x_{t},t\right)-x_{0}\| ^{2}\right] \end{equation}\] <p>This, finally, explains the name “<em>denoising</em> diffusion” - all we want from our model is to denoise the inputs! Given a point $x_{t}$, our model basically tries to guess what $x_{0}$ was, and the loss is weighted by the variance we expect to see.</p> <p><br/></p> <h1 id="when-is-the-variational-bound-tight"><strong>When is the Variational Bound Tight?</strong></h1> <d-byline></d-byline> <p>When we talked about VAEs, we said that the ELBO is tight when:</p> \[\begin{equation} q_{\phi}\left(z\vert x\right)=p_{\theta}\left(z\vert x\right) \end{equation}\] <p>Exactly the same considerations are true for diffusion models, where the ELBO is tight when:</p> \[\begin{equation} \forall t\quad p_{\theta}\left(x_{t-1}\vert x_{t}\right)=q\left(x_{t-1}\vert x_{t}\right) \end{equation}\] <p>But, unlike VAEs, choosing $p_{\theta}\left(x_{t-1}\vert x_{t}\right)$ as a Gaussian distribution is much more principled. It can be shown (I’ll give a semblance of a proof in the next post) that if we have enough steps with small enough differences between them (i.e. $\sigma_{t}\rightarrow0$) in the forward process, then the reverse process will also be a Markov chain with Gaussian transitions. In other words, if we design the forward process to have enough steps with small enough variances, then our modeling for the reverse process is correct.</p> <p><br/></p> <h1 id="conclusion"><strong>Conclusion</strong></h1> <d-byline></d-byline> <p>The original DDPM paper <d-cite key="ho2020denoising"></d-cite> made quite a splash, since training it was much simpler than GANs (more on them later on) but the sample quality was very good. These results, paired with the simple denoising loss we found, were a big surprise, and consequently there was (and still is, at the time of writing) <em>a lot</em> of research into diffusion models.</p> <p>Still the naive DDPM (as described here) is still a bit lacking as a generative model. This is mainly due to the fact that every operation (whether generation or inference) requires iterating over the whole Markov chain, which can be quite slow. As described here, getting good results requires Markov chains with around 1000 steps, which is like denoising 1000 images <em>sequentially</em>. The next post will focus on a way to generalize DDPMs into score-based generative models, which will ultimately enable faster sampling and easier control over design choices. <br/></p> <d-byline></d-byline> <p><span style="float:left"><a href="https://friedmanroy.github.io/blog/2024/gen3/">← Normalizing Flows</a></span><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen5/">Score-Based Models →</a></span></p>]]></content><author><name>Roy Friedman</name></author><category term="machine-learning"/><category term="generative-models"/><summary type="html"><![CDATA[← Normalizing FlowsScore-Based Models →]]></summary></entry><entry><title type="html">Generative Models 5 - Score-Based Generative Models</title><link href="https://friedmanroy.github.io/blog/2024/gen5/" rel="alternate" type="text/html" title="Generative Models 5 - Score-Based Generative Models"/><published>2024-08-21T00:00:00+00:00</published><updated>2024-08-21T00:00:00+00:00</updated><id>https://friedmanroy.github.io/blog/2024/gen5</id><content type="html" xml:base="https://friedmanroy.github.io/blog/2024/gen5/"><![CDATA[<p><span style="float:left"><a href="https://friedmanroy.github.io/blog/2024/gen4/">← DDPM</a> </span><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen6/">Energy-Based Models →</a></span></p> <p><br/></p> <d-byline></d-byline> <blockquote> <p>While informative, the DDPM (or variational) view of diffusion models is quite limiting. It is limiting in the sense that it is hard to get any theoretical intuition regarding ways to improve the generative model. For instance, the variational bound is only tight when we use $T\rightarrow\infty$ and $\sigma_{t}\rightarrow0$. Because of this, $T\approx1000$ steps need to be used to get meaningful results, a high computational load. How can we improve on this?</p> <p>By slightly changing our perspective, we might be able to get a better understanding leading to better results.</p> </blockquote> <d-byline></d-byline> <p>Score-based generative models (SBGMs, <d-cite key="song2020score"></d-cite>) slightly predate DDPM, but the two are actually pretty much equivalent. Still, I’m gonna start with SBGMs as if they’re completely different models and later we’ll see exactly how/if the two are related to each other.</p> <p><br/></p> <h1 id="sampling-with-langevin-dynamics"><strong>Sampling with Langevin Dynamics</strong></h1> <d-byline></d-byline> <p>The backbone of SBGMs is sampling according to Langevin dynamics, a fairly intuitive algorithm that’s used very often. Given some distribution $p\left(x\right)$, the following iterative scheme is called <a href="https://friedmanroy.github.io/blog/2022/Langevin/">the Langevin dynamics (LD) algorithm</a><d-footnote>I'm assuming that the sampling procedure goes backwards in time, even though this isn't standard, just to remain consistent with what we saw before with DDPM</d-footnote>:</p> \[\begin{equation} x_{t-\Delta t}=x_{t}+\frac{\Delta t}{2}\cdot\nabla_{x_{t}}\log p\left(x_{t}\right)+\sqrt{\Delta t}\eta_{t} \end{equation}\] <p>where $\eta_{t}\sim\mathcal{N}\left(0,I\right)$. It can be shown (we won’t do that here) that this algorithm samples from $p\left(x_{t}\right)$ as long as $\Delta t\rightarrow0$ and it is run for many iterations.</p> <p>This algorithm is very very general and applicable in many situations, which is great. We’ll want to use this algorithm to sample new data points, which means that we’ll have to learn $s_{\theta}\left(x\right)\approx\nabla_{x}\log p\left(x\right)$, i.e. the <em>score function</em> of the distribution $p\left(x\right)$. Of course, if we don’t have access to $\log p\left(x\right)$, this will be rather difficult - let’s make our life a bit easier.</p> <p><br/></p> <h1 id="tweedie-formula"><strong>Tweedie Formula</strong></h1> <d-byline></d-byline> <p>How are we going to learn $\nabla_{x}\log p\left(x\right)$? Well, let’s start by thinking about a seemingly completely unrelated problem.</p> <p>Suppose we observe a noisy image:</p> \[\begin{equation} x\sim p\left(x\right)\qquad y=x+\eta\quad\eta\sim\mathcal{N}\left(0,I\sigma^{2}\right) \end{equation}\] <p>This noisy image $y$ is sampled from a different distribution than $x$, basically a “noisy” version of $p\left(x\right)$. We can define this distribution exactly in the following way:</p> \[\begin{equation} p\left(y;\sigma^{2}\right)=\intop p\left(y\vert x;\sigma^{2}\right)p\left(x\right)dx=\intop\mathcal{N}\left(y\vert \,x,I\sigma^{2}\right)p\left(x\right)dx \end{equation}\] <p>Obviously the image $y$ is less probably than $x$ under $p\left(\cdot\right)$, right? But we’ll also expect it to be a little less probable under the noisy distribution, $p\left(\cdot;\sigma^{2}\right)$. That is, it is intuitive to believe that $p\left(y;\sigma^{2}\right)&lt;p\left(x;\sigma^{2}\right)$. Then, if we have a <em>really really</em> good denoiser $D\left(y;\sigma^{2}\right)$ that is able to denoise $y$ and return something that is basically $x$, then this’ll give us some indication as to the direction $\delta$ that we need to move $y$ in order for $p\left(y+\delta;\sigma^{2}\right)$ to increase. So, maybe, a really good denoiser will give us some information regarding the gradient:</p> \[\begin{equation} D\left(y;\sigma^{2}\right)\stackrel{?}{\approx}y+c\nabla_{y}\log p\left(y;\sigma^{2}\right) \end{equation}\] <p>where $c$ is some constant.</p> <p>This simplistic intuition turns out to be correct, and is called <em>Tweedie’s formula</em>.</p> <blockquote> <p><strong>Theorem</strong> (Tweedie’s Formula, see <d-cite key="efron2011tweedie"></d-cite> for more):</p> <p>Let $p\left(y;\sigma^{2}\right)$ be as defined above. Then:</p> </blockquote> \[\begin{equation} \nabla_{y}\log p\left(y;\sigma^{2}\right)=\frac{\mathbb{E}\left[x\vert y\right]-y}{\sigma^{2}} \end{equation}\] <details><summary>Click here to see the proof</summary> <p>For the proof, we’ll just go at it directly:</p> \[\begin{align} \nabla_{y}\log p\left(y;\sigma^{2}\right) &amp; =\frac{1}{p\left(y;\sigma^{2}\right)}\frac{\partial}{\partial y}p\left(y;\sigma^{2}\right)\\ &amp; =\frac{1}{p\left(y;\sigma^{2}\right)}\frac{\partial}{\partial y}\intop\mathcal{N}\left(y\vert \,x,I\sigma^{2}\right)p\left(x\right)dx\\ &amp; =\frac{1}{p\left(y;\sigma^{2}\right)}\intop p\left(x\right)\frac{\partial}{\partial y}\mathcal{N}\left(y\vert \,x,I\sigma^{2}\right)dx \end{align}\] <p>where the last step is possible because the integration variable is not $y$. We will now use the fact that gradient of a Gaussian function is given by:</p> \[\begin{equation} \frac{\partial}{\partial y}\mathcal{N}\left(y\vert \,x,I\sigma^{2}\right)=\mathcal{N}\left(y\vert \,x,I\sigma^{2}\right)\cdot\left[-\frac{1}{\sigma^{2}}\left(y-x\right)\right] \end{equation}\] <p>Plugging this back in:</p> \[\begin{align} \nabla_{y}\log p\left(y;\sigma^{2}\right) &amp; =\frac{1}{\sigma^{2}}\intop\frac{p\left(x\right)\mathcal{N}\left(y\vert \,x,I\sigma^{2}\right)}{p\left(y;\sigma^{2}\right)}\left[x-y\right]dx\\ &amp; =\frac{1}{\sigma^{2}}\intop p\left(x\vert y;\sigma^{2}\right)\left[x-y\right]dx\\ &amp; =\frac{1}{\sigma^{2}}\left[\mathbb{E}\left[x\vert y\right]-y\right] \end{align}\] <p>where the first step above was due to Bayes’ law. <span style="float:right"> $\square$ </span></p> </details> <p>Amazing! We now have a way to access the score of a distribution… well, almost. We can now access the score of the <em>noisy</em> distribution $p\left(y;\sigma^{2}\right)$, <em>not</em> the score of the original distribution $p\left(x\right)$. Of course, we’re not going to let that stop us. <br/></p> <h1 id="score-matching"><strong>Score Matching</strong></h1> <d-byline></d-byline> <p>In a moment we’ll talk about how to actually use the noisy scores, but let’s focus on the fact that we now have a roundabout way to access the score. What we should above is that we basically only need to learn a really good denoiser in order to know the score of the noisy distribution. After all<d-footnote>For a more thorough explanation of this equality, see <a href="https://friedmanroy.github.io/BML/2_estimates/">my summary of decision theory</a>.</d-footnote>:</p> \[\begin{equation} \mathbb{E}\left[x\vert y\right]=\arg\min_{\hat{x}\left(y\right)}\mathbb{E}_{x\sim p\left(x\right),y\sim p\left(y\vert x;\sigma^{2}\right)}\left[\| \hat{x}\left(y\right)-x\| ^{2}\right] \end{equation}\] <p>So, if all we have is a dataset $\mathcal{D}=\left{ x_{i}\right} _{i=1}^{N}$ of samples from $p\left(x\right)$, what we want to do is train a denoiser with the following loss:</p> \[\begin{equation} L\left(\theta\right)=\mathbb{E}_{x\sim\mathcal{D},\eta\sim\mathcal{N}\left(0,I\sigma^{2}\right)}\left[\| D_{\theta}\left(x+\eta;\sigma^{2}\right)-x\| ^{2}\right] \end{equation}\] <p>and then our model for the score will be:</p> \[\begin{equation} s\left(y;\sigma^{2}\right)\approx s_{\theta}\left(y;\sigma^{2}\right)=\frac{D\left(y;\sigma^{2}\right)-y}{\sigma^{2}} \end{equation}\] <p>Training a model in this way is called <em>score matching</em> (<d-cite key="hyvarinen2005estimation"></d-cite>) because we’re matching our model’s score to that of the data. In this case, we’re not exactly learning the score of the distribution itself, but of the “noisy” distribution (<d-cite key="song2019generative"></d-cite>), but as we’ll see it’s enough to use this noisy score to sample from the distribution. Specifically, if we do this when $\sigma$ is really small, then maybe the noisy distribution is close enough to the true distribution and:</p> \[\begin{equation} s\left(y;\sigma^{2}\right)\approx s\left(x\right) \end{equation}\] <p><br/></p> <h1 id="sampling-with-noisy-scores"><strong>Sampling with Noisy Scores</strong></h1> <d-byline></d-byline> <p>We learned our $s\left(y;\sigma_{\text{0}}^{2}\right)$ for some small value of $\sigma_{0}\approx0$, but there’s a problem. LD really samples from the true distribution when $\Delta t\rightarrow0$, but in real life we’ll use larger values than 0 for $\Delta t$. Since $\Delta t$ is larger than 0, then noise is effectively added to our distribution. This is most clear to see if we look at the process without the score and going forwards in time:</p> \[\begin{equation} x_{t+\Delta t}=x_{t}+\sqrt{\Delta t}\eta_{t} \end{equation}\] <p>It’ll be best if we take this added noise into account during sampling. Using the same tricks we had in DDPM, given $x_{0}$ we know how much noise was added up to time $t$:</p> \[\begin{equation} x_{t}=x_{0}+\sqrt{\sum_{n=1}^{t/\Delta t}n\Delta t}\cdot\eta=x_{0}+\sqrt{\frac{t}{\Delta t}\Delta t}\cdot\eta\qquad\eta\sim\mathcal{N}\left(0,I\right) \end{equation}\] <p>So if we want to take into account the fact that at different stages of the Markov chain we’ll effectively see different amount of noise, we should actually train our score model for every possible noise value (I’m going to assume $0\le t\le T$ and that $\Delta t\ll T$):</p> \[\begin{equation} L\left(\theta\right)=\mathbb{E}_{t\in[0,T],x\in\mathcal{D},\eta}\left[\| D_{\theta}\left(x+\sqrt{t}\eta;\;\max\left(\sigma_{0}^{2},t\right)\right)-x\| ^{2}\right] \end{equation}\] <p>Having trained $D_{\theta}\left(y;\sigma^{2}\right)$ in this manner, our sampling algorithm might look something like the following:</p> <ol> <li>Sample $x_{T}\sim\mathcal{N}\left(0,I\cdot T\right)$</li> <li> <p>for $t=T,\left(T-\Delta t\right),\left(T-2\Delta t\right),\cdots,\Delta t$:</p> <p>(a) $\eta\sim\mathcal{N}\left(0,I\right)$</p> <p>(b) $x_{t-\Delta t}=x_{t}+\Delta t\cdot\frac{D_{\theta}\left(x_{t};\;t\right)-x_{t}}{t}+\sqrt{\Delta t}\cdot\eta$</p> </li> <li>return $x_0$</li> </ol> <p>We now have a very simple algorithm for generating images. Also, notice, we trained a generative model without maximizing the log-likelihood! How strange.</p> <blockquote> <p>The above is just the intuition behind the reverse sampling procedure, but it turns out that we can show mathematically that this is the true reverse process. I left an overview of the steps to show this below, but they’re a bit involved.</p> </blockquote> <details><summary>Reverse and forward processes</summary> <p>What we did above is, basically, defining the forward and reverse processes just like in DDPM. By defining that given a clean image $x_{0}$ the distribution at time $t$ is given by:</p> \[\begin{equation} x_{t}=x_{0}+\sqrt{t}\cdot\eta_{t} \end{equation}\] <p>we defined the forward chain given by $p\left(x_{t}\vert x_{0}\right)$.</p> <p>Having defined the forward process, we now want to know what $p\left(x_{t-\Delta t}\vert x_{t}\right)$ is equal. Actually finding the reverse process is fairly difficult. Instead, I’ll try to give some intuition as to why this might be correct (this is an adaptation of the derivation per <d-cite key="nakkiran2024step"></d-cite>). If $\Delta t\rightarrow0$, then the difference between $p\left(x_{t};t\right)$ and $p\left(x_{t-\Delta t};t-\Delta t\right)$ should be very small. At the same time, $x_{t}$ and $x_{t-\Delta t}$ are also going to be fairly similar in most cases, with:</p> \[\begin{equation} \mathbb{E}_{x_{t-\Delta t}}\left[\| x_{t}-x_{t-\Delta t}\| ^{2}\right]=\Delta t \end{equation}\] <p>Because we are looking at the limit of $\Delta t\rightarrow0$, we’re eventually going to ignore all terms that are of a $\Delta t$ factor. To indicate this, we’ll write, for instance, that $| x_{t}-x_{t-\Delta t}| ^{2}=O\left(\Delta t\right)$. We are now going to do a couple of consecutive Taylor approximations that are accurate at the limit $\Delta t\rightarrow0$:</p> \[\begin{equation} p\left(x_{t-\Delta t};t-\Delta t\right)\stackrel{\text{Taylor}}{=}p\left(x_{t-\Delta t};t\right)+\Delta t\cdot\frac{\partial}{\partial t}p\left(x_{t-\Delta t};t\right)+O\left(\Delta t\right) \end{equation}\] <p>I’m going to want to write down the logarithm of this distribution, so we are going to use another Taylor series:</p> \[\begin{align} &amp; \log p \left(x_{t-\Delta t};t-\Delta t\right) =\log\left[p\left(x_{t-\Delta t};t\right)+\Delta t\cdot\frac{\partial}{\partial t}p\left(x_{t-\Delta t};t\right)+O\left(\Delta t\right)\right]\\ &amp; =\log p\left(x_{t-\Delta t};t\right)+\log\left[1+\Delta t\frac{\partial_{t}p\left(x_{t-\Delta t};t\right)+O\left(1\right)}{p\left(x_{t-\Delta t};t\right)}\right]\\ &amp; =\log p\left(x_{t-\Delta t};t\right)+\log\left[1+\Delta t\left(\frac{\partial}{\partial t}\log p\left(x_{t-\Delta t};t\right)+O\left(1\right)\right)\right]\\ &amp; \stackrel{\text{Taylor}}{=}\log p\left(x_{t-\Delta t};t\right)+\Delta t\left(\frac{\partial}{\partial t}\log p\left(x_{t-\Delta t};t\right)\right)+O\left(\Delta t\right) \end{align}\] <p>Here we’re going to make an assumption: $\log p\left(\cdot;t\right)$ changes smoothly as a function of the time $t$. Concretely, we will say that there exists some constant $k$ such that $\log p\left(\cdot;t\right)$ is $k$-Lipschitz. If this assumption holds then we can say that:</p> \[\begin{equation} \log p\left(x_{t-\Delta t};t-\Delta t\right)\stackrel{\text{Taylor}}{=}\log p\left(x_{t-\Delta t};t\right)+O\left(\Delta t\right) \end{equation}\] <p>That was (in essence) one Taylor approximation in the time domain. We will now do the same in the spatial domain:</p> \[\begin{align} \log p\left(x_{t-\Delta t};t\right) &amp; \stackrel{\text{Taylor}}{=}\log p\left(x_{t};t\right) \\ &amp; +\left(x_{t-\Delta t}-x_{t}\right)^{T}\nabla_{x_{t}}\log p\left(x_{t};t\right) \\ &amp; +\left(x_{t-\Delta t}-x_{t}\right)^{T}\frac{\partial^{2}\log p\left(x_{t};t\right)}{\partial x_{t}^{2}}\left(x_{t-\Delta t}-x_{t}\right) \\ &amp; +\cdots \end{align}\] <p>Once again we’re going to have to make an assumption: $\log p\left(x_{t};t\right)$ changes slowly enough for all of it’s second order derivatives to be bounded. If this is true, we can write:</p> \[\begin{equation} \log p\left(x_{t-\Delta t};t\right)\stackrel{\text{Taylor}}{=}\log p\left(x_{t};t\right)+\left(x_{t-\Delta t}-x_{t}\right)^{T}s\left(x_{t};t\right)+O\left(\| x_{t-\Delta t}-x_{t}\| ^{2}\right) \end{equation}\] <p>Following from this, we can write:</p> \[\begin{equation} \log p\left(x_{t-\Delta t};t-\Delta t\right)\stackrel{\text{Taylor}\times3}{=}\log p\left(x_{t};\,t\right)+\left(x_{t-\Delta t}-x_{t}\right)^{T}s\left(x_{t};t\right)+O\left(\Delta t\right) \end{equation}\] <p>Using Bayes’ law:</p> \[\begin{align} \log p\left(x_{t-\Delta t}\vert x_{t}\right) &amp; \propto\log p\left(x_{t}\vert x_{t-\Delta t}\right)+\log p\left(x_{t-\Delta t};t-\Delta t\right)\\ &amp; =-\frac{1}{2\Delta t}\| x_{t-\Delta t}-x_{t}\| ^{2}+x_{t-\Delta t}^{T}s\left(x_{t};t\right)+O\left(\Delta t\right)+\text{const} \end{align}\] <p>At $\Delta t\rightarrow0$, this is a log-quadratic function with respect to $x_{t-\Delta t}$, so we know that $p\left(x_{t-\Delta t}\vert x_{t}\right)$ is a Gaussian distribution. To find the mean and variance, we can either complete the squares or differentiate the log-density. We’ll take the second option:</p> \[\begin{align} \frac{\partial}{\partial x_{t-\Delta t}}-\log p\left(x_{t-\Delta t}\vert x_{t}\right)&amp;=\frac{1}{\Delta t}\left(x_{t-\Delta t}-x_{t}\right)-s\left(x_{t};t\right)\\&amp;=\underbrace{\frac{1}{\Delta t}}_{\text{cov}\left[x_{t-\Delta t}\vert x_{t}\right]^{-1}}\left(x_{t-\Delta t}-\underbrace{\left(x_{t}+\Delta t\cdot s\left(x_{t};t\right)\right)}_{\mathbb{E}\left[x_{t-\Delta t}\vert x_{t}\right]}\right) \end{align}\] <p>So. What we found is that:</p> \[\begin{equation} p\left(x_{t-\Delta t}\vert x_{t}\right)\stackrel{\Delta t\rightarrow0}{\longrightarrow}\mathcal{N}\left(x_{t-\Delta t}\vert \quad x_{t}+\Delta t\cdot s\left(x_{t};t\right),\quad I\Delta t\right) \end{equation}\] </details> <p><br/></p> <h1 id="changing-the-scheduling-of-the-noise"><strong>Changing the Scheduling of the Noise</strong></h1> <d-byline></d-byline> <p>As defined so far, the amount of noise at time $t$ that we “expect” to see in our samples is:</p> \[\begin{equation} \sigma_{t}^{2}=t \end{equation}\] <p>But we can generalize this and choose a different noising-schedule.</p> <p>Suppose that at each iteration we add $g^{2}\left(t\right)\cdot\Delta t$ noise instead of just $\Delta t$ noise, giving the following forward process:</p> \[\begin{equation} x_{t+\Delta t}=x_{t}+\sqrt{\Delta t}\cdot g\left(t\right)\cdot\eta_{t} \end{equation}\] <p>Using the same arguments as above, the total amount of noise we’ll add (that is, we’re not taking into account the score) is:</p> \[\begin{equation} x_{t}=x_{0}+\sqrt{\sum_{n=1}^{t/\Delta t}g^{2}\left(n\cdot\Delta t\right)\Delta t}\cdot\eta\qquad\eta\sim\mathcal{N}\left(0,I\right) \end{equation}\] <p>The term $\sum_{n=1}^{t/\Delta t}g^{2}\left(n\cdot\Delta t\right)\Delta t$ is hard to understand in it’s current form. But actually, if we take $\Delta t\rightarrow0$, this is basically the Riemannian sum of $g\left(t\right)$. So at time $t$ we actually have:</p> \[\begin{equation} \lim_{\Delta t\rightarrow0}\sum_{n=1}^{t/\Delta t}g^{2}\left(n\cdot\Delta t\right)\Delta t=\intop_{0}^{t}g^{2}\left(\tau\right)d\tau=\sigma^{2}\left(t\right) \end{equation}\] <p>Doing this is like “dilating” time a bit. Instead of moving $\Delta t$ at each iteration, the process moves $g^{2}\left(t\right)\Delta t$ at each iteration, which we have to somehow take into account in the reverse process as well. This means that our sampling algorithm now becomes:</p> \[\begin{equation} x_{t-\Delta t}=x_{t}+\Delta t\cdot g^{2}\left(t\right)\cdot s_{\theta}\left(x_{t};\;\sigma^{2}\left(t\right)\right)+\sqrt{\Delta t}\cdot g\left(t\right)\cdot\eta_{t}\label{eq:scheduled-SGBM} \end{equation}\] <p>We are now completely free to choose the schedule of added noise $g\left(t\right)$ in any way we wish. Or, instead, we can decide to define the total noise we added $\sigma\left(t\right)$ instead, by noticing that:</p> \[\begin{equation} \sigma^{2}\left(t\right)=\intop_{0}^{t}g^{2}\left(\tau\right)d\tau\Rightarrow g^{2}\left(t\right)=\frac{d}{dt}\sigma^{2}\left(t\right)=2\sigma\left(t\right)\frac{d}{dt}\sigma\left(t\right) \end{equation}\] <p>which means we can rewrite equation \eqref{eq:scheduled-SGBM} as:</p> \[\begin{equation} x_{t-\Delta t}=x_{t}+\Delta t\cdot2\sigma\left(t\right)\frac{d}{dt}\sigma\left(t\right)\cdot s_{\theta}\left(x_{t};\;\sigma^{2}\left(t\right)\right)+\sqrt{\Delta t\cdot2\sigma\left(t\right)\frac{d}{dt}\sigma\left(t\right)}\cdot\eta_{t} \end{equation}\] <p><br/></p> <h1 id="continuous-time"><strong>Continuous Time</strong></h1> <d-byline></d-byline> <p>The last few sections of this summary, we assumed that $\Delta t\rightarrow0$. This means that we more or less assume that for any time point $t$, the random variable $x\left(t\right)$ has some distribution. Instead of writing the update rule, as we’ve done so far, we can recast everything into differential equations.</p> <p>For a moment, let’s ignore the noise we’re adding at each step of the sampling procedure. In such a case, we have the following update step:</p> \[\begin{equation} x_{t-\Delta t}=x_{t}+\Delta t\cdot g^{2}\left(t\right)\cdot s_{\theta}\left(x_{t};\;\sigma^{2}\left(t\right)\right) \end{equation}\] <p>We can move $x_{t}$ to the left-hand-side of the equation and divide by $\Delta t$ to get something that looks an awful lot like a derivative<d-footnote>The minus sign in the following is because we so consistently insisted that the sampling procedure should happen in the "reverse process". In other words, we had this weird insistence that going backwards in time is the direction we need to go in order to ""reverse the noising procedure"". This was just a choice, we could've defined everything in the more intuitive direction and be rid of this minus sign, but it's here to stay now.</d-footnote>:</p> \[\begin{equation} \frac{x_{t}-x_{t-\Delta t}}{\Delta t}=-g^{2}\left(t\right)\cdot s_{\theta}\left(x_{t};\;\sigma^{2}\left(t\right)\right)\label{eq:continuous-SGBM} \end{equation}\] <p>Taking the limit of $\Delta t\rightarrow0$, this is exactly the definition of an <em>ordinary differential equation</em> (ODE) given by:</p> \[\begin{equation} \frac{dx}{dt}=-g^{2}\left(t\right)\cdot s_{\theta}\left(x\left(t\right);\;\sigma^{2}\left(t\right)\right) \end{equation}\] <p>Of course, when we sampled we also added noise. The typical notation for a <em>stochastic differential equation</em> (SDE) defined in equation \eqref{eq:scheduled-SGBM} is (using something called Ito calculus notation, this reverse process was shown/proven in <d-cite key="song2020score"></d-cite>):</p> \[\begin{equation} dx=-\underbrace{g^{2}\left(t\right)\cdot s_{\theta}\left(x\left(t\right);\;\sigma^{2}\left(t\right)\right)\cdot dt}_{\text{deterministic (drift)}}+\underbrace{g\left(t\right)dW\left(t\right)}_{\text{stochastic (diffusion)}} \end{equation}\] <p>where $dW\left(t\right)$ is called a <em>Wiener process</em> and is defined in the following way:</p> \[\begin{equation} dx=g\left(t\right)dW\left(t\right)\Leftrightarrow p\left(x\left(t\right)\vert x\left(0\right)\right)=\mathcal{N}\left(x\left(t\right)\vert \;0,\quad I\intop_{0}^{t}g^{2}\left(\tau\right)d\tau\right) \end{equation}\] <p>All of this is just <em>notation</em> for what we talked about above at the limit of $\Delta t\rightarrow0$. All SDEs of this form can be separated into a deterministic drift term, multiplied by $dt$ to remind us that only time matters, and a stochastic diffusion term, multiplied by $dW\left(t\right)$ to remind us that a random walk is taking place above the deterministic drift.</p> <p>In general, solving SDEs - in other words, finding an equation $x\left(t\right)$ for all $t$ - is very very difficult. Instead, many times numerical solutions are employed that try to simply following the path defined by the SDE. This process is sometimes called <em>integrating</em> the SDE or <em>discretizing</em> it, which is what we’ve been doing all along. Our discretization is the most naive one and is very bad when $\Delta t$ is not sufficiently small, but makes everything so intuitive. This discretization is called a <em>first-order Euler-Maruyama</em> discretization - first order because we only use gradients (and not higher order derivatives) and Euler-Maruyama because Euler invented pretty much everything, including the first solvers for ODEs. Maruyama extended the ODE solver to the SDE case as well, I think.</p> <p><br/></p> <h1 id="a-deterministic-alternative"><strong>A Deterministic Alternative</strong></h1> <d-byline></d-byline> <p>All of our derivations so far assumed that we’re okay with a stochastic algorithm for generating new samples. But using stochastic algorithms is actually kind of a pain. Many applications of (decoder-based) generative models assume that the mapping between the latent codes $z$ to the image space $x$ are deterministic and easy to manipulate. Can we make score-based models (or diffusion models) deterministic?</p> <p>Well, consider the following SDE for the sampling procedure:</p> \[\begin{equation} dx=-s_{\theta}\left(x\left(t\right);\;\sigma^{2}\left(t\right)\right)\cdot dt+dW\left(t\right) \end{equation}\] <p>We already know how samples from this distribution look at any time $t$, right? As we already showed before:</p> \[\begin{equation} x_{t}=x_{0}+t\eta\qquad\eta\sim\mathcal{N}\left(0,I\right) \end{equation}\] <p>So maybe all we need is a deterministic algorithm that receives $x_{t}$ and somehow reduces the noise?</p> <p>Let’s try to build such an algorithm.</p> <h3 id="when-x_0-is-known">When $x_0$ is Known</h3> <p>What we’ll try to do now is build a mapping $v\left(\cdot;t\right)$ such that:</p> \[\begin{equation} x_{t}\sim p\left(x;t\right)\quad\Rightarrow x_{t-\Delta t}=v\left(x_{t};t\right)\sim p\left(x;t-\Delta t\right) \end{equation}\] <p>Notice that if someone gives us $x_{0}$, this becomes simple since:</p> \[\begin{align} x_{t-\Delta t} &amp; =x_{0}+\sqrt{t-\Delta t}\cdot\eta\\ x_{t} &amp; =x_{0}+\sqrt{t}\cdot\eta \end{align}\] <p>So if we have $x_{0}$, we can just do the following:</p> \[\begin{equation} x_{t-\Delta t}=x_{0}+\frac{\sqrt{t-\Delta t}}{\sqrt{t}}\overbrace{\left(x_{t}-x_{0}\right)}^{\sqrt{t}\eta}\label{eq:cond-map} \end{equation}\] <p>In essence, this is like a <em>conditional</em> mapping: conditioned on the fact that we know $x_{0}$, the mapping function from $x_{t}$ to $x_{t-\Delta t}$ is as given in equation \eqref{eq:cond-map}.</p> <p>You can kind of consider this case as if:</p> \[\begin{equation} x_{t}\sim p\left(x\vert x_{0};t\right) \end{equation}\] <p>in which case:</p> \[\begin{equation} v\left(x_{t}\vert x_{0};t\right)\sim p\left(x\vert x_{0};t-\Delta t\right) \end{equation}\] <p>We now want to find a mapping that works when we don’t know $x_{0}$.</p> <h3 id="when-x_0-is-unknown">When $x_0$ is Unknown</h3> <p>Instead of using $x_{0}$, we’re basically going to use our_ best guess_ for $x_{0}$. That’s right, we’ll use the conditional mean $\mathbb{E}\left[x_{0}\vert x_{t}\right]$ as a stand-in for $x_{0}$.</p> <p>This means that our mapping function will be:</p> \[\begin{equation} x_{t-\Delta t}=v\left(x_{t};t\right)=\mathbb{E}\left[x_{0}\vert x_{t}\right]+\frac{\sqrt{t-\Delta t}}{\sqrt{t}}\left(x_{t}-\mathbb{E}\left[x_{0}\vert x_{t}\right]\right)\label{eq:marginal-map} \end{equation}\] <p>While this seems naive, it turns out it’s actually the correct thing to do.</p> <p>Basically, we thought of $v\left(x_{t}\vert x_{0};t\right)$ as a <em>function over random variables</em>. Using the change of variable formula, we have:</p> \[\begin{equation} p\left(v\left(x_{t}\vert x_{0};t\right)\vert x_{0};t-\Delta t\right)=\frac{p\left(x_{t}\vert x_{0};t\right)}{\left\vert \det\frac{\partial v\left(x_{t}\vert x_{0};t\right)}{\partial x_{t}}\right\vert } \end{equation}\] <p>If we want to define a mapping that is correct for any $x_{0}$ we need to somehow take the expectation over $x_{0}$.</p> <p>It turns out that taking the expectation over the mapping $v\left(\cdot\vert x_{0};t\right)$ is enough (<d-cite key="lipman2022flow"></d-cite>). That is, if we define:</p> \[\begin{equation} v\left(x_{t};t\right)=\intop v\left(x_{t}\vert x_{0};t\right)p\left(x_{0}\vert x_{t}\right)dx_{0}=\mathbb{E}_{x_{0}}\left[v\left(x_{t}\vert x_{0};t\right)\vert x_{t}\right]\label{eq:expectation-over-flow} \end{equation}\] <p>then:</p> \[\begin{equation} p\left(v\left(x_{t};t\right);t-\Delta t\right)=p\left(x_{t};t\right)/\left\vert \text{det}\left[\frac{\partial v\left(x_{t};t\right)}{\partial x_{t}}\right]\right\vert \end{equation}\] <p>If we take the expectation over the mapping $v\left(x_{t}\vert x_{0};t\right)$ as we defined in equation \eqref{eq:cond-map}, then we get equation \eqref{eq:marginal-map}.</p> <details><summary>Expectations over vector fields</summary> <p>While <d-cite key="lipman2022flow"></d-cite> do give a proof that equation \eqref{eq:expectation-over-flow}, it also makes some intuitive sense. Again, we can think of $v\left(x_{t}\vert x_{0};t\right)$ as the “correct” mapping function (or <em>flow</em>) if we know that we need to end up at $x_{0}$. Then, if we don’t know the end point, the expectation $\mathbb{E}_{x_{0}}\left[v\left(x_{t}\vert x_{0};t\right)\vert x_{t}\right]$ weighs all of the possible end goals $x_{0}$, giving the largest weight to the one where the path from $x_{t}$ to $x_{0}$ is most likely.</p> <p>As a constructed example, suppose that:</p> \[\begin{equation} p\left(x_{0}\right)=\frac{1}{2}\mathcal{N}\left(-1,\varphi^{2}\right)+\frac{1}{2}\mathcal{N}\left(1,\varphi^{2}\right) \end{equation}\] <p>with $\varphi^{2}\ll1$. Then if $x_{t}\gg0$, it’s path most likely ends at the cluster $\mathcal{N}\left(1,\varphi^{2}\right)$. If $x_{t}\ll0$, then it should probably end up near -1. On the other hand, if $t$ is really large or $x_{t}\approx0$, then there is no real preference as to where $x_{t}$ should end up, and the flow should push it to be somewhere in-between the two. That’s the kind of intuition, anyway.</p> </details> <h3 id="the-deterministic-mapping">The Deterministic Mapping</h3> <p>From Tweedie’s formula, we can rewrite equation \eqref{eq:marginal-map} as:</p> \[\begin{align} x_{t-\Delta t} &amp; =x_{t}+t\cdot s\left(x_{t};t\right)-\frac{\sqrt{t-\Delta t}}{\sqrt{t}}t\cdot s\left(x_{t};t\right)\\ &amp; =x_{t}+\left(\sqrt{t}-\sqrt{t-\Delta t}\right)\cdot\sqrt{t}\cdot s\left(x_{t};t\right) \end{align}\] <p>This formula is kinda awkward. When $\Delta t\rightarrow0$, we can actually simplify things a bit:</p> \[\begin{equation} \lim_{\Delta t\rightarrow0}\frac{\sqrt{t}-\sqrt{t-\Delta t}}{\Delta t}=\frac{d\sqrt{t}}{dt}=\frac{1}{2\sqrt{t}} \end{equation}\] <p>So:</p> \[\begin{align} \lim_{\Delta t\rightarrow0}\left(\sqrt{t}-\sqrt{t-\Delta t}\right)\cdot\sqrt{t} &amp; =\lim_{\Delta t\rightarrow0}\frac{\sqrt{t}-\sqrt{t-\Delta t}}{\Delta t}\cdot\Delta t\sqrt{t}\\ &amp; =\frac{\Delta t\cdot\sqrt{t}}{2\cdot\sqrt{t}}=\frac{\Delta t}{2} \end{align}\] <p>Finally, we can write:</p> \[\begin{equation} x_{t-\Delta t}\stackrel{\Delta t\rightarrow0}{=}x_{t}+\frac{1}{2}\Delta t\cdot s\left(x_{t};t\right) \end{equation}\] <p>We can now move to the continuous mapping, as we did in equation \eqref{eq:continuous-SGBM} to find that:</p> \[\begin{equation} \frac{dx}{dt}=-\frac{1}{2}s\left(x\left(t\right);t\right) \end{equation}\] <p>Let the last few steps sink in. We basically started by saying that we have an SDE of the following form:</p> \[\begin{equation} dx=-s\left(x;t\right)dt+dW\left(t\right) \end{equation}\] <p>and we found that this SDE is equivalent to an ODE. If you think it’s weird that we can do this, you’re not alone. And why that $\frac{1}{2}$? I’m just as surprised as you.</p> <details><summary>Generalizing to other SDEs</summary> <p>What we showed above is only true when $g\left(t\right)=1$. Given an SDE of the form:</p> \[\begin{equation} dx=\left(f\left(t\right)x-g^{2}\left(t\right)\cdot s\left(x;t\right)\right)dt+g\left(t\right) \end{equation}\] <p>then there is an equivalent ODE given by:</p> \[\begin{equation} \frac{dx}{dt}=f\left(t\right)x-\frac{1}{2}g^{2}\left(t\right)\cdot s\left(x;t\right) \end{equation}\] <p>What do I mean by “equivalent”? Basically, if we start with:</p> \[\begin{equation} x\left(T\right)\sim p\left(x;T\right) \end{equation}\] <p>and iterate backwards in time according to the SDE <em>or</em> the ODE, then for every time point $t$, we’ll get:</p> \[\begin{equation} x\left(t\right)\vert x\left(T\right)\sim p\left(x;t\right) \end{equation}\] <p>That is, for each time point $t$, the marginal distributions $p\left(x;t\right)$ of the SDE and the ODE will be the same. But this is <em>only true if we started the chains from</em> $p\left(x;T\right)$! This ODE is sometimes called the probability flow instead, as it has some relation to the continuous normalizing flows we talked about before.</p> <p>Actually, because of this relation to continuous normalizing flows, the likelihood of a sample $x\left(0\right)$ can be calculated under the ODE formualtion, with:</p> \[\begin{equation} \log p\left(x\left(0\right)\right)=\log p\left(x\left(T\right);T\right)+\frac{1}{2}\intop_{0}^{T}g^{2}\left(t\right)\text{trace}\left[\frac{\partial s_{\theta}\left(x\left(\tau\right);\tau\right)}{\partial x\left(\tau\right)}\right]d\tau \end{equation}\] </details> <p><br/></p> <h1 id="conclusion"><strong>Conclusion</strong></h1> <d-byline></d-byline> <p>Diffusion models as we have seen in the last two sections have become incredibly popular because they are able to generate extremely realistic samples. However, if we want to use these models for anything beyond generating samples, then they are quite cumbersome. For instance, we might want to use the likelihood of the model to solve some task. While we can get a lower bound for the likelihood with DDPM or an exact value for SBGM, many many steps are needed to get an accurate estimation, making diffusion models difficult to use in some practical applications.</p> <p><br/></p> <d-byline></d-byline> <p><span style="float:left"><a href="https://friedmanroy.github.io/blog/2024/gen4/">← DDPM</a></span><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen6/">Energy-Based Models →</a></span></p>]]></content><author><name>Roy Friedman</name></author><category term="machine-learning"/><category term="generative-models"/><summary type="html"><![CDATA[← DDPM Energy-Based Models →]]></summary></entry><entry><title type="html">Generative Models 0 - What is a Generative Model?</title><link href="https://friedmanroy.github.io/blog/2024/gen0/" rel="alternate" type="text/html" title="Generative Models 0 - What is a Generative Model?"/><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://friedmanroy.github.io/blog/2024/gen0</id><content type="html" xml:base="https://friedmanroy.github.io/blog/2024/gen0/"><![CDATA[<p><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen1/">A Linear Model →</a></span> <br/></p> <d-byline></d-byline> <p>You are given some data (let’s say it isn’t labeled). This data, you assume, has some significance. For instance, it might be data collected from a manufacturing factory and it could contain information regarding inefficient work lines. Or it contains information regarding the number of animals and their species in certain areas and you think that the number of animals depends on some human structure that was placed next to these areas. Or, this data is a set of CT scans of cancer patients and you think that certain places in the body have higher or lower chances of growing tumors.</p> <p>Whatever the case, you have data that (again, you assume) contains significant information for a scientific question which you want to answer. To aid you in the process of answering these important questions, you want to somehow simulate the process that created the data. In other words, you want to <em>model the generative process of the creation of the data</em>. After all, if you do so well then maybe, just maybe, you’ll be able to meaningfully solve the problem of interest.</p> <p><br/></p> <h1 id="definition-of-a-generative-model"><strong>Definition of a Generative Model</strong></h1> <d-byline></d-byline> <p>To be a bit more precise, given the set of data $\mathcal{D}=\left\{x_{i}\right\}_{i=1}^{N}$ where each point $x_{i}$ is assumed to have been sampled from a distribution $p_{\text{data}}\left(x\right)$, we want to find a parametric distribution such that:</p> \[\begin{equation} p_{\theta}\left(x\right)\equiv p\left(x;\theta\right)\approx p_{\text{data}}\left(x\right) \end{equation}\] <p>where $\theta$ are the parameters of the distribution. All of the assumptions and modeling decisions we make with regards to the distribution of the data are hidden inside the notation $\theta$.</p> <h3 id="a-simple-example-kernel-density-estimation-kde">A Simple Example: Kernel Density Estimation (KDE)</h3> <p>The simplest example for a generative model that I can think of is just to fit a Gaussian to the training data. But usually this won’t give us all that much information (and sometimes it will be hard/impossible to do).</p> <p>Beyond the Gaussian, one of the simplest examples of a generative model is <em>kernel density estimation</em> (KDE, also sometimes called <em>Parzen windows</em>). We’ll start by looking at a very basic instance of KDE and then slightly abstract it. This model assumes very little. In fact, the only thing we’ll assume is that points close to the observed data should have high density and that the distribution should decay quickly the further we are from the training points.</p> <p>So, again, let’s assume we have a dataset $\mathcal{D}=\left\{o_{i}\right\}_{i=1}^{N}$ (I’m calling them $o_{i}$ now so it will be less confusing in the next part). We want something that has high density near training points and decays fast. A base distribution that does this is an isotropic Gaussian distribution! So we can just put a lot of Gaussians centered around the training data:</p> \[\begin{equation} p_{\beta}\left(x\right)=\frac{1}{N}\sum_{i=1}^{N}\mathcal{N}\left(x\vert \ o_{i},I\beta\right)\label{eq:RBF-KDE} \end{equation}\] <p>where $\beta&gt;0$ is called the bandwidth and is the same for all points, that is somehow chosen. This <em>is a valid distribution</em> - you can check to make sure that $p_{\beta}\left(x\right)$ integrates to one.</p> <p>This very basic model “smoothes out” the empirical distribution, defined as:</p> \[\begin{equation} p_{\text{emp}}\left(x\right)=\frac{1}{N}\sum_{i=1}^{N}\delta\left(x-o_{i}\right) \end{equation}\] <p>where $\delta\left(\cdot\right)$ is Dirac’s delta, defined as:</p> \[\begin{equation} \delta\left(x\right)=\begin{cases} \infty &amp; x=0\\ 0 &amp; \text{otherwise} \end{cases}\qquad\quad\intop\delta\left(x\right)dx=1 \end{equation}\] <p>Anyway, in equation \eqref{eq:RBF-KDE} the smoothing function was a Gaussian, but there are many other possible choices that we could have made. The smoothing function is usually called the kernel, and it has to be a positive definite (PD) kernel if we want the KDE approximation to be a valid distribution. Given a kernel $K\left(x,o_{i}\right)$, the general KDE distribution is defined as:</p> \[\begin{equation} p_{\text{KDE}}\left(x\right)=\frac{1}{N}\sum_{i=1}^{N}K\left(x,o_{i}\right) \end{equation}\] <p>which is only a valid distribution if:</p> \[\begin{equation} \intop K\left(x,o_{i}\right)dx&lt;\infty \end{equation}\] <p>That said, everyone just uses the RBF kernel, which is the one from equation \eqref{eq:RBF-KDE}. While this seems like a very simple definition, it’s a very good baseline for comparison (which is why I even put it here). This form of KDE is sort of the nearest neighbors equivalent in the world of generative models, since the nearest neighbor will typically have the largest impact on the density of the point $x$ in $p_{\text{KDE}}\left(x\right)$. In this case, the kernel defines the distance function for the nearest neighbors (and is euclidean when using the RBF kernel).</p> <p><br/></p> <h1 id="training-generative-models"><strong>Training Generative Models</strong></h1> <d-byline></d-byline> <p>In general, we will want to train our generative models to be as adequate as possible to explain the observed data. Training a generative model in practice amounts to defining a way to compare two models, then choosing the parameters that give the better model under said comparison.</p> <p>Actually, it’s good that I’ve already presented the KDE, that way we’ll have a concrete example of this. Given two possible bandwidths $\beta_{1}$ and $\beta_{2}$, how are we supposed to choose which bandwidth is better suited to use in a KDE for our purposes?</p> <p>Training, or choosing between models, requires a way to compare generative models, as I’ve already said. How do we decide in practice? This depends on what we’re trying to achieve. It might be that we only want samples $x\sim p_{\theta}$ to be similar to samples from the real data $x\sim p_{\text{data}}$, in which case our criteria might be “. Or we could say that there is a set of statistics (say the mean, variance, etc.) gleaned from the observed data which we have to match, otherwise the distribution should be as general as possible. Or, the most popular, that $p_{\theta}\left(x\right)$ should be as close as possible to $p_{\text{data}}\left(x\right)$ under some divergence.</p> <p>Let’s talk about the last one.</p> <h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3> <p>A natural way to evaluate the quality of a model is through the use of some divergence between distributions:</p> \[\begin{equation} \text{error}\left(\theta\right)=D\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta\right)\right) \end{equation}\] <p>Here “divergence” just means some function $D\left(p\vert \vert q\right)$ which is 0 if and only if $\forall x\quad p(x)=q(x)$, and is monotonically increasing as a function of some difference between $p\left(x\right)$ and $q\left(x\right)$. Basically, a divergence is a sort of distance between distributions.</p> <p>The most commonly used divergence is the <em>Kullback-Leibler divergence</em> (KL-divergence) defined as:</p> \[\begin{align} D_{\text{KL}}\left(p\vert \vert q\right) &amp; =\intop p\left(x\right)\log\frac{p\left(x\right)}{q\left(x\right)}dx=\mathbb{E}_{x\sim p}\left[\log p\left(x\right)-\log q\left(x\right)\right]\\ &amp; =-\mathbb{E}_{p}\left[\log q\left(x\right)\right]-H\left(p\right) \end{align}\] <p>where $H\left(p\right)$ is called the <em>(differential) entropy</em> of the distribution $p\left(x\right)$.</p> <p>We want to compare the data distribution to our model’s distribution:</p> \[\begin{equation} D_{\text{KL}}\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta\right)\right)=-\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta\right)\right]-H\left(p_{\text{data}}\right) \end{equation}\] <p>Now, $H\left(p_{\text{data}}\right)$ will be impossible to calculate if we just observe some data, since we don’t actually know the function $p_{\text{data}}\left(\cdot\right)$. However, if all we want to do is compare between two parameterizations $\theta_{1},\theta_{2}\in\Theta$ and to determine which is best, we have to ask:</p> \[\begin{align} D_{\text{KL}}\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta_{1}\right)\right) &amp; \stackrel{?}{&gt;}D_{\text{KL}}\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta_{2}\right)\right)\\ D_{\text{KL}}\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta_{1}\right)\right)-D_{\text{KL}}\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta_{2}\right)\right) &amp; \stackrel{?}{&gt;}0\\ -\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta_{1}\right)\right]+\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta_{2}\right)\right] &amp; \stackrel{?}{&gt;}0\\ \mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta_{1}\right)\right] &amp; \stackrel{?}{&lt;}\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta_{2}\right)\right] \end{align}\] <p>That is, to decide if $p\left(x;\theta_{1}\right)$ is worse than $p\left(x;\theta_{2}\right)$ you don’t even need access to $H\left(p_{\text{data}}\right)$!</p> <p>The term $\log p\left(x;\theta\right)$ is called the <em>log-likelihood</em> of the model $\theta$. What we saw above is that it is enough to find $\theta\in\Theta$ that maximize the expected log-likelihood in order to say that we found the best parameters to describe the distribution (in terms of the KL-divergence):</p> \[\begin{equation} \theta^{\star}=\arg\min_{\theta\in\Theta}D_{\text{KL}}\left(p_{\text{data}}\left(x\right)\vert \vert p\left(x;\theta\right)\right)=\arg\max_{\theta\in\Theta}\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta\right)\right] \end{equation}\] <p>This concept is aptly called <em>maximum likelihood estimation</em> (MLE), because we estimate $p_{\text{data}}\left(x\right)$ with the model $p\left(x;\theta^{\star}\right)$ that maximizes the likelihood.</p> <p>Of course, in the real world calculating the expectation is impossible and we only have access to samples from the data distribution, $\mathcal{D}$. Also, researchers in machine learning are used to talking about losses (or errors). So usually the <em>negative log-likelihood</em> (NLL) loss is used to train generative models:</p> \[\begin{equation} L\left(\theta\right)=-\frac{1}{N}\sum_{x_{i}\in\mathcal{D}}\log p\left(x_{i};\theta\right)\approx-\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta\right)\right] \end{equation}\] <h3 id="difficulties-with-mle">Difficulties with MLE</h3> <p>MLE is the <em>most</em> popular way to train generative models, since it seems like such a natural criteria. After all, even without the whole thing with the KL-divergence, maximizing the log-likelihood of the training examples alone already seems like a good idea. So our criteria is simple to understand and instead training generative models is difficult because it’s hard in many cases to exactly calculate the log-likelihood $\log p\left(x;\theta\right)$.</p> <p>For instance, the most popular form of generative models in computer vision work in the following way:</p> \[\begin{equation} z\sim p\left(z\right)\stackrel{G_{\theta}}{\mapsto}x\in\mathcal{X}\label{eq:latent-space-model} \end{equation}\] <p>In words, there is a <em>latent code</em> $z\in\mathcal{Z}$ that is sampled according to a distribution $p\left(z\right)$, which is then mapped to the image space $\mathcal{X}$ using a (always complicated) function $G_{\theta}\left(z\right)$. If we write this out as a probability function, we would say that:</p> \[\begin{equation} p_{\theta}\left(z,x\right)=p\left(z\right)p_{\theta}\left(x\vert z\right) \end{equation}\] <p>In this case, to calculate the log-likelihood of the model we need to solve the following integral:</p> \[\begin{equation} p_{\theta}\left(x\right)=\intop p\left(z\right)p_{\theta}\left(x\vert z\right)dz \end{equation}\] <p>If $G_{\theta}\left(z\right)$ is anything beyond linear, then calculating the above integral will be pretty much impossible.</p> <p>The bottom line is that training generative models almost always involves maximizing the log-likelihood. To actually do so, we will need to use tricks and approximations in order to calculate the log-likelihood in almost every case.</p> <details><summary>A note about naming</summary> <p>Generative models that have a latent space that is mapped to the data space are sometimes called <em>decoder-based generative models</em> because they use a decoder to describe how data is generated. As I mentioned above, they are very common in computer vision, so now is a good time to get acquainted with some of the terminology.</p> <p>The naming convention of these generative models is a bit weird. Many times, the distribution over the latent codes $p\left(z\right)$ is called the <em>prior distribution</em>. Seemingly, these models follow from Bayesian statistics. If that is the case, $p_{\theta}\left(x\vert z\right)$ should be called the likelihood, but isn’t and instead $p_{\theta}\left(x\vert z\right)$ is usually called the <em>observation model</em> or <em>observation probability</em> or something like that. The distribution $p_{\theta}\left(z\vert x\right)$ is the <em>posterior distribution</em> and finally $p_{\theta}\left(x\right)$ is either the <em>likelihood, marginal likelihood</em> or the <em>evidence</em>. This is a weird mishmash of terms from Bayesian statistics that isn’t fully faithful to the Bayesian interpretation.</p> <p>The way to think about everything so it will make sense is this: given a specific decoder $G_{\theta}:\mathcal{Z}\rightarrow\mathcal{X}$, which we have no influence over, the distribution over $z$ basically defines a prior distribution over the possible $x$s. However, in the real world we don’t directly see $x$, there’s usually some noise involved in the observation - think about photos (especially when it’s dark), they tend to have some grain or noise. So, what we observe in the real world is more like $x=G_{\theta}\left(z\right)+\text{noise}$, which explains the distribution $p_{\theta}\left(x\vert z\right)$ and why it’s called the <em>observation model</em>. The name of the probability $p_{\theta}\left(x\right)$ has to match both Bayesian statistics and generative models in general, so it’s called either the <em>evidence</em> or <em>likelihood</em>, interchangeably.</p> </details> <p><br/></p> <h1 id="conclusion"><strong>Conclusion</strong></h1> <d-byline></d-byline> <p>Generative models, the main focus of this series of posts, are very popular. This popularity comes from their unprecedented performance in generating data from computer vision and natural language processing, most of which is very recent. Because of this sudden acceleration in the quality of samples generated from such models, there has been a surge of research into generative models.</p> <p>The purpose of this primer is to give a good foundation to learn about more complex methods, which I hope will be accessible. In the next post in this series we’ll explore a really simple generative model, which should give a kind of starting point into exploring the more complex, state of the art models.</p> <p><br/></p> <d-byline></d-byline> <p><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen1/">A Linear Model →</a></span></p>]]></content><author><name>Roy Friedman</name></author><category term="machine-learning"/><category term="generative-models"/><summary type="html"><![CDATA[A Linear Model →]]></summary></entry><entry><title type="html">Generative Models 1 - A Linear Model</title><link href="https://friedmanroy.github.io/blog/2024/gen1/" rel="alternate" type="text/html" title="Generative Models 1 - A Linear Model"/><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://friedmanroy.github.io/blog/2024/gen1</id><content type="html" xml:base="https://friedmanroy.github.io/blog/2024/gen1/"><![CDATA[<p><span style="float:left"><a href="https://friedmanroy.github.io/blog/2024/gen0/">← What is a Generative Model?</a></span><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen2/">Variational Methods →</a></span> <br/></p> <d-byline></d-byline> <blockquote> <p>Last post we saw a very general introduction and definition of generative models. In this post we’ll take a look at a specific class of very simple generative models.</p> </blockquote> <d-byline></d-byline> <h1 id="a-linear-model-probabilistic-principal-component-analysis"><strong>A Linear Model (Probabilistic Principal Component Analysis)</strong></h1> <p>For now, I’m staying with the latent space models, or decoder-based generative models, as very very loosely defined in the previous post. We’ll start off with the most basic decoder: a linear transformation. We will also assume that the latent codes are standard normal, so that everything’s nice and easy. Observed data points are now modeled through:</p> \[\begin{equation} x=Wz+\mu\qquad z\sim\mathcal{N}\left(0,I\right) \end{equation}\] <p>I’ll also assume that $z\in\mathbb{R}^{m}$ and $x\in\mathbb{R}^{d}$ such that $m\ll d$. However, if $m&lt;d$ then we might face a problem: the probability of ever sampling something not on this linear transformation is zero! However, real-world examples will probably never neatly lie on a hyperplane.</p> <p>Instead we’re going to have to assume that there was some noise added during the observation of $x$, i.e.:</p> \[\begin{equation} x=Wz+\mu+\text{noise} \end{equation}\] <p>The simplest noise we can assume is an isotropic Gaussian (the noise is the same in all directions), which will give us:</p> \[\begin{equation} p_{\theta}\left(x|z\right)=\mathcal{N}\left(x|\ Wz+\mu,I\varphi^{2}\right) \end{equation}\] <p>where $\varphi$ is the standard deviation of the noise we assume was added. Conveniently, this can be written in the following form:</p> \[\begin{equation} x=\mu+Wz+\varphi\cdot\eta\qquad z\sim\mathcal{N}\left(0,I_{m}\right),\:\eta\sim\mathcal{N}\left(0,I_{d}\right) \end{equation}\] <p>The joint distribution for our model is now very simply:</p> \[\begin{align} p_{\theta}\left(z,x\right) &amp; =p\left(z\right)p_{\theta}\left(x|z\right)\\ &amp; =\mathcal{N}\left(z|\,0,I_{m}\right)\times\mathcal{N}\left(x|\,\mu+Wz,\:I_{d}\varphi^{2}\right) \end{align}\] <p>with the set of parameters $\theta=\left{ \mu\in\mathbb{R}^{d},W\in\mathbb{R}^{d\times m},\varphi\in\mathbb{R}_{+}\right}$ . That’s $d\cdot\left(m+1\right)+1$ parameters.</p> <p>As defined, the above is the generative model equivalent of linear regression. This model is also called a probabilistic principal component analysis (pPCA, <d-cite key="tipping1999probabilistic"></d-cite>) model, for reasons that will become clear in a bit.</p> <p><br/></p> <h1 id="likelihood">Likelihood</h1> <d-byline></d-byline> <p>We are now dealing with multiplications and additions of Gaussian distributions. This makes calculating the likelihood extremely simple, as it means that it will also be Gaussian<d-footnote>See my <a href="https://friedmanroy.github.io/BML/3_gaussians/">notes on the Gaussian distribution</a> for more information.</d-footnote>.</p> <p>Our goal is to maximize the log-likelihood. To do so, we first need to be able to calculate the log-likelihood. Luckily, $x$ is a linear transformation of a Gaussian plus another Gaussian. That is, the marginal $p_{\theta}\left(x\right)$ will also be a Gaussian distribution, which is completely described by the mean and covariance. So let’s calculate the mean and covariance of $x$:</p> \[\begin{align} \mathbb{E}\left[x\right] &amp; =W\mathbb{E}\left[z\right]+\mu+\varphi\cdot\mathbb{E}\left[\eta\right]=\mu\\ \text{cov}\left[x\right] &amp; =W\text{cov}\left[z\right]W^{T}+\varphi^{2}\text{cov}\left[\eta\right]=WW^{T}+I\varphi^{2} \end{align}\] <p>So, the likelihood of $x$ under our model is:</p> \[\begin{equation} p\left(x;\theta\right)=\mathcal{N}\left(x|\;\mu,\:WW^{T}+I\varphi^{2}\right) \end{equation}\] <p><br/></p> <h1 id="maximizing-the-likelihood">Maximizing the Likelihood</h1> <d-byline></d-byline> <p>The model we described is basically a Gaussian. The ML solution for a Gaussian is simple. Given our dataset $\mathcal{D}=\left{ x_{i}\right} _{i=1}^{N}$, the best mean is:</p> \[\begin{equation} \hat{\mu}=\frac{1}{N}\sum_{i=1}^{N}x_{i} \end{equation}\] <p>The best covariance we can hope for is:</p> \[\begin{equation} S=\frac{1}{N}\sum_{i=1}^{N}\left(x_{i}-\hat{\mu}\right)\left(x_{i}-\hat{\mu}\right)^{T} \end{equation}\] <p>However, the covariance in our model has a specific parameterization. Basically, we want to find $W$ and $\varphi$ such that:</p> \[\begin{equation} \Sigma_{W}\stackrel{\Delta}{=}WW^{T}+I\varphi^{2}\approx S \end{equation}\] <p>The best $W$ and $\varphi$ are explicitly related to $S$, and we’ll need the EVD of the data’s covariance to define them:</p> \[\begin{equation} S=U\Lambda U^{T} \end{equation}\] <p>where $\Lambda_{ij}=0$ whenever $i\neq j$ and the eigenvalues $\lambda_{i}=\Lambda_{ii}$ are sorted in descending order. The MLE solution for $W$ and $\varphi$ turns out to be (see <d-cite key="tipping1999probabilistic"></d-cite> for the derivation):</p> \[\begin{align} \hat{\varphi}^{2} &amp; =\frac{1}{d-m}\sum_{i=m+1}^{d}\lambda_{i}\\ \hat{W} &amp; =U_{1:m}\left(\Lambda_{1:m}-\hat{\varphi}^{2}\right)^{1/2} \end{align}\] <p>where $U_{1:m}\in\mathbb{R}^{d\times m}$ are the first $m$ columns of $U$ (the first $m$ eigenvectors) and $\Lambda_{1:m}\in\mathbb{R}^{m\times m}$ are the first $m$ eigenvalues in $\Lambda$.</p> <p>Essentially, all we need to do in order to fit this linear model is to calculate the mean of the dataset and the SVD of the centered datapoints, since:</p> \[\begin{equation} \text{SVD}\left(\left\{ x_{1}-\hat{\mu},\cdots,x_{N}-\hat{\mu}\right\} \right)=U\Lambda^{1/2}V \end{equation}\] <p>This spares us from calculating the full data covariance $S$, which may sometimes be difficult/impossible to calculate when the dimensions of the data are very very large.</p> <p><br/></p> <h1 id="relation-to-pca"><strong>Relation to PCA</strong></h1> <d-byline></d-byline> <p>This linear model, as I mentioned, is called the <em>probabilistic principal component analysis</em> (pPCA) model. This name is because we model distribution as sitting mostly on the hyperplane described by the first $m&lt;d$ principal components of the dataset. Of course, data points don’t actually lie inside this hyperplane, and this discrepancy is modeled as additional isotropic noise. Simply put: this model is a probabilistic version of PCA. When $\varphi\rightarrow0$, it is exactly equal to the standard PCA.</p> <p>The main advantage of this model is it’s simplicity and obvious modeling assumptions. The model is simple because it is basically a Gaussian with a low-rank covariance, and what it means for the data to be modeled as a linear transformation of a standard normal distribution is pretty clear. In this sense, pPCA is a very good baseline for comparing other generative models. Also, sometimes it is surprising just how much can be described by this simple linear function.</p> <h3 id="related-models">Related Models</h3> <p>This is a pretty good place to discuss models similar to pPCA. Remember, we defined a pPCA as:</p> \[\begin{equation} \text{pPCA:}\qquad x=\mu+Wz+\eta\qquad\begin{matrix}\eta\sim\mathcal{N}\left(0,I\varphi^{2}\right)\\ z\sim\mathcal{N}\left(0,I\right) \end{matrix} \end{equation}\] <p>A closely related model is the <em>factor analysis</em> (FA) model, defined as basically the same thing:</p> \[\begin{equation} \text{FA}:\qquad x=\mu+Wz+\tilde{\eta}\qquad\begin{matrix}\tilde{\eta}\sim\mathcal{N}\left(0,\text{diag}\left(\varphi_{1}^{2},\cdots,\varphi_{d}^{2}\right)\right)\\ z\sim\mathcal{N}\left(0,I\right) \end{matrix} \end{equation}\] <p>The difference is that an FA allows different dimensions to have different amounts of noise, basically taking into account their scales. Funnily enough, this rather small change already makes it so there is no closed-form solution for the MLE. Instead, an <em>expectation maximization</em> (EM) algorithm has to be used to fit this model.</p> <p>Another model that should be considered as a baseline is a <em>pPCA mixture model</em> (pPCAMM or MoPPCA, <d-cite key="tipping1999mixtures"></d-cite>), which basically gathers a bunch of pPCAs into one model:</p> \[\begin{equation} p_{\theta}\left(x\right)=\sum_{k=1}^{K}\pi_{k}\text{pPCA}\left(x|\;\mu_{k},W_{k},\varphi_{k}\right)\qquad\begin{matrix}\forall k\quad\pi_{k}\ge0\\ \sum_{k}\pi_{k}=1 \end{matrix} \end{equation}\] <p>This is like the difference between a single Gaussian and a Gaussian mixture model (GMM). FAs can also (as any other distribution) be gathered this way, of course.</p> <p><br/></p> <h1 id="practical-considerations"><strong>Practical Considerations</strong></h1> <d-byline></d-byline> <p>We basically covered everything I wanted to cover with regards to pPCAs. I would be remiss, however, not to give a brief overview of how these models can actually be used in practice.</p> <p>As stated, $W\in\mathbb{R}^{d\times m}$ with $m\ll d$. If you’re using a pPCA and not a regular Gaussian, then the dimension of the data is probably large. Very large. However, in order to get the likelihood of data points, you’ll need to calculate, store in memory, and even invert the full covariance:</p> \[\begin{equation} \Sigma_{W}=WW^{T}+I\varphi^{2} \end{equation}\] <p>This matrix is a $d\times d$ matrix, which is a huge pain when $x$ is… huge. Intuitively, it doesn’t make sense that we need to calculate something in $d$ dimensions if we already know that it is a mostly $m&lt;d$ dimensional object. This intuition is correct.</p> <p>Almost all operations with Gaussians require inverting the covariance matrix. It turns out that there’s a mathematical equivalence, known as Woodbury’s matrix identity (among other names), that simplifies the inversion of low-rank matrices such as $\Sigma_{W}$ considerably:</p> \[\begin{align} \underbrace{\left(WW^{T}+I_{d}\varphi^{2}\right)^{-1}}_{d\times d\text{ inversion}} &amp; =\left(I_{d}-W\underbrace{\left(W^{T}W+I_{m}\varphi^{2}\right)^{-1}}_{m\times m\text{ inversion}}W^{T}\right)/\varphi^{2}\\ &amp; =\left(I_{d}-WM^{-1}W^{T}\right)/\varphi^{2} \end{align}\] <p>Instead of inverting the full $d\times d$ matrix, it turns out that it’s enough to invert the $m\times m$ matrix:</p> \[\begin{equation} M=W^{T}W+I_{m}\varphi^{2} \end{equation}\] <p>But this is still a $d\times d$ matrix, so in the face of it we still didn’t gain all that much by this change (although, believe me, inverting a small matrix instead of a big matrix is already an improvement).</p> <p>In fact, most of the times we don’t need just the inverse of the covariance (also called the precision, by the way). We need the precision matrix times some vector:</p> \[\begin{equation} \Sigma_{W}^{-1}v=\frac{1}{\varphi^{2}}v-\frac{1}{\varphi^{2}}\underbrace{WM^{-1}}_{\in\mathbb{R}^{d\times m}}\underbrace{W^{T}v}_{\in\mathbb{R}^{m}} \end{equation}\] <p>Beyond the fact that this takes much less time, the numerical stability of this operation is much better than if we were to invert a $d\times d$ matrix.</p> <p>The last thing to note is that the posterior is also completely tractable and equal to:</p> \[\begin{equation} p_{\theta}\left(z|x\right)=\mathcal{N}\left(z|\;M^{-1}W^{T}\left(x-\mu\right),M/\varphi^{2}\right) \end{equation}\] <p>This being easy to calculate means that (given an observed $x$) we can carry out all needed operations in the latent dimension with ease. This is sometimes difficult to do, as we will see in the next parts of this primer.</p> <p><br/></p> <h1 id="conclusion"><strong>Conclusion</strong></h1> <d-byline></d-byline> <p>The pPCA model is one of the simplest generative models one can use for continuous data. It’s basically a Gaussian with most of the probability concentrated around a lower-dimensional projection. Because this model is so simple, it serves as a good baseline. Also, it allows us to do solve many tasks with a closed-form solution, which is really rare for generative models.</p> <p>So, where do we proceed? The obvious next step after a linear model is one that isn’t linear. This turns out to complicate matters considerably. In the next post, we’ll take a look at variation methods (such as variational auto-encoders), which give us a framework for training and using non-linear models in inference tasks.</p> <p><br/></p> <d-byline></d-byline> <p><span style="float:left"><a href="https://friedmanroy.github.io/blog/2024/gen0/">← What is a Generative Model?</a></span><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen2/">Variational Methods →</a></span></p>]]></content><author><name>Roy Friedman</name></author><category term="machine-learning"/><category term="generative-models"/><summary type="html"><![CDATA[← What is a Generative Model?Variational Methods →]]></summary></entry><entry><title type="html">Generative Models 2 - Variational Methods</title><link href="https://friedmanroy.github.io/blog/2024/gen2/" rel="alternate" type="text/html" title="Generative Models 2 - Variational Methods"/><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://friedmanroy.github.io/blog/2024/gen2</id><content type="html" xml:base="https://friedmanroy.github.io/blog/2024/gen2/"><![CDATA[<p><span style="float:left"><a href="https://friedmanroy.github.io/blog/2024/gen1/">← A Linear Model</a></span><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen3/">Normalizing Flows →</a></span> <br/></p> <d-byline></d-byline> <p>In the previous post we saw how to define $x=G_{\theta}\left(z\right)$ such that $p\left(x;\theta\right)$ is tractable (can be calculated easily). But what if we wanted to use more general decoders? For instance, we believe that the latent space is in a lower dimension, but that the decoder is not linear. What do we do in that case?</p> <p>If we want to calculate $\log p_{\theta}\left(x\right)$ in general models, we’re going to have to start approximating some things. After all, calculating:</p> \[\begin{equation} \log p_{\theta}\left(x\right)=\log\intop p_{\theta}\left(x\vert z\right)p\left(z\right)dz \end{equation}\] <p>is going to be impossible in almost every scenario.</p> <p><em>Variational methods</em>, the focus of this section, attempt to lower-bound the log-likelihood instead of calculating it exactly. Recall, our goal is to maximize the expected log-likelihood:</p> \[\begin{equation} \text{goal:}\;\text{find }\arg\max_{\theta}\mathbb{E}_{p_{\text{data}}}\left[\log p\left(x;\theta\right)\right] \end{equation}\] <p>If we can find a lower bound $\log p\left(x;\theta\right)\ge\phi\left(x;\theta\right)$ and maximize $\phi\left(x;\theta\right)$, then we know that our distribution will be at least as good as that.</p> <p><br/></p> <h1 id="evidence-lower-bound-elbo"><strong>Evidence Lower Bound (ELBO)</strong></h1> <d-byline></d-byline> <p>Finding a lower bound for the log-likelihood sounds kind of difficult, but follows a set of simple steps. To find the lower bound, we will need a guess for $p_{\theta}\left(z\vert x\right)$. Let’s call this guess $q_{\phi}\left(z\vert x\right)$. The log-likelihood can now be written as:</p> \[\begin{align} \log p_{\theta}\left(x\right) &amp; =\log\intop p_{\theta}\left(x,z\right)dz\\ &amp; =\log\intop p_{\theta}\left(x,z\right)\frac{q_{\phi}\left(z\vert x\right)}{q_{\phi}\left(z\vert x\right)}dz\\ &amp; =\log\mathbb{E}_{q_{\phi}}\left[\frac{p_{\theta}\left(x,z\right)}{q_{\phi}\left(z\vert x\right)}\right]\\ &amp; \ge\mathbb{E}_{q_{\phi}}\left[\log p_{\theta}\left(x\vert z\right)+\log\frac{p\left(z\right)}{q_{\phi}\left(z\vert x\right)}\right]\\ &amp; =\mathbb{E}_{q_{\phi}}\left[\log p_{\theta}\left(x\vert z\right)\right]-D_{\text{KL}}\left(q_{\phi}\left(z\vert x\right)\vert \vert p\left(z\right)\right)\\ &amp; =-F\left(x;\theta,\phi\right)\label{eq:free-energy} \end{align}\] <p>The last step is due to Jensen’s inequality. Notice how this is true for any $q_{\phi}\left(z\vert x\right)$ - we now have a lower bound on the log-likelihood! The value $F\left(x;\theta,\phi\right)$ is sometimes called the <em>free energy</em> for reasons that are broadly trivia and specifically way too complicated to actually talk about. At any rate, it’s useful to know that it’s called the free energy, so you won’t be surprised if you see the name ever again.</p> <p>When is this lower bound tight? If we happen to choose $q_{\phi}\left(z\vert x\right)=p_{\theta}\left(z\vert x\right)$, then:</p> \[\begin{align} \log p_{\theta}\left(x\right) &amp; \ge\mathbb{E}_{q_{\phi}\left(z\vert x\right)}\left[\log\frac{p_{\theta}\left(z\vert x\right)}{q_{\phi}\left(z\vert x\right)}+\log p_{\theta}\left(x\right)\right]\\ &amp; =\mathbb{E}_{p_{\theta}\left(z\vert x\right)}\left[\log\frac{p_{\theta}\left(z\vert x\right)}{p_{\theta}\left(z\vert x\right)}+\log p_{\theta}\left(x\right)\right]\\ &amp; =\mathbb{E}_{p_{\theta}\left(z\vert x\right)}\left[\log p_{\theta}\left(x\right)\right]=\log p_{\theta}\left(x\right) \end{align}\] <p>So, <em>the best</em> guess we can have for $q_{\phi}\left(z\vert x\right)$ is the conditional distribution we are approximating to begin with, $p_{\theta}\left(z\vert x\right)$.</p> <p>In practice, we will assume a simple form for $q_{\phi}\left(z\vert x\right)$, almost always a Gaussian distribution. The quality of the lower bound will then be the difference from the guess $q_{\phi}\left(z\vert x\right)$ and the true posterior $p_{\theta}\left(z\vert x\right)$.</p> <p><br/></p> <h1 id="classical-variational-inference-vi"><strong>Classical Variational Inference (VI)</strong></h1> <d-byline></d-byline> <p>What we wrote above in equation \eqref{eq:free-energy} is a lower for the log-likelihood of a <em>single data point</em>. Remember, what we actually want to maximize is the log-likelihood of <em>all training examples</em>. The “traditional” way <d-cite key="blei2017variational"></d-cite> to use this lower bound for training a model is to alternate the following steps:</p> \[\begin{align} \left(I\right)\qquad &amp; \text{for }i=1,\cdots,N:\quad\hat{\phi}_{i}=\arg\max_{\phi}-F\left(x_{i};\theta^{\left(t-1\right)},\phi\right)\\ \left(II\right)\qquad &amp; \theta^{\left(t\right)}=\arg\max_{\theta}\left\{ -\frac{1}{N}\sum_{i=1}F\left(x_{i};\theta,\hat{\phi}_{i}\right)\right\} \end{align}\] <p>The first step gives us a better lower bound for the true log-likelihood. The second step is where we make a better prediction of the parameters $\theta$ under the specific lower bound.</p> <p>Notice that step $\left(I\right)$ basically requires an <em>optimization of a distribution for every observed data point</em> $x_{i}$. This is quite expensive. However, as a byproduct we get, for the same price, the approximation $q\left(z\vert x_{i};\hat{\phi}_{i}\right)$ for each posterior $p\left(z\vert x_{i};\hat{\theta}\right)$. This can sometimes be useful (maybe more on that later on).</p> <details><summary>Example</summary> <p>I’ve kept everything pretty vague until now, so an example might be good right about now.</p> <p>Let’s, for a moment, imagine that we don’t know how to find the posterior distribution for the pPCA. Our model is:</p> \[\begin{equation} p_{\theta}\left(x,z\right)=\mathcal{N}\left(z\vert \;0,I\right)\times\mathcal{N}\left(x\vert \;\mu+Wz,\;I\varphi^{2}\right) \end{equation}\] <p>Our parameters are $\theta=\left\{ \mu,W,\varphi\right\}$. We are assuming that we don’t know the posterior distribution, so let’s approximate it with some other distribution. The simplest distribution to use many times is a isotropic Gaussian, so let’s define:</p> \[\begin{equation} \phi=\left\{ m,\sigma^{2}\right\} \qquad q_{\phi}\left(z\vert x\right)=\mathcal{N}\left(z\vert \:m,I\sigma^{2}\right)\approx p_{\theta}\left(z\vert x\right) \end{equation}\] <p>Now, for each data point $x_{i}$, we will try to optimize $\phi_{i}$ so the following is as high as possible:</p> \[\begin{align} \hat{\phi}\left(x_{i}\right) &amp; =\arg\max_{\phi}-F\left(x_{i};\theta,\phi\right)\\ &amp; =\arg\max_{\phi}\left\{ \mathbb{E}_{q_{\phi}}\left[\log p_{\theta}\left(x_{i}\vert z\right)\right]- D_{\text{KL}}\left(q_{\phi}\left(z\vert x_{i}\right)\vert \vert p\left(z\right)\right)\right\} \end{align}\] <p>This seems quite difficult to calculate. Luckily for us, both $p\left(z\right)$ and $q_{\phi}\left(z\vert x_{i}\right)$ are Gaussian distributions, so the KL-divergence between the two has a closed-form expression:</p> \[\begin{align} D_{\text{KL}}\left(q_{\phi}\left(z\vert x_{i}\right)\vert \vert p\left(z\right)\right) &amp; =\frac{1}{2}\left(\text{trace}\left(I\sigma^{2}\right)-\text{dim}\left(z\right)+\| m\| ^{2}-\log\left\vert I\sigma^{2}\right\vert \right)\\ &amp; =\frac{\text{dim}\left(z\right)}{2}\left(\sigma^{2}-\log\sigma^{2}\right)+\frac{1}{2}\| m\| ^{2} \end{align}\] <p>So that’s one part of the lower bound we can directly calculate. How about the expectation? Well, in this case there is also a closed-form expression for the expectation, but many times there won’t be. Instead, we can try to approximate $\mathbb{E}_{q_{\phi}}\left[\log p_{\theta}\left(x_{i}\vert z\right)\right]$ using, you guessed it, Monte Carlo (MC) samples. Basically, we will draw $M$ samples from $z_{j}\sim q_{\phi}\left(z\vert x_{i}\right)$ and approximate the expectation using these samples:</p> \[\begin{equation} \mathbb{E}_{q_{\phi}}\left[\log p_{\theta}\left(x_{i}\vert z\right)\right]\approx\frac{1}{M}\sum_{j:\,z_{j}\sim q_{\phi}\left(z\vert x_{i}\right)}^{M}\log p_{\theta}\left(x_{i}\vert z_{j}\right) \end{equation}\] <p>Putting all of that together, for each data point $x_{i}$ we will try to find $\hat{\phi}_{i}=\left\{ m_{i},\sigma_{i}^{2}\right\}$ that maximizes the following:</p> \[\begin{align} \hat{\phi}_{i}$=\arg\max_{\phi}\left\{ \frac{1}{M}\sum_{j:\,z_{j}\sim q_{\phi}\left(z\vert x_{i}\right)}^{M}\log p_{\theta}\left(x_{i}\vert z_{j}\right) \right. \\ \left. -\frac{\text{dim}\left(z\right)}{2}\left(\sigma^{2}- \log\sigma^{2}\right)+\frac{1}{2}\| m\| ^{2}\right\} \end{align}\] <p>If we do this for each $x_{i}$, then our <em>lower-bound</em> for the expected log-likelihood of the whole data will be:</p> \[\begin{align} \frac{1}{N}\sum_{i=1}^{N}\log p_{\theta}\left(x_{i}\right) &amp; \ge\frac{1}{N}\sum_{i=1}^{N}\left[\frac{1}{M}\sum_{j:\,z_{j}\sim q_{\hat{\phi}_{i}}\left(z\vert x_{i}\right)}^{M}\log p_{\theta}\left(x_{i}\vert z_{j}\right)\right. \\&amp; \left. -\frac{\text{dim}\left(z\right)}{2}\left(\sigma_{i}^{2}-\log\sigma_{i}^{2}\right)+\frac{1}{2}\| m_{i}\| ^{2}\right] \end{align}\] <p>If that seems to you like a really round about way to get a lower bound for the log-likelihood then, well, I don’t blame you.</p> </details> <p><br/></p> <h1 id="variational-auto-encoders-vaes"><strong>Variational Auto-Encoders (VAEs)</strong></h1> <d-byline></d-byline> <p>Variational auto-encoders (VAEs, <d-cite key="kingma2013auto"></d-cite>) attempt to do the above in a way that is slightly more efficient. Instead of optimizing the parameters $\phi$ for each data point $x_{i}$ individually, an encoder is trained to try and predict them. At the same time, the mapping from $\mathcal{Z}$ to $\mathcal{X}$ is also trained.</p> <p>Concretely, suppose $z\sim\mathcal{N}\left(0,I\right)$. Because there’s a closed-form expression for the KL-divergence between two Gaussians, and because it’s easy to sample from a Gaussian, it will be convenient if we assume that:</p> \[\begin{equation} q_{\phi}\left(z\vert x\right)=\mathcal{N}\left(z\vert \:\mu_{\phi}\left(x\right),\Sigma_{\phi}\left(x\right)\right) \end{equation}\] <p>In other words, our guess for the posterior $p_{\theta}\left(z\vert x\right)$ is a Gaussian distribution whose mean and covariance are functions of the observed data, $x$. In practice, a neural network (NN) will be used to <em>encode</em> $x$ into the mean $\mu_{\phi}\left(x\right)$ and covariance $\Sigma_{\phi}\left(x\right)$. Now, instead of finding $\hat{\phi}_{i}$ as we did in VI before, we’ll just try to train the encoder $\mu_{\phi}\left(x_{i}\right)$ and $\Sigma_{\phi}\left(x_{i}\right)$ to give a good guess for the posterior.</p> <p>In other words, we’ll train both the encoders and the decoder, $G_{\theta}\left(z\right)$, at the same time using the usual variational loss we saw in equation \eqref{eq:free-energy}:</p> \[\begin{equation} \left\{ \hat{\theta},\hat{\phi}\right\} =\arg\max_{\theta,\phi}\left\{ \frac{1}{N}\sum_{i=1}^{N}\left[\mathbb{E}_{q_{\phi}}\left[\log p_{\theta}\left(x_{i}\vert G_{\theta}\left(z\right)\right)\right]-D_{\text{KL}}\left(q_{\phi}\left(z\vert x_{i}\right)\vert \vert p\left(z\right)\right)\right]\right\} \end{equation}\] <p>As we mentioned earlier, we know how to calculate $D_{\text{KL}}\left(q_{\phi}\left(z\vert x_{i}\right)\vert \vert p\left(z\right)\right)$ exactly:</p> \[\begin{equation} D_{\text{KL}}\left(q_{\phi}\left(z\vert x_{i}\right)\vert \vert p\left(z\right)\right)=\frac{1}{2}\left(\text{trace}\left[\Sigma_{\phi}\left(x_{i}\right)\right]+\| \mu_{\phi}\left(x_{i}\right)\| ^{2}-\log\left\vert \Sigma_{\phi}\left(x_{i}\right)\right\vert -\text{dim}\left(z\right)\right) \end{equation}\] <p>which will be pretty simple to calculate on the fly.</p> <p>But we are missing other ingredients. For instance, how should we define the observation model $p_{\theta}\left(x\vert G_{\theta}\left(z\right)\right)$? And how do we calculate (and back-propagate through) the expectation of the first term?</p> <p>In a second we’ll get to what people usually use for the observation model, but for now let’s leave it up to the user. Having defined $p_{\theta}\left(x\vert G_{\theta}\left(z\right)\right)$, we’re still left with the question of how to calculate the expectation. What is usually done is the most straightforward - simply use an MC approximation. Actually, approximate the expectation with <em>a single sample</em> from $q_{\phi}\left(z\vert x_{i}\right)$:</p> \[\begin{equation} \mathbb{E}_{q_{\phi}}\left[\log p_{\theta}\left(x_{i}\vert G_{\theta}\left(z\right)\right)\right]\approx\log p_{\theta}\left(x_{i}\vert G_{\theta}\left(\tilde{z}\right)\right)\quad\tilde{z}\sim\mathcal{N}\left(\mu_{\phi}\left(x_{i}\right),\Sigma_{\phi}\left(x_{i}\right)\right) \end{equation}\] <p>While this sounds kind of ridiculous, it actually works okay.</p> <h3 id="observation-model">Observation Model</h3> <p>The most common observation model is, as you probably already guessed, simply a Gaussian distribution:</p> \[\begin{equation} \log p_{\theta}\left(x\vert G_{\theta}\left(z\right)\right)=-\frac{\beta}{2}\| x-G_{\theta}\left(z\right)\| ^{2}+\frac{\text{dim}\left(x\right)}{2}\log\beta+\text{const} \end{equation}\] <p>where $1/\beta$ is the variance of the observation model. In vanilla VAEs, $\beta$ is almost ubiquitously set to 1. If we think about the whole story so far, this is like assuming that every example has been observed with Gaussian noise whose variance is 1. This might make sense some times, but in images (for instance) where pixels take values between 0 and 1, maybe it doesn’t make sense? Just something to think about.</p> <p>Anyway, setting $\beta=1$, we get the “standard” VAE loss (ignoring constants and multiplicative factors, as is usually done):</p> \[\begin{equation} L\left(\theta,\phi\right)=\underbrace{\| x-G_{\theta}\left(z\right)\| ^{2}}_{\text{reconstruction}}+\underbrace{\text{trace}\left[\Sigma_{\phi}\left(x_{i}\right)\right]+\| \mu_{\phi}\left(x_{i}\right)\| ^{2}-\log\left\vert \Sigma_{\phi}\left(x_{i}\right)\right\vert }_{\text{KL term}} \end{equation}\] <p>If we use a different value for $\beta$, we recover something called $\beta$-VAEs:</p> \[\begin{equation} L_{\beta}\left(\theta,\phi\right)=\beta\cdot\| x-G_{\theta}\left(z\right)\| ^{2}+\text{trace}\left[\Sigma_{\phi}\left(x_{i}\right)\right]+\| \mu_{\phi}\left(x_{i}\right)\| ^{2}-\log\left\vert \Sigma_{\phi}\left(x_{i}\right)\right\vert \end{equation}\] <p><br/></p> <h1 id="conclusion"><strong>Conclusion</strong></h1> <d-byline></d-byline> <p>VAEs, as described in this post, are very general. We didn’t impose any conditions on the decoder (or the encoder) in any real way, and in theory they should work quite well. In practice, however, (when used in vision) VAEs tend to produce very blurry images. For a long time, the observation model was blamed for these subpar results<d-footnote>I'm happy those concerns have faded away after the success of diffusion models.</d-footnote>, but it’s not really clear why they underperform in the task of generation. As a consequence of this bad performance, many additions were added onto the vanilla VAE. These include using a more expressive variational distribution ($q_{\phi}(z)$, <d-cite key="tomczak2018vae"></d-cite>), adding more MCMC samples with appropriate weights in the ELBO calculation (<d-cite key="burda2015importance"></d-cite>), and changing the observation model (<d-cite key="larsen2016autoencoding"></d-cite>). None of these seem to help in a meaningful way, as far as I know (<d-cite key="chadebec2022pythae"></d-cite>).</p> <p>While VAEs are kind of unpopular at the moment, they are still a very good introduction into generative models. Moreover, they are still used to some extent - the most ubiquitous use is as auto-encoders for other generative models (<d-cite key="rombach2022high"></d-cite>). The fact that they inherently give a lower bound for the likelihood of newly observed samples is also a major plus.</p> <p><br/></p> <d-byline></d-byline> <p><span style="float:left"><a href="https://friedmanroy.github.io/blog/2024/gen1/">← A Linear Model</a></span><span style="float:right"><a href="https://friedmanroy.github.io/blog/2024/gen3/">Normalizing Flows →</a></span></p>]]></content><author><name>Roy Friedman</name></author><category term="machine-learning"/><category term="generative-models"/><summary type="html"><![CDATA[← A Linear ModelNormalizing Flows →]]></summary></entry><entry><title type="html">Annealed Importance Sampling</title><link href="https://friedmanroy.github.io/blog/2023/AIS/" rel="alternate" type="text/html" title="Annealed Importance Sampling"/><published>2023-11-09T00:00:00+00:00</published><updated>2023-11-09T00:00:00+00:00</updated><id>https://friedmanroy.github.io/blog/2023/AIS</id><content type="html" xml:base="https://friedmanroy.github.io/blog/2023/AIS/"><![CDATA[<p>Suppose you can’t find your keys. You know you left them in your apartment somewhere, but don’t remember where. This happens pretty often, so you have a keep note of the possible places you may have left your keys. You want to find out the probability that the keys are in a specific room of your apartment. Let’s call this room $R$ - mathematically speaking, you want to calculate: \(\begin{equation} P\left(\text{keys}\in R\right)=\intop \mathbf{1}\left[x\in R\right]\cdot p_\text{keys}(x)dx \end{equation}\)</p> <p>where $x\in\mathbb{R}^2$ is a two-dimensional coordinate where the keys may have been forgotten and $p_\text{keys}(x)$ is the PDF for the keys to be in the location $x$. The function $\mathbf{1}\left[x\in R\right]$ equals 1 if $x\in R$, otherwise it is 0. The above can be rewritten:</p> \[\begin{equation} P\left(\text{keys in }R\right) = \mathbb{E}_{x\sim p_\text{keys}}\left[\mathbf{1}\left[x\in R\right]\right] \end{equation}\] <p>So… how can we calculate (or approximate) this expectation? Which room is most probable to contain your lost keys?</p> <p>While this example is a bit silly, the problem can be abstracted to fit many different situations. In this post, I’m going to show how it can be solved using Annealed Importance Sampling (AIS, <d-cite key="AIS"></d-cite>). Honestly speaking, when the data is 2-dimensional there are better ways to do this, but 2D allows for simple (intuitive!) visualizations, so let’s stick with our somewhat wonky example.</p> <p><br/></p> <h1 id="problem-statement"><strong>Problem Statement</strong></h1> <d-byline></d-byline> <p>Let’s define the problem again, just a bit more generally.</p> <p>We have some distribution over the domain $\mathcal{X}$: \(\begin{equation} p(x)= \frac{1}{Z}\tilde{p}(x) \end{equation}\)</p> <p>where for a given $x$ we know how to calculate $\tilde{p}(x)$. In this example, I’m going to assume the normalizing constant $Z$ isn’t known. This setting matches a situation where you keep track of where the keys were left in the past and have a non-parametric formulation for the density $\tilde{p}(x)$, in which case $Z$ is hard to calculate.</p> <p>I will also assume that there’s a function $f:\mathcal{X}\rightarrow \mathbb{R}$ and we want (for whatever reason) to calculate:</p> \[\begin{equation}\label{eq:f-exp} \overline{f}=\mathbb{E}_{x\sim p(x)}\left[f\left(x\right)\right] \end{equation}\] <p>In our example, $p(x)=p_\text{keys}(x)$ and $f(x)=\mathbf{1}\left[x\in R\right]$.</p> <p>The question is: how can we calculate $\overline{f}$? And as a secondary goal: is there a way to estimate $Z$ simultaneously, so we have access to the full (normalized) distribution?</p> <h3 id="first-approach">First Approach</h3> <p>One way to find $\overline{f}$ and $Z$ is using <em>importance sampling</em> (IS). In IS, a simple distribution $q(x)$ is chosen and the expectation in equation \eqref{eq:f-exp} is approximated according to (see <a href="#a1-importance-sampling">A.1</a> for more details):</p> \[\begin{equation} \tilde{w}(x)=\frac{\tilde{p}(x)}{q(x)}\qquad\overline{f}\approx \frac{\sum_{x_i\sim q}\tilde{w}(x_i)f(x_i)}{\sum_{x_i\sim q}\tilde{w}(x_i)} \end{equation}\] <p>The number $\tilde{w}(x)$ defines the relative importance of $x$ under our distribution of interest $\tilde{p}(x)$ and the simple distribution $q(x)$, which is why $\tilde{w}(x)$ are called <em>importance weights</em>.</p> <p>IS also let’s us approximate the normalization constant $Z$, using only the importance weights: $Z=\mathbb{E}<em>q\left[\tilde{w}(x)\right]\approx\frac{1}{N}\sum</em>{x_i\sim q}^N\tilde{w}(x_i)$.</p> <p>This seems to solve the problem we defined earlier. Of course, this post is about <em>annealed</em> IS, not IS, so there’s going to be a bit more to read.</p> <p>While incredibly simple and easy to use, IS is actually pretty hard to calibrate. Here, calibration means choosing a distribution $q(x)$ that is similar in some sense to $p(x)$. The best we can do, after all, is $q(x)=p(x)$. In that case, all of the importance weights will be equal to 1 and we would get a perfect approximation of $\overline{f}$ and $Z$. No, usually a much simpler distribution $q(x)$ is chosen, and if $q(x)\gg p(x)$ in some region of space then many samples from $q(x)$ will end up with very low importance weights $\tilde{w}(x)$. In such a situation, an enormous number of samples has to be used in order to get a sound approximation.</p> <h3 id="another-way">Another Way</h3> <p>If we are solely interested in estimating the expectation in equation \eqref{eq:f-exp}, then another alternative is available - as long as we have some method for producing samples from $p(x)$ using only the unnormalized function $\tilde{p}(x)$. If that is the case, then $M$ points $x_1,\cdots,x_M$ can be sampled and used to get an unbiased approximation of the expectation:</p> \[\begin{equation} \overline{f}\approx\frac{1}{M}\sum_{i:\ x_i\sim p}^M f(x_i) \end{equation}\] <p>To this end, Markov chain Monte Carlo (MCMC) methods can be used, such as <a href="https://friedmanroy.github.io/blog/2022/Langevin/">Langevin dynamics</a>, in order to sample from the distribution. Many of these MCMC methods only require access to the gradient of the log of the distribution, $\nabla \log p(x)=\nabla \log \tilde{p}(x)$, so not knowing the normalization constant isn’t a problem. However, this doesn’t give us any estimate of $Z$ and many times it’s also difficult to tune an MCMC sampler.</p> <h3 id="combining-is-with-mcmc">Combining IS with MCMC</h3> <p>At it’s core, AIS is a way to combine the importance weights in IS with an MCMC approach. The idea is relatively simple: start with a sample from a simple distribution $q(x)$ and use MCMC iterations to get this sample closer to the distribution of interest $p(x)$. At the same time, we can also keep track of the relative importance of the sample, getting better calibrated importance weights.</p> <p>That’s the main intuition behind AIS. Don’t worry if it’s still unclear, you have a bit more to read which I hope will clarify things. <br/></p> <h1 id="annealing-the-importance-distribution"><strong>Annealing the Importance Distribution</strong></h1> <d-byline></d-byline> <p>As in IS, we begin by choosing: \(\begin{equation} q(x)=\frac{1}{Z_0}\tilde{q}(x) \end{equation}\) which is easy to sample from and whose normalization constant, $Z_0$, is known.</p> <p>How are we going to get this sample closer to $p(x)$? We’re going to define a series of intermediate distributions that gradually get closer and closer to $p(x)$. For now I’ll define the $T$ intermediate distributions as:</p> \[\begin{aligned} \pi_t(x)&amp;=\tilde{q}(x)^{1-\beta(t)}\cdot\tilde{p}(x)^{\beta(t)}\\ \beta(t)&amp;=t/T \end{aligned}\] <p>where $p(x)=\tilde{p}(x)/Z_T$ is the distribution we’re actually interested in. Notice that $\beta(0)=0$ and $\beta(T)=1$, so:</p> \[\begin{align} \pi_0(x)&amp;=\tilde{q}(x)\\ \pi_T(x)&amp;=\tilde{p}(x) \end{align}\] <p>Furthermore, the values of $\beta(t)$ gradually move from 0 to 1, so for each $t$ the function $\pi_t(x)$ is an unnormalized distribution somewhere between the two distributions $\tilde{q}(x)$ and $\tilde{p}(x)$. These intermediate distributions will allow a smooth transition from the simple distribution to the complex.</p> <p>If we use many iteration $T$, then the difference between each $\pi_t(x)$ and $\pi_{t+1}(x)$ will be very small, such that a sample from $\pi_t(x)$ is almost (but not quite) a valid sample from $\pi_{t+1}(x)$. Accordingly, we can use a relatively lightweight MCMC approach to get a sample from $\pi_{t+1}(x)$ starting from the $\pi_t(x)$ sample. And we can do this for all $t$, starting from the initial simple distribution $\pi_0(x)$.</p> <p>At the same time, the importance weights for $\pi_{t+1}(x)$ given the $\pi_t(x)$ “proposal distribution” are $w_t=\frac{\pi_{t+1}(x)}{\pi_{t}(x)}$. We essentially want to get the importance weights for the whole chain $\pi_0(x)\rightarrow \pi_1(x)\rightarrow\cdots\rightarrow \pi_T(x)$, so we will multiply the time-based importance weights along the way. Ultimately, given a chain of $x_0,\cdots,x_{T-1}$ the importance weight of the whole chain will be given by:</p> \[\begin{equation} w(x_0,\cdots,x_T)=Z_0\frac{\pi_1(x_0)}{\pi_0(x_0)}\cdot\frac{\pi_2(x_1)}{\pi_1(x_1)}\cdots \frac{\pi_T(x_{T-1})}{\pi_{T-1}(x_{T-1})} \end{equation}\] <p>Notice that for all the intermediate $t$s that are not equal to 0 or $T$, the unnormalized distribution $\pi_t(x)$ always appears in the numerator <em>and</em> denominator once, meaning that we don’t need to estimate the normalizing coefficients $Z_t$ as they cancel out.</p> <p>Putting all of this together, the AIS algorithm proceeds as follows (see appendix <a href="#a2-ais-importance-weights**">A.2</a> for something a bit more formal):</p> <blockquote> <ol> <li>sample $x_0\sim q(x)$</li> <li>set $w_0=Z_0$</li> <li>for $t=1,\cdots,T$:</li> <li>$\qquad$set $w_t=w_{t-1}\cdot\frac{\pi_t(x_{t-1})}{\pi_{t-1}(x_{t-1})}$</li> <li>$\qquad$sample $x_t\sim \pi_t(x)$ starting from $x_{t-1}$</li> </ol> </blockquote> <p>That’s it.</p> <h3 id="small-notes">Small Notes</h3> <p>For this post, I chose a particular (“standard”) way to define the intermediate distributions $\pi_t(x)$. However, any set of intermediate distributions can be chosen, as long as the unnormalized form of each of them can be calculated and the change is gradual enough.</p> <p>Additionally to that, while $\pi_t(x)=\pi_0^{1-\beta(t)}(x)\pi_T^{\beta(t)}(x)$ is the definition most often used in practice, $\beta(t)$ is usually <em>not</em> just linear in $t$. There are many options for the scheduling/annealing of $\beta(t)$, where different heuristics are taken into account in the definition of the schedule.</p> <h3 id="examples-and-visualizations">Examples and Visualizations</h3> <details><summary>Implementation details</summary> <p>In all of the following examples, I’m using Langevin dynamics or the Metropolis corrected version (<a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">called MALA</a>) with a single step as the MCMC algorithm between intermediate distributions. Moreover, I always used $q(x)=\mathcal{N}(x;\ 0, I)$ as the proposal distribution.</p> <p>To be honest, this would not work in any real application - a single Langevin step <em>doesn’t</em> sample from the distribution (you usually need many more steps). Luckily, for these visualizations a single step <em>was</em> enough and conveys the message equally well, so I’d rather keep the simpler approach for now.</p> </details> <p>The first example is really simple - the target and proposal distributions are both Gaussian:</p> <div class="l-page"> <p align="center"> <img src="https://github.com/friedmanroy/friedmanroy.github.io/blob/master/assets/blog_figs/AIS/two_gaussians.gif?raw=true" alt="AIS from one Gaussian to another, non-isotropic Gaussian." style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> Figure 1: a really simple example of using AIS to anneal between a standard normal to another (non-isotropic) Gaussian. Brighter values indicate regions with higher probability, and the two black dots are the samples across the intermediate distributions. Notice how the intermediate distributions "pull" the two samples after them, finally reaching the target distribution. </div> <p>An important advantage of AIS is that it anneals between a simple distribution, slowly morphing into the more complicated distribution. If properly calibrated, this allows it to sample from all modes:</p> <div class="l-page"> <p align="center"> <img src="https://github.com/friedmanroy/friedmanroy.github.io/blob/master/assets/blog_figs/AIS/1to3_gaussians.gif?raw=true" alt="AIS from one Gaussian to another, non-isotropic Gaussian." style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> Figure 2: AIS from one Gaussian to a mixture of 3 Gaussians. When the proposal distribution is properly set, the annealing process ensures that all modes are properly "covered". </div> <p>Of course, AIS can be used to sample from much more complex distributions:</p> <div class="l-page"> <p align="center"> <img src="https://github.com/friedmanroy/friedmanroy.github.io/blob/master/assets/blog_figs/AIS/spiral.gif?raw=true" alt="AIS from one Gaussian to another, non-isotropic Gaussian." style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> Figure 3: Notice how AIS doesn't "waste" samples in regions with practically 0 density towards the end. </div> <p><br/></p> <h1 id="importance-weights"><strong>Importance Weights</strong></h1> <d-byline></d-byline> <p>The mathematical trick of AIS is the way we defined the weights, $w_T$ (see <a href="#a2-ais-importance-weights">A.2</a> for more details regarding the definition). Like in regular importance sampling, the weights are defined in such a way that:</p> \[\begin{equation} \mathbb{E}_{x_0\sim q}\left[w_T\right]=Z_T \end{equation}\] <p>So, we can use $M$ samples $x_T^{(1)},\cdot,x_T^{(M)}$ and importance weights $w_T^{(1)},\cdots,w_T^{(M)}$ created using the AIS algorithm to estimate the expectation from equation \eqref{eq:f-exp}:</p> \[\begin{equation} \overline{f}\approx\hat{f}= \frac{\sum_i^M w_T^{(i)}f(x_T^{(i)})}{\sum_i^M w_T^{(i)}} \end{equation}\] <p>In fact, this $\hat{f}$ is an unbiased estimator for $\overline{f}$!</p> <h3 id="calculations-in-log-space">Calculations in Log-Space</h3> <p>If you’ve ever dealt with probabilistic machine learning, you probably already know that multiplying many (possible very small) probabilities is a recipe for disaster. This is also true here.</p> <p>Recall:</p> \[\begin{equation} w_T=Z_0\cdot\frac{\pi_1(x_0)}{\pi_0(x_0)}\cdot\frac{\pi_2(x_1)}{\pi_1(x_1)}\cdots\frac{\pi_T(x_{T-1})}{\pi_{T-1}(x_{T-1})} \end{equation}\] <p>In almost all practical use cases, the values $\pi_i(x)$ are going to be very small numbers. So, $w_T$ is the product of many small numbers. If $T$ is very large, it is almost guaranteed that the precision of our computers won’t be able to handle the small numbers and eventually we’ll end up with $w_T=0/0$.</p> <p>Instead, the importance weights are usually calculated in <em>log-space</em>, which modifies the update for the importance weight into:</p> \[\begin{equation} \log w_t=\log w_{t-1}+\log \pi_t(x_{t-1})-\log\pi_{t-1}(x_{t-1}) \end{equation}\] <p>The log-weights can then be averaged to get an estimate of $\log Z_t$… well, almost.</p> <p>Averaging out the log-weights gives us $\mathbb{E}_{x_0\sim q(x)}[\log w_T]$ , however by Jensen’s inequality <d-cite key="grosse2015"></d-cite>:</p> \[\begin{equation} \mathbb{E}_{x_0\sim q}[\log w_T] \le \log \mathbb{E}_{x_0\sim q}[w_T]=\log Z_T \end{equation}\] <p>So, when we use the log of importance weights, it’s important to remember that they only provide us with a <em>stochastic lower bound</em><d-footnote>The lower bound is stochastic because we only get an estimate of $Z_T$ when the number of samples is finite. This makes things a bit hard sometimes: the variance of the estimator can sometimes push the estimate to be larger than the true value, even though it's a lower bound!</d-footnote> of the normalization constant. When $T$ is very large, it can be shown that the variance of the estimator tends to 0, meaning the lower bound becomes tight.</p> <p>Bottom line is: the number of intermediate distributions $T$ should be quite large and carefully calibrated.</p> <h3 id="reversing-the-annealing">Reversing the Annealing</h3> <p>There is a silver lining to the above. If we reverse the AIS procedure, that is start at $\pi_T(x)$ and anneal to $\pi_0(x)$, then we can generate a <em>stochastic upper bound</em> of $Z_T$.</p> <p>Keeping the same notation as above, let $w_T$ be the importance weights of the regular AIS and $m_0$ be the importance weights of the reverse annealing. Then:</p> \[\begin{align} \mathbb{E}_{x_T\sim p}[\log m_0]&amp;\le \log \mathbb{E}_{x_T\sim p}[m_0]=\log\frac{1}{Z_T}\\ \Leftrightarrow \log Z_T&amp;\ge - \mathbb{E}_{x_T\sim p}[\log m_0] \end{align}\] <p>The only problem, which you may have noticed, is that the reverse procedure needs to start from samples out of $p(x)$, our target distribution. Fortunately, such samples were produced by the forward procedure of AIS<d-footnote>This method for finding both the stochastic lower and upper bounds is called <i>bidirectional Monte Carlo</i> <d-cite key="grosse2015"></d-cite>.</d-footnote>!</p> <p><br/></p> <h1 id="finding-your-keys"><strong>Finding Your Keys</strong></h1> <d-byline></d-byline> <p>Back to our somewhat contrived problem.</p> <p>Here’s your apartment and the PDF for $p_\text{key}(x)$ representing the distribution of probable key placements:</p> <div class="l-page"> <p align="center"> <img src="https://github.com/friedmanroy/friedmanroy.github.io/blob/master/assets/blog_figs/AIS/room_distribution.png?raw=true" alt="AIS from one Gaussian to another, non-isotropic Gaussian." style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> Figure 4: The floor plan with the density of finding the keys at each point in space (brighter is higher density). It's impossible to find the keys outside the house or in the walls, so the darkest blue in this image should be treated as essentially 0 density. </div> <p>Your place is really big<d-footnote>Yeah, the floor design isn't that good... but I'm not an architect or anything, so it's fine.</d-footnote>!</p> <p>As you can see, there are rooms more likely and less likely to contain the keys and there are regions where it would be almost impossible to find the keys (all the places with the darkest shade of blue). Such places are, for instance, outside the house, in the walls or in the middle of a hallway.</p> <p>Conveniently, the rooms are numbered. We want to estimate, given this (unnormalized) PDF the probability that the keys are in a room, say room 7:</p> \[\begin{equation} P(\text{keys}\in R_7)=? \end{equation}\] <p>Well, let’s use AIS to calculate the importance weights. Here’s the compulsory animation:</p> <div class="l-page"> <p align="center"> <img src="https://github.com/friedmanroy/friedmanroy.github.io/blob/master/assets/blog_figs/AIS/keys.gif?raw=true" alt="AIS from one Gaussian to another, non-isotropic Gaussian." style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> Figure 5: Running AIS on the floor plan. The points towards the end really look as if they were sampled from the correct distribution, even though it's such a weird one. Also, note that I ran this algorithm for many more iterations than the previous ones - this helped the sampling procedure, but could probably be done with less iterations. </div> <details><summary>More implementation details</summary> <p>Unlike the previous animations, for these trajectories I actually used 100 samples and am only showing 30 (otherwise everything would be full of moving black dots). Also, notice that towards the end of the AIS procedure the particles get “stuck”; this is because I used Metropolis-Hastings acceptance steps<d-footnote>If you are unfamiliar with this term, don't sweat it. Basically, I used a method that rejects sampled points that aren't from the distribution I'm trying to sample from.</d-footnote> and most of the sampling steps towards the end were rejected, because of the really small densities at the edges of the rooms.</p> <p>Also, the annealing for this animation was a bit tricky to set. Because the density outside the house is basically constant (and equal to 0), if the annealing isn’t carefully adjusted points have a tendency of getting stuck there. My solution was to also anneal the impossibility of being in those regions, just in a much slower pace than the other parts of the distribution<d-footnote>If you've ever heard of <i>log-barriers</i> in optimization, then I think it's basically the same concept.</d-footnote>.</p> </details> <p>Using the importance weights accumulated during this sampling procedure, we can now calculate the probability of the keys being in any one of the rooms, for instance room 7:</p> \[\begin{align} P(\text{keys}\in R_7)&amp;=\mathbb{E}_x\left[\textbf{1}[x\in R_7]\right]\\ &amp;\approx\frac{\sum_i w_T^{(i)}\cdot \textbf{1}[x\in R_7]}{\sum_i w^{(i)}_T} \end{align}\] <p>Using this formula to calculate the probabilities of the keys being in each of the rooms, we get:</p> <div class="l-page"> <p align="center"> <img src="https://github.com/friedmanroy/friedmanroy.github.io/blob/master/assets/blog_figs/AIS/key_probabilities.png?raw=true" alt="AIS from one Gaussian to another, non-isotropic Gaussian." style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> Figure 6: The same floor plan, only with the probabilities of the keys being in any of the rooms overlayed on top. Brighter rooms have higher probability. </div> <p>And there you have it! You should probably check in either room 9 or 6 and only then search in the other rooms.</p> <p><br/></p> <h1 id="practical-applications-of-ais"><strong>Practical Applications of AIS</strong></h1> <d-byline></d-byline> <p>While I believe the example in this post is good for visualization and intuition, it’s pretty silly (as I already mentioned). In 2D, rejection sampling probably achieves the same results with much less fuss.</p> <p>The more common use for AIS that I’ve seen around is as a method for <em>Bayesian inference</em> (e.g. <d-cite key="wu2016"></d-cite>).</p> <p>Suppose we have some prior distribution $p(\theta;\ \varphi)$ parametrized by $\varphi$ and a likelihood $p(x\vert\theta)$. Bayesian inference is, at it’s core, all about calculating the posterior distribution and the evidence function:</p> \[\overbrace{p(\theta\vert x;\varphi)}^\text{posterior}=\frac{p(\theta)\cdot p(x\vert \theta)}{\underbrace{p(x;\varphi)}_\text{evidence}}\] <p>For most distributions in the real world this is really really hard. As a consequence, using MCMC methods for sampling from the posterior (or <em>posterior sampling</em>) is very common. However, such methods don’t allow for calculation of the evidence, which is one of the primary ways models are selected in Bayesian statistics.</p> <p>AIS offers an elegant solution both to posterior sampling and evidence estimation. Let’s define our proposal and target distributions once more, adjusted for Bayesian inference:</p> \[\begin{equation} \pi_0(\theta)=p(\theta;\ \varphi)\qquad\ \ \ \ \ \ \ \ \pi_T(\theta)=p(\theta;\varphi)\cdot p(x\vert\ \theta) \end{equation}\] <p>As you have probably already noticed, $\pi_T(\theta)$ is the unnormalized version of the posterior. The normalization constant of $\pi_T(\theta)$ is exactly the evidence. We only need to choose an annealing schedule between the proposal and target distributions. Taking inspiration from our earlier annealing schedule, we can use (for example):</p> \[\begin{equation} \pi_t(\theta)=p(\theta;\varphi)\cdot p(x\vert\theta)^{\beta(t)} \end{equation}\] <p>where $\beta(0)=0$ and $\beta(T)=1$.</p> <p>That’s it. If $T$ is large enough, then we can be sure that the samples procured from the AIS algorithm will be i.i.d. from the posterior. Moreover, the weights $w_T^{(i)}$ can be used to estimate the evidence:</p> \[\begin{equation} p(x;\varphi)\approx \frac{1}{M}\sum_i w_T^{(i)} \end{equation}\] <p>And there you have it! Instead of simply sampling from the posterior, you can get an estimate for the evidence at the same time<d-footnote>As long as you don't use a batched method on many data points $x$ like they do in Bayesian neural networks, I don't think this will work there (although variants do exist).</d-footnote> <br/></p> <h1 id="conclusion"><strong>Conclusion</strong></h1> <d-byline></d-byline> <p>You now (maybe) know what annealed importance sampling is and how to use it. My main hope was to give some intuition into what happens in the background when you use AIS. I find the concept of sampling by starting at a simple distribution and moving to a more complex one really cool, especially when it is treated in such a clear and direct manner.</p>]]></content><author><name>Roy Friedman</name></author><category term="sampling"/><category term="MCMC"/><category term="machine-learning"/><summary type="html"><![CDATA[Suppose you can’t find your keys. You know you left them in your apartment somewhere, but don’t remember where. This happens pretty often, so you have a keep note of the possible places you may have left your keys. You want to find out the probability that the keys are in a specific room of your apartment. Let’s call this room $R$ - mathematically speaking, you want to calculate: \(\begin{equation} P\left(\text{keys}\in R\right)=\intop \mathbf{1}\left[x\in R\right]\cdot p_\text{keys}(x)dx \end{equation}\)]]></summary></entry><entry><title type="html">Boltzmann Waiting at a Bus Stop</title><link href="https://friedmanroy.github.io/blog/2023/Entropy/" rel="alternate" type="text/html" title="Boltzmann Waiting at a Bus Stop"/><published>2023-08-17T00:00:00+00:00</published><updated>2023-08-17T00:00:00+00:00</updated><id>https://friedmanroy.github.io/blog/2023/Entropy</id><content type="html" xml:base="https://friedmanroy.github.io/blog/2023/Entropy/"><![CDATA[<p>Your’e late. Again. Your final exam in probability theory starts in 30 minutes and the bus <em>is not showing up.</em> The pamphlet on the station says it arrives “approximately once every 10 minutes”, and you’ve already been waiting for 12 with no bus on the horizon. <em>What are the odds?! Why does this always happen???</em></p> <p>In an attempt to distract yourself from your anxiety, you start thinking: “Actually, what is the probability?” Say you were asked during the exam to write down a distribution of a bus arriving “approximately once every 10 minutes”, what would that distribution look like? Well, “once every 10 minutes” seems clear enough, but what does “approximately” mean here? How do we take into account that there’s road construction seemingly <em>everywhere</em> today, adding a bunch of variability to the travel times from the first station?</p> <p>Modeling the uncertainty of a situation is not a problem limited to exams, especially where we have access to some, but not all, of the affecting variables. In fact, solving this problem is one of the most common tasks in scientific modeling, dressed up in different forms and in a range of diverse situations.</p> <p>The difficulty, of course, is to incorporate only the available information, without letting anything else sneak into the model. In this post we will translate this sentiment into a precise mathematical statement, useful for solving problems such as your frequently tardy bus, finding a distribution which appropriately uses all of our relevant knowledge without assuming anything beyond.</p> <hr/> <h1 id="youre-waiting-for-a-bus">You’re Waiting for a Bus</h1> <p>Let’s start with the simplest possible formulation of the problem, using very strong and unrealistic simplifying assumptions. Then, we can slowly add in relevant real-world complications. Doing things this way will allow us to give us an indication of possible complications we might encounter, while also providing some insight that maybe, hopefully, will help us solve the more complex and realistic variants.</p> <p>One more thing - all of the models considered will be discrete; that is, we will only care about the resolution of minutes. Of course, this whole analysis can be extended to the resolution of seconds, or even continuous time, but using discrete distributions on the scale of minutes keeps everything simple.</p> <h2 id="ideal-world---deterministic-model">Ideal World - Deterministic Model</h2> <p>In an ideal world, the bus arrives <em>exactly</em> every 10 minutes. Never any delays, no problems, a new bus just arrives punctually every 10 minutes.</p> <p>So, in this idealized world, no matter what time you arrive at the bus stop, the most you will ever have to wait is 10 minutes. Of course, since the schedule is unknown, the probability that you wait any amount of time (smaller than 10 minutes) is the same.</p> <p>This is exactly the description of a uniform distribution between 1 and 10 minutes: \(\begin{equation} \forall\ t\in \left\{1,\cdots,10\right\}\qquad\quad P(T=t)=\frac{1}{10} \end{equation}\) where the random variable $T$ represents the arrival time of the bus.</p> <h2 id="possible-delays---infinite-delay-model">Possible Delays - Infinite Delay Model</h2> <p>The deterministic model above, with a bus that arrives exactly every 10 minutes, is a good place to start. Obviously, however, it is unrepresentative of the situation you find yourself in. For one thing, you’ve already waited 14 minutes, which is impossible under said model. Additionally, busses can get delayed in their route, adding variability to their arrival times and we have to take that into account.</p> <p>One important feature of the deterministic model is that we gain information the more time we wait. If we’ve already waited for 9 minutes, we know for certain that the bus will arrive in the next minute. As we saw, we don’t have this certainty in the real world scenario.</p> <p>Let’s start by thinking of the opposite extreme than the deterministic model. So now let’s say that busses start their journey exactly 10 minutes apart, but then they get delayed for time periods that are so wildly variable that their arrival times are essentially distributed uniformly from the time the first bus arrives to the arrival time of the last bus of the day.</p> <p>It may seem that this setting violates the sign that states that “busses arrive approximately every 10 minutes”. Note though that, since a bus leaves every 10 minutes, there are $\frac{N}{10}$ busses distributed uniformly in the $N$ minutes from the first bus arrival to the time the last one arrives. So the average difference between two consecutive arrivals still turns out to be 10 minutes.</p> <p>In this extreme, waiting for any amount of time in the station gives us absolutely no information! The bus arrival times can be effectively thought of as independent events and so the time to wait is independent of how long we have already waited. This property is known as <em>memorylessness</em>.</p> <p>Amazingly, the <em>only</em> discrete, memoryless, distribution is the geometric distribution: \(\begin{equation} \forall\ t=1,2,\cdots \qquad\quad P\left(T=t\right)=\theta\cdot\left(1-\theta\right)^{t} \end{equation}\)where $\theta\in[0,1]$ is a parameter of the distribution that has to be defined in some manner. <em>We know</em> that a bus arrives approximately every 10 minutes, that is the mean of the distribution has to be equal to 10; we can use this to choose $\theta$. <a href="https://en.wikipedia.org/wiki/Geometric_distribution">Luckily for us, the geometric distribution is a well known distribution</a> and it’s mean is equal to: \(\begin{equation} \mathbb{E}[T]=\frac{1-\theta}\theta \end{equation}\) So, if we know that $\mathbb{E}[T]=M=10$, we can set $\theta=\frac{1}{M+1}$ to satisfy the constraint of the average arrival time. Here’s the corresponding distribution:</p> <div class="fake-img l-page"> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/MaxEntropy/noisy_model.png" style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> At the limit of very large variance in the delay of busses, the arrival time is distributed geometrically. </div> <h2 id="modeling-finite-delays">Modeling Finite Delays</h2> <p>Okay, so a geometric distribution (which we will call the “infinite delay model” from now on) is much better than the deterministic model, but it doesn’t make sense that a bus could be “infinitely” delayed. As long as the bus’s route is sound and wasn’t canceled or something, the bus should probably arrive within the same day, if not earlier. This is another tidbit of information we can take into account when defining our model. In other words, we still believe that the variability between the delays is very large, but there is also a “maximal waiting time” which we will take into account.</p> <p>So, how do we find a model that takes into account that the delays have to be finite? For the last two settings we had sound arguments which led us to a unique distribution of arrival times. This time however, there are numerous distributions that could fit our setting, the setting where the mean is known and the number of possible arrival times is finite. Here are three such possible distributions with a wait time of 50 minutes:</p> <div class="fake-img l-page"> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/MaxEntropy/possible_dists.png" style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> Three possible distributions to describe the bus arrival times under our constraints. In particular, all 3 of the distributions are defined on a finite set of values - that is, there is a maximal time the bus can be delayed - and the mean of all three distributions is $M=10$. Which of these distributions best describes the situation at hand? Can we be sure that they don't insert bias into the model? </div> <h2 id="a-broader-look">A Broader Look</h2> <p>If we take a step back for a moment, we can appreciate that what we’re looking for is more general than the issue of arrival times of busses. Abstracting the problem at hand reveals a much more fundamental question:</p> <blockquote> <p><strong>Given a set of constraints, is there a principled way to find a probabilistic model that will satisfy said constraints without assuming further information?</strong></p> </blockquote> <p>By principled we mean a single, unified, approach to choosing the distribution; no matter which constraints are given, we will know what needs to be done in order to find a distribution that satisfies them. Both the deterministic and geometric settings were definitely not solved in a principled manner: a different argument had to be made up for each specific case and were quite distinct from each other, predicated on our knowledge of esoteric properties of distributions.</p> <p>Well, how about we find a single way to solve problems of this kind, one that we can always repeat and understand?</p> <hr/> <h1 id="maximum-entropy">Maximum Entropy</h1> <p>To choose between a set of possibilities, we need a criterion which will tell us which distribution is “best”. This criterion is subjective, but important. Usually in mathematical modeling, distributions that do not insert any further bias are preferred - those that do not assume we have access to more information than we have observed. Simply put, we would usually prefer a model which is as “generic” as possible while still capturing quantities we care about.</p> <p>This sounds reasonable, but what does it even mean for a distribution to be generic? Consider the following two distributions:</p> <div class="fake-img l-page"> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/MaxEntropy/generic_and_not.png" style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> Two different distributions, one that is close to uniform (left) and one that is closer to an indicator (right). Which of these is the more generic distribution? </div> <p>The distribution on the left spreads out the probability almost equally among all possible outcomes. Conversely, the one on the right assigns most of the probability to a single outcome. Which would you call more generic?</p> <p>Well, the distribution on the right represents a very specific setting where one of the values is so much more probable than all of the other values. On the other hand, the distribution on the left is how we would describe a setting we know very little about. If we’ve only seen very few real-world samples, we would probably choose to model the empirical distribution with the more uniform distribution, since it assumes less. In particular, it doesn’t assume that one of the values is significantly more probable to occur.</p> <p>Let’s try to build up a measure for the genericness of a distribution.</p> <h2 id="enter-shannon">Enter Shannon</h2> <p>Looking at $P(X=x)$ for a specific value of $x$ already gives us a strong indication of whether a distribution is generic or not, right? If $P(x)$ <d-footnote>We will use this notation from now, since it's so much shorter while still being easy to understand</d-footnote> is large, then it alone is damning evidence that the distribution is not generic. Alternatively, when $P(x)$ is small for a specific value of $x$ it is rather hard to say whether the distribution is generic or not, since the probability can still be large for a different value of $x$; we will say that the probability of the outcome is <em>uninformative</em><d-footnote>For those versed in information theory: you will notice that syntactically we use _exactly the opposite_ word than in usual information theoretic contexts. However, semantically we mean the same as in information theory, and the same conclusion is reached (i.e. please don't be angry with us).</d-footnote>. We can use this as a base ingredient to define the genericness of the whole distribution. In particular, a generic distribution will be one that, on average, is uninformative: \(\begin{equation} \text{genericness}=\sum_x P(x)\cdot I(x) \end{equation}\) where $I(\cdot)$ represents the uninformative-ness of an observation $x$.</p> <p>There are many many ways to define $I(\cdot)$ so it’s behavior will match our abstract description. But, it would be <em>nice</em> if the function $I(\cdot)$ could also have the following properties:</p> <ol> <li>As we said, values with higher probabilities are more informative (less uninformative), so $I(x)$ should be monotonically decreasing with $P(x)$</li> <li>Since we are going to attempt to find distributions that maximize the genericness, it would help if $I(\cdot)$ were continuous in $P(x)$ (so we could differentiate the genericness with respect to the probabilities). In turn, we need $I(x)$ to be continuous in $P(x)$</li> <li>If $P(x)=1$, i.e. the outcome is certain, then $I(x)=0$, the lowest possible value</li> <li>If we observe two different outcomes, say $x_1$ and $x_2$, the uninformative-ness of one shouldn’t effect the uninformative-ness of the other. That is to say, the product of uninformative-ness of a couple values should be equal to their sum: \(\begin{equation} I(x_1\cdot x_2)=I(x_1)+I(x_2) \end{equation}\)</li> </ol> <p>Under these criteria, finding a specific form is (perhaps suspiciously) simple. Properties 1 and 2 are easily met if $I(x)=f(\frac{1}{P(x)})$ and $f(\cdot)$ is a continuous, monotonically increasing function. Of these possible functions, the first that comes to mind that combines properties 3 and 4 is the logarithm <d-footnote>By logarithm we mean the natural logarithm, but we will use the information-theoretic notation $\log(x)=\ln(x)$</d-footnote>, which implies the following form for the uninformative-ness: \(\begin{equation} I(x)=\log\left(\frac {1}{P(x)}\right)=-\log P(x) \end{equation}\) Incredibly, it turns out that the logarithm is the <em>only</em> function that fulfills all of our required properties! <d-footnote>Well, up to a constant anyway, a fact we will completely ignore from now on.</d-footnote></p> <p>So, as long as our earlier requirements for un-informativeness make sense, the genericness of a distribution is given by: \(\begin{equation} \text{genericness}=-\sum_x P(x)\log P(x) \end{equation}\) Now, actually, this value has a much better name than “genericness”; it is the <em>entropy</em> of a distribution, usually denoted by: \(\begin{equation} H(X)=-\sum_x P(x)\log P(x) \end{equation}\)</p> <h2 id="back-to-our-busses---a-sanity-check">Back to Our Busses - a Sanity Check</h2> <p>Before continuing on, let’s see whether maximizing the entropy really results in the distribution we’ve chosen for the deterministic model from earlier in this post. A simple sanity check to see that we are faithful to our earlier work.</p> <p>For the deterministic model, the only unknown factor is when you arrive between the equally spaced busses. So, the distribution is over a finite set of values $T\in\left{1,\cdots,10\right}$, and that is the only thing we know about it. As mentioned before, the uniform distribution is the one with the highest entropy (in other words, is most generic) on a finite support. This is the same distribution as we saw before!</p> <p>The fact that using the entropy is consistent with our earlier arguments is reassuring. But, how about the infinite delay model, the one with the geometric distribution? Well, for that we need to insert the fact that we know the mean of the distribution into this framework.</p> <h2 id="adding-constraints">Adding Constraints</h2> <p>Back to the problem at hand. We have a set of constraints and want to find the most general distribution possible that still satisfies these constraints. So, what were our constraints?</p> <ol> <li>The distribution is discrete $T\in\left{1,\cdots,K\right}$ (let’s assume there are only finitely many values for now)</li> <li>The mean of the distribution is equal to $M$</li> </ol> <p>Let’s rewrite the probabilities as: \(\begin{equation} p_t=P(T=t) \end{equation}\) This doesn’t change anything, but makes the next part a bit easier to write.</p> <p>Basically, we want to solve the following maximization problem: \(\begin{equation}\label{eq:maximization} \begin{aligned} (p_1,\cdots,p_K)&amp;=\arg\max_{\tilde{p}_1,\cdots,\tilde{p}_K} H(\tilde{p}_1,\cdots,\tilde{p}_K)\\ \text{subject to:}\quad &amp; (I)\quad\sum_{t=1}^Kt\cdot \tilde{p}_t=M\\ &amp; (II)\quad\forall t\quad \tilde{p}_t\ge 0\\ &amp; (III)\quad \sum_{t=1}^K\tilde{p}_t=1 \end{aligned} \end{equation}\) This looks like a lot. Unpacking the constraints, we have: $(I)$ the mean is equal to $M$ (where it all started), $(II)$ all of the probabilities are larger than 0 and $(III)$ the probabilities sum up to 1. Again, it looks quite complicated, but actually constraints $(II)$ and $(III)$ just make sure that $p_1,\cdots, p_K$ is a proper distribution.</p> <hr/> <h1 id="enter-boltzmann">Enter Boltzmann</h1> <p>Honestly, solving the problem in equation \eqref{eq:maximization} doesn’t look easy. Instead of directly trying to do so, it will be better to try and get some intuition of what’s <em>actually</em> happening.</p> <h2 id="the-simplex">The Simplex</h2> <p>As we saw, constraints $(II)$ and $(III)$ just make sure that we’re maximizing over the set of proper distributions. But how what is this space of all proper distributions?</p> <p>If a random variable can take a finite set of values (again, the thing we’re actually interested in) all possible distribution are inside what is called the <em>simplex</em>. The simplex is the set of all vectors of length $K$ whose elements are nonnegative and sum up to 1 - does that sound familiar?</p> <p>Here’s a concrete but simple example. Forget about the distribution of busses for a second, let’s look at a random variable that can only take on 3 values, $X\in{1, 2, 3}$. All possible distributions over $X$ can be written as vectors of size 3: \(\begin{equation} \left(\begin{matrix}P(X=1)\\P(X=2)\\P(X=3)\end{matrix}\right)=\left(\begin{matrix}p_1\\p_2\\p_3\end{matrix}\right) \end{equation}\)</p> <p>However, because the three elements $p_1,p_2,p_3$ have to sum up to 1, if we know $p_1$ and $p_2$ then $p_3$ is known and given by: \(\begin{equation} p_3=1-p_1-p_2 \end{equation}\) That way, we know for sure that $p_1+p_2+p_3=1$. So in fact, if $p_1$ and $p_2$ are known, $p_3$ can be deterministically set.</p> <p>So, the set of all 3-dimensional distributions can actually be visualized in 2 dimensions, the simplex:</p> <div class="fake-img l-page"> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/MaxEntropy/simplex_3d.png" style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> The set of all possible 3-dimensional distributions. Notice that although the distribution is indeed 3-dimensional, the set of distributions is actually a _closed_ (and convex) 2-dimensional set of points. </div> <p>(As an aside, using the same reasoning any $D$-dimensional distribution can be described by a $(D-1)$ -dimensional vector.)</p> <h2 id="entropy-with-constraints">Entropy with Constraints</h2> <p>Visualizing the set of possible distributions in this manner allows us to easily overlay the entropy on top of the simplex:</p> <div class="fake-img l-page"> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/MaxEntropy/simplex_entropy.png" style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> Plot of the entropy of each distribution in the 3-dimensional simplex; darker means higher entropy. Notice how the corners of the simplex have very low entropy, which gradually rises, finally peaking at the center of the simplex. The point with maximal entropy, where the star is, also happens to be the uniform distribution $(1/3,\ 1/3,\ 1/3)$. </div> <p>We started all of this by saying we want to constrain the mean of the distribution to be equal to $M$. We can now directly visualize this, focusing our search to a specific region on the simplex:</p> <div class="fake-img l-page"> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/MaxEntropy/simplex_expectation_constraint_with_max.png" style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> Again , the entropy of the distributions in the simplex, but now we restrict our attention to a subset: the set of distributions with a particular mean. This set is contained within the red line, and the distribution inside this set that attains the maximal entropy is the one denoted by the star. </div> <p>So, if we want to find the distribution that maximizes the entropy whose mean is equal to a specific value, all we have to do is find the maximal entropy in the region depicted by the constraint!</p> <div class="fake-img l-page"> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/MaxEntropy/entropy_mean_const.png" style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> The entropy of distributions in the region with a constrained mean. Again, the maximal value is represented by the star. Notice that since the constrained region is convex, and the entropy is a convex function, that the problem we need to solve is also, inevitably, convex. </div> <p>In fact, using this we can maximize the entropy under <em>any</em> constrain we want and with as many of them as we want, as long as there exists a distribution that satisfies all of the constraints. Different constraints will force us to search in different parts of the simplex, but in theory any constraint is valid.</p> <h2 id="boltzmann-distributions">Boltzmann Distributions</h2> <p>Okay, okay, so we have some basic intuition for what we’re trying to do. Now, how do we find the maximum entropy distribution <em>mathematically</em>?</p> <p>For now, let’s restrict ourself constraints of the form<d-footnote>Actually, any constraint will work. However, this is a very common set of constraints (basically making sure some of the moments are equal to a specific value) and their solution takes on a very neat, general distribution.</d-footnote>: \(\begin{equation} \sum_x P(x)f(x)=F \end{equation}\) where $F$ is simply a number and $f(\cdot)$ is some function over the possible values of the random variable $X$. A possible example of $f(\cdot)$ is simply the identity function, in which case we get the constraint over the mean we have been talking about for so long now. But, let’s continue using the more general notation.</p> <p>To find the maximum, we need to use Lagrange multipliers. Under their framework, we define the following function: \(\begin{equation} \mathcal{L}\left(p_1,\cdots,p_K,\lambda,\gamma\right)=\underbrace{H(X)}_{\text{entropy}}-\lambda\cdot\left(\sum_x p_xf(x)-F\right)-\gamma\cdot\left(\sum_x p_x-1\right) \end{equation}\) If you haven’t heard of Lagrange multipliers, all you need to know is that the stationary points of $\mathcal{L}(\cdot)$ will also be stationary points of our constrained problem. These stationary points will (after some math) give us the following form for the distribution: \(\begin{equation} P(X=x)=p_x=\frac{1}{Z}e^{\lambda f(x)}\qquad\quad Z=\sum_xe^{\lambda f(x)} \end{equation}\)</p> <details><summary>Click here to see the derivation</summary> <p>We need to find the stationary points of the function $\mathcal{L}(\cdot)$ with respect to $p_1,\cdots,p_K$ as well as $\lambda$ and $\gamma$. Notice that $\mathcal{L}(\cdot)$ is convex in all of these (the entropy is a convex function)! This is a good thing - we can just differentiate, equate to 0, and we will have found the maximum.</p> <p>We’ll do just that: \(\begin{equation} \begin{aligned} \frac{\partial\mathcal{L}}{\partial p_x} = -\log p_x-\frac{p_x}{p_x} &amp;-\lambda\cdot f(x)-\gamma\stackrel{!}{=}0\\ \Rightarrow \log p_x &amp;= \lambda\cdot f(x)+\gamma+1\\ \Rightarrow p_x &amp;= e^{\lambda f(x)}\cdot \Gamma \end{aligned} \end{equation}\) where, for simplicity, we have defined $\Gamma=\exp[\gamma + 1]$. So we have the general form of the distribution… all that’s left is to find what $\Gamma$ equals.</p> <p>We can find the value of $\Gamma$ (by extension $\gamma$) by making sure that the distribution sums up to 1: \(\begin{equation} \begin{aligned} \sum_x p_x&amp;=\Gamma\cdot\sum_x e^{\lambda f(x)}=1\\ \Rightarrow \frac{1}{\Gamma}=Z&amp;= \sum_x e^{\lambda f(x)} \end{aligned} \end{equation}\)</p> </details> <p>This distribution is called the <em>Boltzmann distribution</em> and the normalization constant $Z$ is called the <em>partition function</em> (it <em>is</em> a function, just of $\lambda$ <em>not</em> of $x$)<d-footnote>The eagle-eyed readers will notice that this is exactly the definition of the <a href="https://en.wikipedia.org/wiki/Exponential_family#Entropy">exponential family of distributions</a>! In fact, every distribution from the exponential family maximizes the entropy under different moment constraints (and supports)</d-footnote>.</p> <p>The only problem is that we still have that pesky $\lambda$ stuck in there. We can find $\lambda$ by ensuring that our original constraint is satisfied: \(\begin{equation} \sum_x P(x)\cdot f(x)=F\ \ \Leftrightarrow \ \ \frac{1}{Z}\sum_x e^{\lambda f(x)}f(x)=F \end{equation}\) Unfortunately, analytically finding $\lambda$ in general is actually quite difficult; usually, numerical methods are used to find it.</p> <h2 id="one-last-sanity-check">One Last Sanity Check</h2> <p>A while ago we introduced a setting where the bus can be infinitely delayed and all we know is that the average wait time between busses is $M$ minutes. Our model in this setting was the geometric distribution with parameter $\theta=\frac{1}{M+1}$. Let’s try to make sure that this really is the distribution that maximizes the entropy.</p> <p>Because our only constraint is the mean, we need to solve: \(\begin{equation} \begin{aligned} \frac{1}{Z}\sum_{t=1}^\infty e^{\lambda t}\cdot t &amp;\stackrel{!}{=}M \\ \Leftrightarrow \sum_{t=1}^\infty e^{\lambda t}\cdot t&amp;\stackrel{!}{=}M\cdot \sum_{t=1}^\infty e^{\lambda t} \end{aligned} \end{equation}\) Let’s break this problem into smaller parts and look at the following geometric series: \(\begin{equation} \sum_{t=1}^\infty \left(e^\lambda\right)^t \end{equation}\) This geometric series converges so long as $e^\lambda&lt;1$. For now, let’s just assume that this actually happens. Then: \(\begin{equation} \sum_{t=1}^\infty \left(e^\lambda\right)^t = \frac{1}{1-e^\lambda} \end{equation}\)</p> <p>We are one step closer. We now only need to find a $\lambda$ so that: \(\begin{equation} \sum_{t=1}^\infty e^{\lambda t}\cdot t\stackrel{!}{=}\frac{M}{1-e^\lambda} \end{equation}\) Now it’s time to take care of the left hand side of this equation. Notice that: \(\begin{equation} \sum_{t=1}^\infty e^{\lambda t}\cdot t=\sum_{t=1}^\infty \frac{d}{d\lambda}e^{\lambda t}=\frac{d}{d\lambda}\frac{1}{1-e^\lambda}=\frac{e^\lambda}{\left(1-e^\lambda\right)^2} \end{equation}\)</p> <p>Plugging this in (after some algebra), gives the following value for $\lambda$:</p> \[\begin{equation} \lambda = \log\frac{M}{M+1} \end{equation}\] <details><summary>Click here if you want to see the algebra needed to get there</summary> \[\begin{aligned} \frac{e^\lambda}{\left(1-e^\lambda\right)^2}&amp;=\frac{M}{1-e^\lambda}\\ \Rightarrow \frac{e^\lambda}{1-e^\lambda}&amp;=M\\ \Rightarrow\frac{1}{e^{-\lambda}-1}&amp;=M\\ \Rightarrow e^{-\lambda} &amp;=\frac{1}{M}+1\\ \Rightarrow \lambda &amp;= \log(\frac{M}{M+1}) \end{aligned}\] <p>Recall that we assumed that $e^\lambda &lt; 1$ - let’s just quickly make sure that this is correct. Notice that $1+\frac1M &gt; 1$, so $\log(1+\frac1M)&gt;0$. So, $\lambda$ is negative, thus $e^\lambda=\frac{1}{e^{\vert\lambda\vert}}&lt;1$ and everything is okay.</p> </details> <p>This doesn’t look like the geometric distribution <em>yet</em>, but let’s shift around some of the terms. Simplifying the smallest element of the distribution is always a good first step: \(\begin{equation} \begin{aligned} e^{\lambda t}&amp;=\left(e^\lambda\right)^t=\left(\frac{M}{M+1}\right)^t \end{aligned} \end{equation}\) Plugging this into the partition function we get <em>yet another</em> geometric series (no wonder the distribution is called geometric): \(\begin{equation} Z=\sum_{t=1}^{\infty}\left(\frac{M}{M+1}\right)^t=\frac{1}{1-\frac{M}{M+1}}=M+1 \end{equation}\) Finally, <em>finally</em>, we substitute everything in to get: \(\begin{equation} P(T=t)=\left(1-\frac{M}{M+1}\right)\left(\frac{M}{M+1}\right)^t=\left(1-\frac{1}{M+1}\right)^t\cdot\frac{1}{M+1} \end{equation}\) which is exactly the definition of a geometric distribution with parameter $\theta=\frac{1}{M+1}$!</p> <h2 id="back-to-modeling-finite-delays">Back to Modeling Finite Delays</h2> <p>Unfortunately, there is no closed form solution to the maximization problem in equation \eqref{eq:maximization}. So, if we want to model finite delays in our bus arrival distribution, we have to solve the maximization problem numerically, except for a few settings. These specific cases will give us at least some intuition for the general behavior of the solution.</p> <p>Let’s refresh our memory. We are trying to find a distribution over a random variable $T\in\left{1,\cdots, T_{\text{max}}\right}$ whose mean is equal to $M$. In the tardy bus setting, $M=10$ and $T_{\text{max}}$ is the largest number of minutes conceivable for the bus to be delayed. For instance, if there is no way that the bus can be more than an hour late, we can set $T_{\text{max}}=60$.</p> <p>From our earlier explorations, we saw that the max-entropy distribution under these constraints will be the Boltzmann distribution: \(\begin{equation} P(T=t)=\frac{1}{Z} e^{\lambda t} \end{equation}\) where: \(\begin{equation}\label{eq:mean-maxent} \frac{1}{Z}\sum_{t=1}^{T_{\text{max}}} t\cdot e^{\lambda t}=M \end{equation}\) Of course, a solution only exists when $1\le M\le T_{\text{max}}$.</p> <h3 id="both-sides-of-the-extreme">Both Sides of the Extreme</h3> <p>Before using numerical methods, notice that when $M=1$ or $M=T_{\text{max}}$, we <em>can</em> find an analytical solution. In both cases, the only distribution that satisfies equation \eqref{eq:mean-maxent} is the indicator: \(\begin{equation} \begin{aligned} M=1:&amp;\quad P(T=t)=\begin{cases}0&amp;t\neq1\\1&amp;t=1\end{cases}=\bf{1}_{\left[t=0\right]}\\ M=T_{\text{max}}:&amp;\quad P(T=t)=\begin{cases}0&amp;t\neq T_{\text{max}}\\1&amp;t=T_{\text{max}}\end{cases}=\bf{1}_{\left[t=T_\text{max}\right]} \end{aligned} \end{equation}\)</p> <p>What are the values of $\lambda$ that give us such behaviors? Well, when $M=1$, we can rewrite our distribution as: \(\begin{equation} P(T=t)=\frac{e^{\lambda t}}{\sum_{\tau=1}^{T_{\text{max}}} e^{\lambda \tau}}=\frac{e^{\lambda t}}{e^{1\cdot\lambda}\cdot\left(1+\sum_{\tau=2}^{T_\text{max}e^{\lambda(\tau-1)}}\right)}=\frac{e^{\lambda(t-1)}}{1+\sum_{\tau=2}^{T_\text{max}e^{\lambda(\tau-1)}}} \end{equation}\) Writing the distribution in this manner is quite revealing. Let’s see what happens when $t=1$: \(\begin{equation} P(T=1)=\frac{e^{\lambda\cdot0}}{1+\sum_{\tau=2}^{T_\text{max}e^{\lambda(\tau-1)}}}=\frac{1}{1+\sum_{\tau=2}^{T_\text{max}e^{\lambda(\tau-1)}}} \end{equation}\) For any other value, the distribution will be proportional to $e^{\lambda(t-1)}$. Now, if we can find a value of $\lambda$ for which $e^{\lambda(t-1)}=0$ for all $t&gt;1$, then we will have found our solution. So, when does this happen?</p> <p>Well, only at the limit $\lambda\rightarrow -\infty$. At this limit, we get the correct distribution. Using <em>exactly</em> the same arguments, when $M=T_\text{max}$ it can be shown that $\lambda\rightarrow\infty$.</p> <h3 id="uniform-again">Uniform, Again?!</h3> <p>Actually, there is an additional situation with an analytical solution. If $M=\frac{1+\cdots+T_{\text{max}}}{T_{\text{max}}}=T_\text{mid}$, then equation \eqref{eq:mean-maxent}: \(\begin{equation} \sum_{t=1}^{T_\text{max}}\frac{e^{\lambda t}}{Z}\cdot t=\frac{1}{T_\text{max}}\sum_{t=1}^{T_\text{max}}t=T_\text{mid} \end{equation}\) Of course, this can <em>only</em> happen when $P(T=t)$ is the same for all values of $t$ - the uniform distribution (again)! What does $\lambda$ equal in this case? Well, the only option is that $\lambda=0$; that’s the only way that $P(T=t)$ will be the same for all values of $t$.</p> <div class="fake-img l-page"> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/MaxEntropy/analytical_sols.png" style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> The finite, discrete distributions with a constrained mean for which we have analytical solutions. </div> <h3 id="somewhere-inbetween">Somewhere Inbetween</h3> <p>Let’s put all of the values of $\lambda$ that we’ve found so far next to each other:</p> \[\begin{aligned} M=1\ :&amp;\qquad \lambda\rightarrow-\infty\\ M=T_\text{mid}\ :&amp;\qquad \lambda=0\\ M=T_\text{max}\ :&amp;\qquad \lambda\rightarrow\infty\\ \end{aligned}\] <p>See how there seems to be an ordering according to $\lambda$? From the lower extreme case ($M=1$), through the middling solution of the uniform distribution, and to the other side ($M=T_\text{max}$). Every solution between any of these values will have some aspect of their neighbors’:</p> <div class="fake-img l-page"> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/MaxEntropy/midway_solutions.png" style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> Distributions between the extremes we found analytically. Notice how the plot on the left has some attributes from the distribution with $\lambda\rightarrow-\infty$ and $\lambda =0$, while for the plot on the right it is vice versa. These analytical solutions give some intuition on how the distributions in between each of the ranges will behave. </div> <p>Using the analytical outcomes, we can expect that<d-footnote>We're kind of sweeping the fact that the value of $\lambda$ is continuous with respect to $M$ under the rug. This is true! The easiest way to convince yourself is to show graphically; the entropy is continuous and changing $M$ by a small amount only moves the constrained set by a small amount</d-footnote>: \(\begin{equation} \begin{aligned} 1&lt;M&lt;T_\text{mid}\ \ &amp;\Rightarrow\ \ \lambda &lt;0\\ T_\text{mid}&lt;M&lt;T_\text{max}\ \ &amp;\Rightarrow\ \ \lambda&gt;0 \end{aligned} \end{equation}\)</p> <h3 id="numerically-finding-solutions">Numerically Finding Solutions</h3> <p>Unfortunately, as mentioned, finding an analytical form for the max-entropy is difficult or completely impossible. Instead, we use numerical methods to maximize the entropy under our constraints.</p> <p>One key feature of max-entropy is that it is relatively easy to find solutions. In the first place, the entropy is a <em>convex</em> function. So, as long as the constraint defines a convex set<d-footnote>Verifying whether a constraint defines a convex set is a difficult problem in itself... However, when constraining the mean/variance, it actually is a convex set and a solution can be found.</d-footnote>, any off-the-shelf convex optimization algorithm can be used. For our problem we didn’t even need to go that far. Since there is only one parameter, $\lambda$, that we need to find we just need to check a whole bunch of values until we land on the right one (tolerating some error).</p> <p>Honestly, the solution to the problem as we set out to solve might not be totally interesting. Here are all of our models together (with numerical estimation):</p> <div class="fake-img l-page"> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/MaxEntropy/all_models.png" style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> The distributions according to all of our models. In this setting, when $M=10$ and $T_\text{max}\gg M$, the infinite delay model (center) and finite delay model are quite similar. Notice, however, that they are not exactly the same. The max-entropy distribution assigns higher probability to early times than the geometric distribution. </div> <p>Playing around with the average time we expect the bus to arrive reveals more interesting behavior:</p> <div class="fake-img l-page"> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/MaxEntropy/alternate_mean.png" style="display: inline-block; margin: 0 auto; "/> </p> </div> <div class="caption"> If we change the mean, the behavior of the noisy and finite delay model diverge. </div> <hr/> <h1 id="wrapping-up">Wrapping Up</h1> <p>Whew, that was a lot. Quite a ride.</p> <p>Starting from a naive question, we went through a lot of trouble to find the answer. The framework we derived in this post is aptly called the max-entropy principle, and was first introduced by Jayne back in 1957. But, most of the math used in this post actually originates from even earlier sources.</p> <p>In the late 19th century, Boltzmann (and later Gibbs) revolutionized thermodynamics and gave us a much deeper understanding of the field by showing how its fundamental equations could be derived using probabilistic arguments. It turns out, as we just explored in this post, that the mathematical tools they developed are relevant for much more than molecules bumping into each other. The concept of entropy started out as an abstract property of a thermodynamic system, later redefined by Boltzmann and Gibbs using their statistical physics framework, and eventually generalized by Shannon to a concept relevant to all of statistical inference.</p> <p>The same principles originally introduced for statistical physics are now used widely in machine learning and statistics. Since the maximum entropy principle is simply a way to find distributions that are generic while retaining known information, it is a method used for scientific discovery and modeling in a range of fields.</p> <p>Hopefully, even though a bit of math was involved, you now know how to model distributions under different constraints.</p> <h2 id="outro">Outro</h2> <p>It is now 25 minutes later. The bus arrived, a bit late but not too bad, and you got to the exam just in time! You found a general solution for the problem you originally posed. It took the whole bus ride, you originally wanted to go over some last-minute details for the exam, but maybe this is okay as well?</p> <p>May all of your busses arrive on time!</p>]]></content><author><name>Hagai Rappeport</name></author><category term="information-theory"/><category term="distributions"/><category term="max-entropy"/><category term="statistics"/><summary type="html"><![CDATA[Busses don't always arrive on time. Finding the appropriate distribution to describe the amount of time spent waiting for a bus seems trivial but is actually quite difficult to pin down. In this post, we explore the principle of maximum entropy and show how it pops up in this deceptively simple sounding problem. This concrete settings serves as an example of an extremely powerful technique for statistical inference, with uses in virtually every scientific field.]]></summary></entry><entry><title type="html">NeurIPS 2022 Summary</title><link href="https://friedmanroy.github.io/blog/2022/NeurIPS2022/" rel="alternate" type="text/html" title="NeurIPS 2022 Summary"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://friedmanroy.github.io/blog/2022/NeurIPS2022</id><content type="html" xml:base="https://friedmanroy.github.io/blog/2022/NeurIPS2022/"><![CDATA[<html> <head> <meta http-equiv="refresh" content="0; url=https://friedroy.notion.site/NeurIPS-2022-Summary-a7ee4455f45f46be9c2ba434e5b28a48"/> </head> </html>]]></content><author><name></name></author><summary type="html"><![CDATA[A summary of the work I found interesting in NeurIPS 2022]]></summary></entry><entry><title type="html">A Simplified Overview of Langevin Dynamics</title><link href="https://friedmanroy.github.io/blog/2022/Langevin/" rel="alternate" type="text/html" title="A Simplified Overview of Langevin Dynamics"/><published>2022-11-01T00:00:00+00:00</published><updated>2022-11-01T00:00:00+00:00</updated><id>https://friedmanroy.github.io/blog/2022/Langevin</id><content type="html" xml:base="https://friedmanroy.github.io/blog/2022/Langevin/"><![CDATA[<p>Langevin dynamics (or sampling) <d-cite key="langevin"></d-cite> is one of the most popular <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo</a> (MCMC) methods out there. It is used for countless tasks that require sampling from a distribution, and is even really simple to use especially since automatic differentiation is easily accessible. While straightforward to implement, I think that it’s hard to build an intuition for what to expect without seeing toy examples. In this post, I want to try and build the intuition for the sampling procedure itself, the stationary distribution that will be reached, and how to mitigate possible problems with the sampling procedure.</p> <h1 id="problem-setting">Problem Setting</h1> <p>Suppose we have the following distribution:</p> \[\begin{equation}\label{eq:prob} p(x)\propto e^{-f(x)}\Leftrightarrow \log p(x)=-f(x)+\text{const} \end{equation}\] <p>where $x\in\mathbb{R}^d$ and $f(x)$ is some function so that the integral $\intop e^{-f(x)}dx$ doesn’t diverge (i.e. $p(x)$ is a valid distribution).</p> <p>We would like to sample from this distribution, but don’t have any way to do so directly. Worse, we don’t know the conditional or marginal distributions under this PDF, so we can’t use methods such as <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs sampling</a>. However, what we <em>do</em> know is the gradient of the function at every point: $\nabla f(x)$ <d-footnote>The function $f(x)$ is sometimes called the energy function and the negative gradient the potential function</d-footnote>.</p> <p>While we don’t know the distribution itself, we obviously know quite a bit about the distribution if we know how to differentiate it. Specifically, we can use gradient descent (GD) in order to find the modes of the distribution:</p> \[\begin{equation} x_t=x_{t-1} - \eta_t \nabla f(x_{t-1}) \end{equation}\] <p>where $\eta_t &gt; 0$ is a scalar usually called the <em>learning rate</em> or <em>step size</em>. If we can find the modes of the distribution, then it follows that we can find the areas with peaks in the density, i.e. areas which may be quite likely in the distribution. So while we may not know the distribution itself (only an unnormalized function that defines the distribution), the gradients actually tell us quite a bit about the function.</p> <p>Langevin sampling, which will be introduced in the next section, looks like a weird modification of GD. We will see that it amounts to adding noise every iteration of the GD algorithm. An easy (but not entirely correct) way to start thinking about Langevin sampling is that if we add some kind of noise to each step of the GD procedure, then most of the time the chain will converge to areas around the biggest peaks, instead of arriving at a local maxima of the distribution.</p> <h1 id="langevin-sampling">Langevin Sampling</h1> <p>As I mentioned, the sampling algorithm is surprisingly simple to implement and is iteratively defined as:</p> \[\begin{equation}\label{eq:langevin} x_{t+1}=x_t - \frac{\epsilon}{2}\nabla f(x_t)+\sqrt{\epsilon}\mathcal{N}\left(0,I\right) \end{equation}\] <p>where $\epsilon&gt;0$ is a (small) constant.</p> <p>Let’s look at a simple example of this in action:</p> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/Langevin/single_corr70.gif" alt="A simple example of Langevin sampling of a Gaussian distribution" style="display: inline-block; margin: 0 auto; "/> </p> <div class="caption"> Figure 1: an example of a single chain of Langevin sampling from a (simple) Gaussian distribution. The brighter areas denote areas with higher probabilities while the black dot is the sample in the current iteration. The trailing dots are previous iterations, which fade out gradually. </div> <p>As you can see, the little dot (which follows $x_t$ though the iterations) moves around in the bright areas of the distribution. Sometimes it explores outside of the bright regions, only to return. In general, the whole distribution is explored pretty quickly , as we would want it to be.</p> <h2 id="convergence">Convergence</h2> <p>Using the update rule of equation \eqref{eq:langevin}, the sample $x_T$ will converge to a sample from the distribution<d-footnote>Under some reasonable (but needed) assumptions over $f(\cdot)$</d-footnote> in equation \eqref{eq:prob} at the limit $T\rightarrow \infty$ and $\epsilon \rightarrow 0$ . Obviously, we can’t wait an infinite number of time steps, so usually $T$ and $\epsilon$ are tuned by hand so that $T$ is “large enough” and $\epsilon$ is “small enough”. Convergence in this sense means that if we have many points that are initialized from the same point, after a long enough time $T$ , we would be able to approximate the distribution with these samples; the follow gives a small example of this:</p> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/Langevin/langevin_1.gif" alt="Convergence of Langevin to samples from the true distribution" style="display: inline-block; margin: 0 auto; "/> </p> <div class="caption"> Figure 2: the left side shows the true distribution and 100 points sampled according to Langevin from this distribution, while the plot on the right shows the approximated distribution according to 1000 sampled points. Notice how the distribution is very different from the true one at the beginning, after which it slowly gets closer and closer to the true distribution. </div> <hr/> <p>Usually each function we want to sample from requires different handling. Notice how in figure 1 it seems like ~200 steps are enough to reach a sample from the distribution. Now, let’s look at a slightly slower example:</p> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/Langevin/single_corr99.gif" alt="Langevin on a narrow Gaussian" style="display: inline-block; margin: 0 auto; "/> </p> <div class="caption"> Figure 3: another example of a single chain of Langevin sampling, only on a narrower Gaussian. While the chain reaches the bright part of the distribution quite quickly, it always remains in the top half (for all 800 iterations). </div> <p>The chain arrives at the distribution fast enough, but then stays in the upper half of the distribution for it’s whole life time - for 800 iterations! This means that if we want to sample two points , which are initialized close to each other, then they will probably be very close to each other even after 800 iterations. In MCMC terms, we would say that the chain hasn’t <em>mixed</em> yet. This is in contrast to the example in figure 1, where ~200 iterations are enough for the chain to mix.</p> <p>In this case, intuition calls for a very simple modification - change the step size! If we increase the step size, then the dot would move further each iteration, and the chain would “mix” very rapidly, right? However, finding the right value for $\epsilon$ is actually pretty difficult. Consider the following toy example:</p> <div class="fake-img l-body"> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/Langevin/bad_langevin.png" alt="Langevin on a narrow Gaussian" style="display: inline-block; " width="100%"/> </p> </div> <div class="caption"> Figure 4: an example of bad settings of $\epsilon$. On the left, you can see the distribution and true samples from it. The middle two images show what happens when $\epsilon$ is too large (left) and too small (right). The right-most graph shows the average negative log-likelihood (the average value of $f(x)$ ) for each of the images to the left of it. </div> <p>Figure 3 illustrates the problems that can happen if $\epsilon$ isn’t correctly calibrated. If it is too large, then we’re adding a bunch of noise each iteration, so while the chain will converge at its’ stationary distribution very quickly, the stationary distribution will be very different from the distribution we actually want to sample from. On the other end of the spectrum, if $\epsilon$ is too small then it will take a very long time for the samples to move from their initial positions.</p> <p>Okay, so forget about getting there quickly, let’s just run a chain for very very long - that should be fine, right?</p> <p>Not really. Up until now I only showed examples where the distribution is mostly gathered together. Let’s see what happens when there are two separated islands:</p> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/Langevin/separated.gif" alt="Langevin when the distribution is separated into islands" style="display: inline-block; margin: 0 auto; "/> </p> <div class="caption"> Figure 5: what happens if the distribution is separated into islands? On the left you can see the true distribution as well as 100 Langevin chains running. On the right is a visualization of the distribution that 1000 Langevin chains approximate. </div> <p>If we use a small $\epsilon$ when there are islands in the distribution, then Langevin might converge to a good representation of one of the islands, but the samples won’t “hop” between the islands<d-footnote>If we wait long enough, they will. But as you can see above, in the 2000 iterations, only one dot from the 1000 made it across.</d-footnote>. On the other hand, if $\epsilon$ is large, then both islands might be well represented by the samples, but the approximated distribution will be much wider than what we are trying to approximate.</p> <h2 id="adding-metropolis-hastings">Adding Metropolis-Hastings</h2> <p>I should mention that all the examples so far were in 2D, so we could see the distribution and the samples. In 2D, calibrating $\epsilon$ isn’t too hard - you just have to try a couple of times and you can <em>actually see</em> the results. However, most practical use cases are in much higher dimensionalities. In such cases, we can’t see the distribution and we only really have marginal signals of whether convergence was reached. This means that understanding whether the chain has converged or not is much, <em>much</em>, more difficult.</p> <p>One way to make life (a bit) simpler is by making sure that, no matter what $\epsilon$ we use, the chain will always converge to the correct distribution, at some point. This can be done by using the so called <em>Metropolis-Hastings</em> correction. I won’t go into too many details regarding this, for that you should go to the much better blog post about Metropolis-Hastings, <a href="https://similarweb.engineering/mcmc/">here</a>. Using this correction, <em>any</em> step size can be used, and the chain will eventually arrive at a sample from the true distribution, although we might need to wait a long while.</p> <p>At a very high level, Metropolis-Hastings is a framework which allows, at each iteration, to determine whether the current step is way off mark, given the preceding step. If the current step diverges from the distribution too much, it will be thrown away (called <em>rejections</em>), otherwise it should be kept. Using Metropolis-Hastings together with Langevin yields the <em>Metropolis adjusted Langevin algorithm</em> <d-cite key="langevin"></d-cite> (MALA)<d-footnote>The "H" from MH is usually omitted for this algorithm, for some reason. However, you might sometimes see it names MHALA instead of MALA</d-footnote>. You can find a very good (interactive!) demo for this algorithm in the following link: <a href="http://chi-feng.github.io/mcmc-demo/app.html?algorithm=MALA&amp;target=banana">here</a>, so I’m not going to try and code up examples for MALA. Instead, let’s look at other solutions for our problems with $\epsilon$ .</p> <h1 id="annealed-langevin">Annealed Langevin</h1> <p>Many times, calculating either the function $f(x)$ or the derivative $\nabla f(x)$ can be a very expensive computation, time-wise. In such cases, using MALA instead of just Langevin accrues a heavy cost <d-cite key="SGLD"></d-cite>. Additionally, there are modern algorithms which attempt to approximate $\nabla f(x)$ <em>without even knowing</em> $f(x)$ <d-footnote>Hopefully I'll write a blog post about this soon</d-footnote>! Examples of these algorithms are score-based matching <d-cite key="score-based"></d-cite> and denoising diffusion probabilistic models <d-cite key="DDPM"></d-cite>, both of which have become extremely popular in recent years (e.g. stable diffusion <d-cite key="stable"></d-cite>). In order to use MALA, it is necessary to calculate $f(x)$ , which means we won’t be able to use it in models that only approximate $\nabla f(x)$ .</p> <p>Instead, these models often use a heuristic common in optimization algorithms - <em>annealing</em> the step size. This means that the sampling procedure begins with a relatively large step size that is gradually decreased. If the step size is decreased slowly enough, and decreased to a small enough number, then the hope is that we will benefit from both sides of the scale. The starting iterations will allow the chain to mix, while iterations towards the end will “catch” the small scale behavior of the distribution. This will also allow particles to hop between islands:</p> <p align="center"> <img src="https://friedmanroy.github.io/assets/blog_figs/Langevin/langevin_annealed.gif" alt="Annealed Langevin for a separated distribution" style="display: inline-block; margin: 0 auto; "/> </p> <div class="caption"> Figure 6: annealed Langevin used on a somewhat separated distribution. Notice how particles move really quickly at the beginning, gradually slowing down until they are almost static by the end of the animation. </div> <p>Annealed Langevin is a bit harder to justify theoretically, but in practice it works quite well. The only problem is that the burden has now shifted from finding one good value for $\epsilon$ to finding a good schedule; good values for $\epsilon_t$ in every time step $t$ . One of the common schedules is the geometric decay:</p> \[\begin{equation} \epsilon_t = \epsilon_0\left(\beta+t\right)^{-\gamma} \end{equation}\] <p>with $\epsilon_0, \beta, \gamma &gt; 0$ . There are works on the optimal values for this schedule (e.g. <d-cite key="convergence"></d-cite>), but I’m pretty sure this is usually set according to a case-to-case basis.</p> <h1 id="conclusion">Conclusion</h1> <p>Langevin sampling is a <em>really</em> popular sampling algorithm, with some intimidating names and keywords thrown around whenever it is used<d-footnote>The ones used in this post alone: Langevin, Metropolis-Hastings, energy functions, potential functions, MCMC, mixing times, etc.</d-footnote>. Many of these terms arise from the fact that this algorithm is actually a discretized version of a physical model for the dynamics of small particles in a fluid (see <a href="https://en.wikipedia.org/wiki/Langevin_equation#Brownian_motion_as_a_prototype">Wikipedia about this</a>). Indeed, if you go back to the first few animations, you can think of the moving dot as a small particle within a fluid that is moved by a force that draws it to the maximums of the distribution, but is also affected by a force that moves it randomly in each iteration, allowing the particle to break free of the maximums of the distribution.</p> <p>Despite all the keywords, I find that the algorithm itself is much simpler than people typically think. In fact, the reason it is used to much is because it is so simple to implement and utilize. The sampling algorithm itself is pretty lousy (iteration-wise) in high dimensions. However, running it is efficient enough and simple enough to actually add it into consideration.</p> <p>Not to mention, the animations generated by this algorithm are pretty nice.</p>]]></content><author><name>Roy Friedman</name></author><category term="sampling"/><category term="MCMC"/><category term="machine-learning"/><summary type="html"><![CDATA[An overview of Langevin dynamics (or sampling), with a focus on building up intuition for how it works, when it works, and what can be done to make it work when it doesn't.]]></summary></entry></feed>