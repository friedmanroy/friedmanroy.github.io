<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="fqpmQwGNtZ1kJ8UAsQMb7RD2N7DcYRDlyGbAJsnZuGM"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Estimating the Gaussian Distribution | Roy Friedman</title> <meta name="author" content="Roy Friedman"> <meta name="description" content="The math of estimating the parameters of a Gaussian using MLE as well as Bayesian inference, with some intuition regarding the effects of sample size and prior selection."> <meta name="keywords" content="academic-website, roy-friedman, machine-learning, ML, bayesian, PhD"> <meta name="google-site-verification" content="EUCyoY6MaSyn4ZvM9TfAuzzeleW7dR0PHS4_UCtyZ4I"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://friedmanroy.github.io/BML/4_gaussian_estimation/"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Estimating the Gaussian Distribution",
      "description": "The math of estimating the parameters of a Gaussian using MLE as well as Bayesian inference, with some intuition regarding the effects of sample size and prior selection.",
      "published": "October 28, 2022",
      "authors": [
        {
          "author": "Roy Friedman",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Hebrew University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Roy </span>Friedman</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/BML/">Bayesian Machine Learning</a> </li> <li class="nav-item "> <a class="nav-link" href="/_pages/cv/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Estimating the Gaussian Distribution</h1> <p>The math of estimating the parameters of a Gaussian using MLE as well as Bayesian inference, with some intuition regarding the effects of sample size and prior selection.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#ml-estimates">ML Estimates</a></div> <div><a href="#1d-bayesian-inference">1D Bayesian Inference</a></div> <div><a href="#multivariate-gaussian">Multivariate Gaussian</a></div> <div><a href="#choices-of-priors">Choices of Priors</a></div> <div><a href="#discussion">Discussion</a></div> </nav> </d-contents> <p><span style="float:left"><a href="https://friedmanroy.github.io/BML/3_gaussians/">← The Gaussian Distribution</a></span><span style="float:right"><a href="https://friedmanroy.github.io/BML/5_linear_regression/">Linear Regression →</a></span> <br> <br></p> <blockquote> <p>In the previous post we saw the definition of the Gaussian distribution and some of its most useful properties. Having defined this distribution, our next point of interest will be to <em>estimate</em> the mean and covariance of a Gaussian distribution given some datapoints. This will be our main focus in this post.</p> </blockquote> <p>While Bayesian statistics is our main interest in this thread of posts, many times it’s easier to go over the frequentist version first, since it’s less mathematically involved. Once we understand the ML solution, we will move on to the Bayesian treatment. Many times, this process reveals how the Bayesian and frequentist views are related to each other.</p> <p>The parameters of a Gaussian distribution are $\mu$ and $\Sigma$, so $\theta=\left{ \mu,\Sigma\right}$ . In the frequentist case we will estimate both, however the Bayesian treatment of $\Sigma$ is a bit more complex and doesn’t teach much, so we will ignore it for now.</p> <p><br></p> <h1 id="ml-estimates">ML Estimates</h1> <p>The log-likelihood of a data set $\mathcal{D}=\left{ x_{i}\right} _{i=1}^{N}$ sampled from a Gaussian distribution is:</p> \[\begin{equation} \begin{split}\log p\left(\mathcal{D}|\mu,\Sigma\right) &amp; =\sum_{i}\log\mathcal{N}\left(x_{i}\,|\,\mu,\Sigma\right)\\ &amp; =\sum_{i}\log\left[\frac{1}{\sqrt{\left(2\pi\right)^{d}\left|\Sigma\right|}}\exp\left\{ -\frac{1}{2}\left(x_{i}-\mu\right)^{T}\Sigma^{-1}\left(x_{i}-\mu\right)\right\} \right]\\ &amp; =\sum_{i}\left[-\frac{d}{2}\log2\pi-\frac{1}{2}\log\left|\Sigma\right|-\frac{1}{2}\left(x_{i}-\mu\right)^{T}\Sigma^{-1}\left(x_{i}-\mu\right)\right]\\ &amp; =-\frac{Nd}{2}\log2\pi-\frac{N}{2}\log\left|\Sigma\right|-\frac{1}{2}\sum_{i}\left(x_{i}-\mu\right)^{T}\Sigma^{-1}\left(x_{i}-\mu\right) \end{split} \end{equation}\] <p>Before we begin the process of finding the ML estimators<d-footnote>Bishop 2.3.4; Murphy 4.1.3.</d-footnote> for $\mu$ and $\Sigma$ , let’s see another way of writing the log-likelihood. Notice that for any scalar $a$ , we can write: \(\begin{equation} a=\text{trace}\left[a\right] \end{equation}\) Since this is true for any scalar, we can apply this to the inner product of 2 vectors $x^{T}y$ (which is just a number) as well:</p> \[\begin{equation} x^{T}y=\text{trace}\left[x^{T}y\right]=\text{trace}\left[yx^{T}\right] \end{equation}\] <p>Now, recall that $\left(x-\mu\right)^{T}\Sigma^{-1}\left(x-\mu\right)$ is also a scalar, so we can use the above identity to rewrite it as:</p> \[\begin{align} \ell\left(\mathcal{D}|\mu,\Sigma\right) &amp; =-\frac{Nd}{2}\log2\pi-\frac{N}{2}\log\left|\Sigma\right|-\frac{1}{2}\sum_{i}\text{trace}\left[\Sigma^{-1}\left(x_{i}-\mu\right)\left(x_{i}-\mu\right)^{T}\right]\nonumber \\ &amp; =-\frac{Nd}{2}\log2\pi-\frac{N}{2}\log\left|\Sigma\right|-\frac{1}{2}\text{trace}\left[\Sigma^{-1}\sum_{i}\left(x_{i}-\mu\right)\left(x_{i}-\mu\right)^{T}\right] \end{align}\] <p>Finally, if we define $S\stackrel{\Delta}{=}\frac{1}{N}\sum_{i}\left(x_{i}-\mu\right)\left(x_{i}-\mu\right)^{T}$ (which is almost the empirical covariance), we get a shorter form for the log-likelihood (which is sometimes used in the literature):</p> \[\begin{equation} \ell\left(\mathcal{D}|\mu,\Sigma\right)=-\frac{N}{2}\left(d\log2\pi+\log\left|\Sigma\right|+\text{trace}\left[\Sigma^{-1}S\right]\right) \end{equation}\] <h2 id="mle-for-mu-">MLE for $\mu$ <a name="mu-MLE"></a> </h2> <p>We begin by finding the mean that maximizes the log-likelihood, by differentiating the log-likelihood:</p> \[\begin{align} \frac{\partial\ell}{\partial\mu} &amp; =-\frac{1}{2}\sum_{i}\frac{\partial}{\partial\mu}\left(x_{i}-\mu\right)^{T}\Sigma^{-1}\left(x_{i}-\mu\right)\\ &amp; =-\frac{1}{2}\sum_{i}2\Sigma^{-1}\left(x_{i}-\mu\right)\\ &amp; =-\Sigma^{-1}\sum_{i}\left(x_{i}-\mu\right)\stackrel{!}{=}0 \end{align}\] <p>By equating to 0 we can find the maxima:</p> \[\begin{equation} \hat{\mu}_{\text{ML}}=\frac{1}{N}\sum_{i}x_{i} \end{equation}\] <p>Here we write $\hat{\mu}_{\text{ML}}$ to show that it is the <em>maximum likelihood estimator</em> for the data set. Notice that, unsurprisingly, the ML estimator for the mean of the Gaussian is the <em>empirical mean</em> of the data.</p> <h2 id="mle-for-sigma">MLE for $\Sigma$</h2> <p>Using the following definition of the derivatives (which are a bit harder to get directly on your own):</p> \[\begin{equation} \frac{\partial}{\partial\Sigma}\log\left|\Sigma\right|=\frac{1}{\left|\Sigma\right|}\frac{\partial}{\partial\Sigma}\left|\Sigma\right|=\frac{1}{\left|\Sigma\right|}\left|\Sigma\right|\Sigma^{-1}=\Sigma^{-1} \end{equation}\] <p>and:</p> \[\begin{equation} \frac{\partial}{\partial\Sigma}\text{trace}\left[\Sigma^{-1}S\right]=-\Sigma^{-1}S\Sigma^{-1} \end{equation}\] <p>we can find the MLE for $\Sigma$. The full derivative of the log-likelihood by $\Sigma$ is:</p> \[\begin{align} \frac{\partial\ell}{\partial\Sigma} &amp; =-\left(\Sigma^{-1}-\Sigma^{-1}S\Sigma^{-1}\right)\stackrel{!}{=}0\nonumber \\ \Rightarrow &amp; \Sigma^{-1}=\Sigma^{-1}S\Sigma^{-1}\nonumber \\ \Rightarrow &amp; I=\Sigma^{-1}S\nonumber \\ \Rightarrow &amp; \Sigma=S\nonumber \\ \Rightarrow &amp; \hat{\Sigma}_{\text{ML}}=\frac{1}{N}\sum_{i}\left(x_{i}-\mu\right)\left(x_{i}-\mu\right)^{T} \end{align}\] <p>Because $\hat{\mu}_{\text{ML}}$ is not dependent on $\hat{\Sigma}_{\text{ML}}$, we can first find the MLE for $\mu$ and then for $\Sigma$ , so that:</p> \[\begin{equation} \hat{\Sigma}_{\text{ML}}=\frac{1}{N}\sum_{i}\left(x_{i}-\hat{\mu}_{\text{ML}}\right)\left(x_{i}-\hat{\mu}_{\text{ML}}\right)^{T} \end{equation}\] <hr> <p>Putting the two equations together, the ML estimators for the parameters of a Gaussian distribution are: \(\begin{equation} \begin{split}\hat{\mu}_{\text{ML}} &amp; =\frac{1}{N}\sum_{i}x_{i}\\ \hat{\Sigma}_{\text{ML}} &amp; =\frac{1}{N}\sum_{i}\left(x_{i}-\hat{\mu}_{\text{ML}}\right)\left(x_{i}-\hat{\mu}_{\text{ML}}\right)^{T} \end{split} \end{equation}\)</p> <p><br></p> <h1 id="1d-bayesian-inference">1D Bayesian Inference</h1> <p>Recall that in the Bayesian treatment, we assume that the parameters are distributed in some manner. We begin by considering the 1D case for Gaussian distributions<d-footnote>See Bishop 2.3.6 for more details.</d-footnote>:</p> \[\begin{equation} p\left(x\right)=\frac{1}{Z}\exp\left[-\frac{\left(x-\mu\right)^{2}}{2\sigma^{2}}\right] \end{equation}\] <p>For now, we will assume that we know the variance $\sigma^{2}$ . We will assume a Gaussian prior over $\mu$ (if we want we can assume different priors as well, but let’s stick with Gaussian priors for now):</p> \[\begin{equation} p\left(\mu\right)=\mathcal{N}\left(\mu_{0},\sigma_{0}^{2}\right) \end{equation}\] <p>Given a data set $\mathcal{D}=\left{ x_{i}\right} _{i=1}^{N}$ , the likelihood is:</p> \[\begin{align} p\left(\mathcal{D}|\mu\right) &amp; =\prod_{i=1}^{N}p\left(x_{i}|\mu\right)=\prod_{i=1}^{N}\mathcal{N}\left(x_{i}\,|\,\mu,\sigma^{2}\right)\\ &amp; \propto\exp\left[-\frac{1}{2\sigma^{2}}\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}\right] \end{align}\] <p>The posterior probability for $\mu$ will then be:</p> \[\begin{align}\label{eq:post} p\left(\mu|\mathcal{D}\right) &amp; =\frac{p\left(\mathcal{D}|\mu\right)p\left(\mu\right)}{p\left(\mathcal{D}\right)}\\ &amp; \propto p\left(\mathcal{D}|\mu\right)p\left(\mu\right) \end{align}\] <p>Recall that the term $p\left(\mathcal{D}\right)$ is constant and only serves as a normalization, so for now we can ignore it.</p> <details><summary>Derivation of the posterior</summary> <p>Let’s look at the product in equation \eqref{eq:post} more closely:</p> \[\begin{align} p\left(\mathcal{D}|\mu\right)p\left(\mu\right) &amp; \propto\exp\left[-\frac{1}{2\sigma^{2}}\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}\right]\exp\left[-\frac{\left(\mu-\mu_{0}\right)^{2}}{2\sigma_{0}^{2}}\right]\\ &amp; =\exp\left[-\frac{1}{2}\left(\frac{1}{\sigma^{2}}\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}+\frac{1}{\sigma_{0}^{2}}\left(\mu-\mu_{0}\right)^{2}\right)\right] \end{align}\] <p>Notice that the term in the exponent is <em>still quadratic in</em> $\mu$ . This means, of course, that this whole term is still a Gaussian distribution. Let’s use the derivative trick <a href="https://friedmanroy.github.io/BML/3_gaussians/">from the previous post</a> in order to find the distribution of $\mu$ exactly. Define:</p> \[\begin{equation} \Delta\stackrel{\Delta}{=}\frac{1}{2}\left(\frac{1}{\sigma^{2}}\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}+\frac{1}{\sigma_{0}^{2}}\left(\mu-\mu_{0}\right)^{2}\right) \end{equation}\] <p>Recall, we can now differentiate $\Delta$ with respect to $\mu$ in order to find the mean and covariance of the posterior distribution:</p> \[\begin{align} \frac{\partial\Delta}{\partial\mu} &amp; =\frac{1}{2}\left(\frac{1}{\sigma^{2}}\sum_{i}\frac{\partial}{\partial\mu}\left(x_{i}-\mu\right)^{2}+\frac{1}{\sigma_{0}^{2}}\frac{\partial}{\partial\mu}\left(\mu-\mu_{0}\right)^{2}\right)\nonumber \\ &amp; =\frac{1}{\sigma^{2}}\sum_{i}\left(\mu-x_{i}\right)+\frac{1}{\sigma_{0}^{2}}\left(\mu-\mu_{0}\right)\nonumber \\ &amp; =\frac{1}{\sigma^{2}}\left(N\mu-\sum_{i}x_{i}\right)+\frac{1}{\sigma_{0}^{2}}\left(\mu-\mu_{0}\right)\nonumber \\ &amp; =\frac{N}{\sigma^{2}}\left(\mu-\mu_{ML}\right)+\frac{1}{\sigma_{0}^{2}}\left(\mu-\mu_{0}\right)\nonumber \\ &amp; =\left(\frac{N}{\sigma^{2}}+\frac{1}{\sigma_{0}^{2}}\right)\left(\mu-\left(\frac{N}{\sigma^{2}}+\frac{1}{\sigma_{0}^{2}}\right)^{-1}\left(\frac{N}{\sigma^{2}}\mu_{\text{ML}}+\frac{1}{\sigma_{0}^{2}}\mu_{0}\right)\right) \end{align}\] <p>where $\mu_{\text{ML}}=\frac{1}{N}\sum_{i}x_{i}$ is the ML estimate for $\mu$ , as we showed in <a href="#mu-MLE">section for the MLE of the mean</a> .</p> </details> <p>Defining: \(\begin{equation} \frac{N}{\sigma^{2}}+\frac{1}{\sigma_{0}^{2}}\stackrel{\Delta}{=}\frac{1}{\sigma_{N}^{2}} \end{equation}\) the posterior of $\mu$ is equal to:</p> \[\begin{equation} p\left(\mu|\mathcal{D}\right)=\mathcal{N}\left(\mu\,|\,\sigma_{N}^{2}\left(\frac{N}{\sigma^{2}}\mu_{\text{ML}}+\frac{1}{\sigma_{0}^{2}}\mu_{0}\right),\,\sigma_{N}^{2}\right) \end{equation}\] <p>where $\sigma_{N}^{2}=\left(\frac{N}{\sigma^{2}}+\frac{1}{\sigma_{0}^{2}}\right)^{-1}=\left(\frac{N\sigma_{0}^{2}+\sigma^{2}}{\sigma_{0}^{2}\sigma^{2}}\right)^{-1}=\frac{\sigma_{0}^{2}\sigma^{2}}{N\sigma_{0}^{2}+\sigma^{2}}$ and $\mu_{\text{ML}}=\frac{1}{N}\sum_{i}x_{i}$ . If we write all of this explicitly, we will get:</p> \[\begin{equation} p\left(\mu|\mathcal{D}\right)=\mathcal{N}\left(\mu\,|\,\frac{\sigma_{0}^{2}\sigma^{2}}{N\sigma_{0}^{2}+\sigma^{2}}\left(\frac{N}{\sigma^{2}}\mu_{\text{ML}}+\frac{1}{\sigma_{0}^{2}}\mu_{0}\right),\,\frac{\sigma_{0}^{2}\sigma^{2}}{N\sigma_{0}^{2}+\sigma^{2}}\right) \end{equation}\] <h2 id="effects-of-sample-size">Effects of Sample Size</h2> <p>It may be a good idea to get some intuition for the posterior we found. Let’s look at the slightly simpler case of $\mu_{0}=0$ (but the analysis that follows is true for any $\mu_{0}$ ). In this case, the posterior is:</p> \[\begin{equation} \mathcal{N}\left(\frac{\sigma_{0}^{2}\sigma^{2}}{N\sigma_{0}^{2}+\sigma^{2}}\cdot\frac{N}{\sigma^{2}}\mu_{\text{ML}},\,\frac{\sigma_{0}^{2}\sigma^{2}}{N\sigma_{0}^{2}+\sigma^{2}}\right) \end{equation}\] <p>Let’s see what happens when $N=0$ . If we don’t have any data, we should probably always fall back to the only thing we know; our prior. At $N=0$ , we have:</p> \[\begin{equation} N=0\qquad\begin{array}{c} \frac{\sigma_{0}^{2}\sigma^{2}}{N\sigma_{0}^{2}+\sigma^{2}}\cdot\frac{N}{\sigma^{2}}\mu_{\text{ML}}=0=\mu_{0}\\ \frac{\sigma_{0}^{2}\sigma^{2}}{N\sigma_{0}^{2}+\sigma^{2}}=\frac{\sigma_{0}^{2}\sigma^{2}}{\sigma^{2}}=\sigma_{0}^{2} \end{array} \end{equation}\] <p>so the posterior (naturally) falls back to the prior. If we look at the other extreme, $N\rightarrow\infty$ , then there should be no ambiguity over the value of $\mu$ whatsoever:</p> \[\begin{equation} N\rightarrow\infty\qquad\begin{array}{c} \frac{\sigma_{0}^{2}\sigma^{2}}{N\sigma_{0}^{2}+\sigma^{2}}\cdot\frac{N}{\sigma^{2}}\mu_{\text{ML}}\rightarrow\frac{\sigma_{0}^{2}\sigma^{2}}{N\sigma_{0}^{2}}\cdot\frac{N}{\sigma^{2}}\mu_{\text{ML}}=\mu_{\text{ML}}\\ \frac{\sigma_{0}^{2}\sigma^{2}}{N\sigma_{0}^{2}+\sigma^{2}}\rightarrow\frac{\sigma_{0}^{2}\sigma^{2}}{N\sigma_{0}^{2}}=0 \end{array} \end{equation}\] <p>If $N$ is somewhere in between, then the term for the mean is (as we saw before): \(\begin{equation}\label{eq:mu-N} \mu_{N}\stackrel{\Delta}{=}\left(\frac{N}{\sigma^{2}}+\frac{1}{\sigma_{0}^{2}}\right)^{-1}\left(\frac{N}{\sigma^{2}}\mu_{\text{ML}}+\frac{1}{\sigma_{0}^{2}}\mu_{0}\right) \end{equation}\) This is a <em>weighted mean</em> of the two values $\mu_{\text{ML}}$ and $\mu_{0}$ (you can find a demo for this behavior <a href="https://www.desmos.com/calculator/dgrvldq2ok" rel="external nofollow noopener noopener noreferrer" target="_blank">here</a>). We can look at the number of samples needed in order for $\mu_{N}$ to be <em>exactly</em> between the ML estimate and the prior by giving equal weight to both terms:</p> \[\begin{equation} \frac{N}{\sigma^{2}}\stackrel{!}{=}\frac{1}{\sigma_{0}^{2}}\Rightarrow\hat{N}=\frac{\sigma^{2}}{\sigma_{0}^{2}} \end{equation}\] <p>So, when the variance of the prior is very small, which is like saying “we are very sure that $\mu$ is close to $\mu_{0}$ “, then a lot of samples are needed in order to move $\mu_{N}$ away from the prior $\mu_{0}$ . If, on the other hand, the variance of the prior is very large, which may mean we are very unsure that $\mu_{0}$ is correct, then few points are needed in order to move the mean from the prior mean. Finally, if the sample variance ( $\sigma^{2}$ ) is very large, then we need to get a lot of data to be sure that the MLE is correct, while if it is very small, then we need very few points in order to be sure of the MLE.</p> <p>Because the posterior is so dependent on the number of samples, it is sometimes written (like in equation \eqref{eq:mu-N}) as:</p> \[\begin{equation} p\left(\mu|\mathcal{D}\right)=\mathcal{N}\left(\mu\,|\,\mu_{N},\sigma_{N}^{2}\right) \end{equation}\] <p>with the intention behind this notation being “this is the posterior mean after having sampled $N$ points”.</p> <p align="center"> <img src="https://friedmanroy.github.io/assets/bml_figs/rec_3/1D_sample_size.png" alt="Effects of sample size on the posterior of the mean of a 1D Gaussian" style="display: inline-block; margin: 0 auto; "> </p> <div class="caption"> Figure 1: Posteriors for the mean after different amount of data are seen. When $N=0$ , the posterior is equal to the prior. As more points are observed, the posterior is pulled towards the true value that generated the points. Brighter posteriors are those with more observed points. </div> <h2 id="map-and-mmse-estimates-for-mu">MAP and MMSE Estimates for $\mu$</h2> <p>The MAP estimate for $\mu$ under the prior above is given by:</p> \[\begin{equation} \hat{\mu}_{\text{MAP}}=\arg\max_{\mu}p\left(\mu|\mathcal{D}\right)=\arg\max_{\mu}\mathcal{N}\left(\mu\,|\,\mu_{N},\sigma_{N}^{2}\right) \end{equation}\] <p>Of course, the Gaussian distribution only has one maxima, which is the mean of the distribution. So the MAP estimate of $\mu$ is simply the mean:</p> \[\begin{equation} \hat{\mu}_{\text{MAP}}=\mathbb{E}\left[p\left(\mu|\mathcal{D}\right)\right]=\mu_{N} \end{equation}\] <p>where $\mu_{N}$ is given explicitly in equation \eqref{eq:mu-N}. Notice that (in this case) the MAP and MMSE estimates are one and the same: \(\begin{equation} \hat{\mu}_{MAP}=\mathbb{E}\left[p\left(\mu|\mathcal{D}\right)\right]=\hat{\mu}_{MMSE} \end{equation}\) The fact that the MAP and MMSE estimates are the same only happens to be true for the Gaussian distribution, in general they might be very different from each other!</p> <p><br></p> <h1 id="multivariate-gaussian">Multivariate Gaussian</h1> <p>Now that we understood the basic premise of the Bayesian inference for $\mu$ in 1D, we can start all over again for the multivariate case. We assume, again, that: \(\begin{equation} x\sim\mathcal{N}\left(\mu,\Sigma\right) \end{equation}\) where the covariance matrix $\Sigma$ is known. We also assume a prior over $\mu$ of the form: \(\begin{equation} \mu\sim\mathcal{N}\left(\mu_{0},\Sigma_{0}\right) \end{equation}\) The likelihood for a data set $\mathcal{D}$ is: \(\begin{equation} p\left(\mathcal{D}|\mu\right)\propto\exp\left[-\frac{1}{2}\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{T}\Sigma^{-1}\left(x_{i}-\mu\right)\right] \end{equation}\) and the posterior is:</p> \[\begin{equation} p\left(\mu|\mathcal{D}\right)\propto\exp\left[-\frac{1}{2}\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{T}\Sigma^{-1}\left(x_{i}-\mu\right)\right]\exp\left[-\frac{1}{2}\left(\mu-\mu_{0}\right)^{T}\Sigma_{0}^{-1}\left(\mu-\mu_{0}\right)\right] \end{equation}\] <p>Essentially nothing has changed from before; the term in the exponent is still quadratic in $\mu$, so we can employ our tricks once again:</p> \[\begin{align*} \frac{\partial}{\partial\mu}\Delta &amp; =\Sigma^{-1}\sum_{i}\left(\mu-x_{i}\right)+\Sigma_{0}^{-1}\left(\mu-\mu_{0}\right)\\ &amp; =N\Sigma^{-1}\left(\mu-\mu_{\text{ML}}\right)+\Sigma_{0}^{-1}\left(\mu-\mu_{0}\right)\\ &amp; =\left(N\Sigma^{-1}+\Sigma_{0}^{-1}\right)\left[\mu-\left(N\Sigma^{-1}+\Sigma_{0}^{-1}\right)^{-1}\left(N\Sigma^{-1}\mu_{\text{ML}}+\Sigma_{0}^{-1}\mu_{0}\right)\right] \end{align*}\] <p>where we used the same definition as before for $\mu_{\text{ML}}$ (the ML estimate for $\mu$ ). The full posterior is given by: \(\begin{equation} \mu|\mathcal{D}\sim\mathcal{N}\left(\mu_{N},\Sigma_{N}\right) \end{equation}\) where: \(\begin{align} \Sigma_{N} &amp; =\left[N\Sigma^{-1}+\Sigma_{0}^{-1}\right]^{-1}\\ \mu_{N} &amp; =\Sigma_{N}\left[N\Sigma^{-1}\mu_{\text{ML}}+\Sigma_{0}^{-1}\mu_{0}\right] \end{align}\)</p> <p>The result is consistent with what we saw in 1D. The main difference here is that now we need to invert the matrix $N\Sigma^{-1}+\Sigma_{0}^{-1}$ in order to find $\Sigma_{N}$ and $\mu_{N}$ . The MAP/MMSE estimates for the multivariate $\mu$ are again the mean of the posterior (since this is a Gaussian distribution as well): \(\begin{equation} \hat{\mu}_{\text{MAP}}=\left[N\Sigma^{-1}+\Sigma_{0}^{-1}\right]^{-1}\left[N\Sigma^{-1}\mu_{\text{ML}}+\Sigma_{0}^{-1}\mu_{0}\right]=\hat{\mu}_{MMSE} \end{equation}\)</p> <p><br></p> <h1 id="choices-of-priors">Choices of Priors</h1> <p>When theoretically analyzing Bayesian methods, they are often described in terms of “the true prior”. However, in practice we don’t actually have explicit access to “the true prior” and instead have to choose which prior to use. This is the main criticism against the Bayesian approach, because when there is not much prior knowledge researchers tend to choose arbitrary distributions as their priors.</p> <p>The fact that a prior can be chosen, however, does give a lot of flexibility. If a researcher understands that their prior knowledge isn’t very good, they can assign a very “wide” prior - one that gives similar densities to most parameter settings. Choosing a wide prior then has the effect of only slightly biasing the MLE. On the flip side, if there is a lot of prior knowledge, then it only seems natural to take that knowledge into account.</p> <div class="fake-img l-page"> <p align="center"> <img src="https://friedmanroy.github.io/assets/bml_figs/rec_3/high_dim_fitting.png" alt="Estimating the mean of an MVN in high dimensions under different priors" style="display: inline-block; margin: 0 auto; "> </p> </div> <div class="caption"> Figure 2: estimating the mean of a 30-dimensional Gaussian under different priors; to generate the data $\mu=0$ was used, while the near prior mean was $\mu_{0}=1$ and the far prior mean was $\mu_{0}=10$. The shaded areas are the areas of the posterior with total probability of 95%. This simple example shows that the prior can positively or negatively affect the performance of the estimation. </div> <p>The figure above illustrates what happens when Gaussian priors of different kinds are chosen. When the variance of the prior is low, i.e. $\Sigma_{0}=I\sigma_{0}^{2}$ with small $\sigma$ (left column), then many samples are needed to change the posterior distribution. When the prior mean is well calibrated to the generating distribution but the variance is still small, this translates to a better MMSE than the ML estimate with few samples but doesn’t get better when more samples are introduced. However, when the prior mean is far from the generating distribution, then the estimate will always be quite bad if the variance is kept small. The other end of the spectrum is when $\sigma$ is large (right column), in which case it doesn’t really matter what the prior mean is since the posterior mean is more or less equal to the ML estimate.</p> <p>The more interesting case is when $\mu_{0}$ is well calibrated and $\Sigma_{0}$ is moderate (middle column, top). In this setting, the MMSE gives a much better estimate than the MLE, <em>especially</em> in low sample-size settings - in this case, more than an order of magnitude.</p> <p>All of this is to say that the choice of prior can, and sometimes <em>should</em>, have big effects on estimates. Choosing a <em>well calibrated prior</em>, something we will look into in a few posts but can be done.</p> <p><br></p> <h1 id="discussion">Discussion</h1> <p>Even though the Gaussian distribution is one of the simplest distributions we can work with, it already illustrates the effects of using the Bayesian approach, which we saw in <a href="#choices-of-priors">the previous section</a>. When the prior is correctly specified, it can greatly boost performance in the low-data regime; on the other hand, when the amount of observed data increases, this posterior “merges” with the ML solution.</p> <p>In the next few posts we will still be concerned with the Gaussian distribution, however we will cast the problem into that of <em>prediction</em> in the regression task. Basically, we will observe what we consider to be a linear transformation of a Gaussian, and we will once again attempt to recover the parameters of the Gaussian. <br></p> <hr> <p><span style="float:left"><a href="https://friedmanroy.github.io/BML/3_gaussians/">← The Gaussian Distribution</a></span><span style="float:right"><a href="https://friedmanroy.github.io/BML/5_linear_regression/">Linear Regression →</a></span></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Roy Friedman. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener noopener noreferrer" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1B05NVC2PJ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1B05NVC2PJ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>